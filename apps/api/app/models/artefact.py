"""Artefact model for generated files and outputs.

Enterprise-grade file artifact tracking with strict Task Master ERD compliance.
Task 5.7: Complete artefact persistence with S3 tagging and audit logging.
"""

from datetime import datetime, timezone
import re
from typing import Optional, TYPE_CHECKING

from sqlalchemy import (
    String, BigInteger, ForeignKey, Index,
    DateTime, UniqueConstraint, Enum as SQLEnum
)
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship
from sqlalchemy.orm.attributes import flag_modified

from .base import Base, TimestampMixin
from .enums import ArtefactFileType

if TYPE_CHECKING:
    from .job import Job
    from .user import User
    from .machine import Machine
    from .topology_hashes import TopologyHash

# Module-level compiled regex patterns
# This avoids recompiling the patterns on every use
SHA256_PATTERN = re.compile(r'^[a-fA-F0-9]{64}$')
# S3 tag allowed characters: Letters, numbers, spaces, and +-=._:/@
S3_TAG_SANITIZE_PATTERN = re.compile(r'[^a-zA-Z0-9\s+\-=._:/@]')


class Artefact(Base, TimestampMixin):
    """File artifacts generated by jobs.
    
    Task 5.7 Requirements:
    - job_id FK with CASCADE behavior for cleanup
    - s3_bucket and s3_key with unique constraint
    - size_bytes bigint for file size tracking
    - sha256 string for integrity verification
    - mime_type string for content type
    - type field for classification (model, gcode, report, invoice, etc.)
    - created_by FK to users table
    - machine_id optional FK to machines table
    - post_processor optional string field
    - version_id for S3 versioning
    - Enterprise security and audit trail
    """
    
    __tablename__ = "artefacts"
    
    # Primary key
    id: Mapped[int] = mapped_column(
        primary_key=True, 
        autoincrement=True
    )
    
    # Foreign key with CASCADE behavior per Task 5.7
    job_id: Mapped[int] = mapped_column(
        ForeignKey("jobs.id", ondelete="CASCADE", name="fk_artefacts_job_id"),
        nullable=False,
        index=True
    )
    
    # S3 storage location per Task 5.7
    s3_bucket: Mapped[str] = mapped_column(
        String(255),
        nullable=False,
        index=True
    )
    
    s3_key: Mapped[str] = mapped_column(
        String(1024),
        nullable=False,
        index=True
    )
    
    # File metadata per Task 5.7
    size_bytes: Mapped[int] = mapped_column(
        BigInteger,
        nullable=False
    )
    
    sha256: Mapped[str] = mapped_column(
        String(64),
        nullable=False,
        index=True
    )
    
    mime_type: Mapped[str] = mapped_column(
        String(100),
        nullable=False
    )
    
    # Artifact classification per Task 5.7
    type: Mapped[str] = mapped_column(
        String(50),
        nullable=False,
        index=True,
        comment="Type: model, gcode, report, invoice, log, simulation, etc."
    )
    
    # User who created the artefact per Task 5.7
    created_by: Mapped[int] = mapped_column(
        ForeignKey("users.id", ondelete="RESTRICT", name="fk_artefacts_created_by"),
        nullable=False,
        index=True
    )
    
    # Optional machine reference per Task 5.7
    machine_id: Mapped[Optional[int]] = mapped_column(
        ForeignKey("machines.id", ondelete="SET NULL", name="fk_artefacts_machine_id"),
        nullable=True,
        index=True
    )
    
    # Optional post-processor per Task 5.7
    post_processor: Mapped[Optional[str]] = mapped_column(
        String(100),
        nullable=True,
        index=True
    )
    
    # S3 version ID for versioned buckets per Task 5.7
    version_id: Mapped[Optional[str]] = mapped_column(
        String(255),
        nullable=True,
        index=True
    )
    
    # Additional metadata (Task 5.7 for extension)
    meta: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True,
        comment="Additional metadata: tags, retention, compliance info, etc."
    )
    
    # File type enum (Task 7.15 addition)
    file_type: Mapped[Optional[ArtefactFileType]] = mapped_column(
        SQLEnum(ArtefactFileType),
        nullable=True,
        comment="File type enumeration for better querying"
    )
    
    # Relationships
    job: Mapped["Job"] = relationship(
        "Job", 
        back_populates="artefacts",
        foreign_keys=[job_id]
    )
    
    created_by_user: Mapped["User"] = relationship(
        "User",
        foreign_keys=[created_by]
    )
    
    machine: Mapped[Optional["Machine"]] = relationship(
        "Machine",
        foreign_keys=[machine_id]
    )
    
    # Task 7.15: Topology hashes relationship
    topology_hashes: Mapped[list["TopologyHash"]] = relationship(
        "TopologyHash",
        back_populates="artefact",
        cascade="all, delete-orphan"
    )
    
    # Enterprise-grade indexing strategy with Task 5.7 requirements
    __table_args__ = (
        # Unique constraint on bucket + key per Task 5.7
        UniqueConstraint(
            's3_bucket', 's3_key',
            name='uq_artefacts_s3_location'
        ),
        # Composite indexes for common queries
        Index(
            'idx_artefacts_job_id_type', 
            'job_id', 
            'type'
        ),
        Index(
            'idx_artefacts_created_by_type',
            'created_by',
            'type'
        ),
        Index(
            'idx_artefacts_machine_post',
            'machine_id',
            'post_processor',
            postgresql_where='machine_id IS NOT NULL'
        ),
        Index(
            'idx_artefacts_sha256', 
            'sha256'
        ),
        Index(
            'idx_artefacts_size_bytes', 
            'size_bytes'
        ),
        Index(
            'idx_artefacts_created_at', 
            'created_at'
        ),
        # GIN index for JSONB meta field for fast tag/metadata queries
        Index(
            'idx_artefacts_meta_gin',
            'meta',
            postgresql_using='gin',
            postgresql_where='meta IS NOT NULL'
        )
    )
    
    def __repr__(self) -> str:
        """Developer-friendly representation."""
        return (
            f"<Artefact(id={self.id}, job_id={self.job_id}, "
            f"type={self.type}, size={self.size_mb:.2f}MB, "
            f"bucket={self.s3_bucket})>"
        )
    
    def __str__(self) -> str:
        """User-friendly representation."""
        return f"Artefact #{self.id} - {self.type} ({self.size_mb:.2f}MB)"
    
    @property
    def size_kb(self) -> float:
        """Get file size in kilobytes."""
        return self.size_bytes / 1024.0
    
    @property
    def size_mb(self) -> float:
        """Get file size in megabytes."""
        return self.size_bytes / (1024.0 * 1024.0)
    
    @property
    def size_gb(self) -> float:
        """Get file size in gigabytes."""
        return self.size_bytes / (1024.0 * 1024.0 * 1024.0)
    
    @property
    def has_integrity_check(self) -> bool:
        """Check if artifact has SHA-256 hash for integrity."""
        return bool(self.sha256)
    
    def get_meta(self, key: str, default=None):
        """Get metadata value safely."""
        if not self.meta:
            return default
        return self.meta.get(key, default)
    
    def set_meta(self, key: str, value) -> None:
        """Set metadata value safely."""
        if self.meta is None:
            self.meta = {}
        self.meta[key] = value
        # Mark JSONB field as modified for SQLAlchemy tracking
        flag_modified(self, "meta")
    
    def add_processing_info(
        self, 
        processing_time_ms: int,
        compression_ratio: Optional[float] = None,
        quality_score: Optional[float] = None
    ) -> None:
        """Add processing metadata."""
        self.set_meta('processing_time_ms', processing_time_ms)
        if compression_ratio is not None:
            self.set_meta('compression_ratio', compression_ratio)
        if quality_score is not None:
            self.set_meta('quality_score', quality_score)
        self.set_meta('processed_at', datetime.now(timezone.utc).isoformat())
    
    def verify_integrity(self, provided_hash: str) -> bool:
        """Verify file integrity using SHA-256 hash."""
        if not self.sha256:
            return False
        return self.sha256.lower() == provided_hash.lower()
    
    @classmethod
    def validate_sha256(cls, sha256: str) -> bool:
        """
        Validate SHA256 hash format using compiled regex pattern.
        
        Args:
            sha256: SHA256 hash string to validate
            
        Returns:
            True if valid SHA256 format, False otherwise
        """
        if not sha256:
            return False
        return SHA256_PATTERN.match(sha256) is not None
    
    @property
    def s3_full_path(self) -> str:
        """Get full S3 path (bucket/key)."""
        return f"{self.s3_bucket}/{self.s3_key}"
    
    @property
    def is_invoice(self) -> bool:
        """Check if artefact is an invoice (requires special handling)."""
        return self.type == 'invoice'
    
    @property
    def is_versioned(self) -> bool:
        """Check if artefact has S3 version ID."""
        return bool(self.version_id)
    
    def _sanitize_s3_tag_value(self, value: str) -> str:
        """
        Sanitize a value for use as an S3 tag.
        
        S3 tag constraints:
        - Maximum key length: 128 characters
        - Maximum value length: 256 characters
        - Allowed characters: Letters, numbers, spaces, and +-=._:/@
        """
        if not value:
            return ""
        
        # Convert to string if not already
        value = str(value)
        
        # Replace invalid characters with underscore using compiled pattern
        sanitized = S3_TAG_SANITIZE_PATTERN.sub('_', value)
        
        # Truncate to max length (256 characters for values)
        return sanitized[:256]
    
    def get_s3_tags(self) -> dict:
        """Get S3 object tags for this artefact."""
        tags = {
            'job_id': str(self.job_id),
            'artefact_id': str(self.id),
            'type': self.type,
            'created_by': str(self.created_by),
            'sha256': self.sha256
        }
        
        if self.machine_id:
            tags['machine_id'] = str(self.machine_id)
        
        if self.post_processor:
            # Sanitize post_processor to comply with S3 tag constraints
            tags['post_processor'] = self._sanitize_s3_tag_value(self.post_processor)
            
        return tags
    
    def get_retention_metadata(self) -> dict:
        """Get retention metadata for invoices."""
        if not self.is_invoice:
            return {}
            
        # Turkish financial regulation: 10 years retention for invoices
        return {
            'retention_years': 10,
            'compliance': 'Turkish_Tax_Law',
            'retention_mode': 'COMPLIANCE',
            'legal_hold': False
        }