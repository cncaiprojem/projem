{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Monorepo, Containers and CI/CD Setup",
        "description": "Initialize monorepo and containerized dev stack for backend (FastAPI), workers (Celery), frontend (Next.js), DB (PostgreSQL), queue (RabbitMQ), cache/rate-limit (Redis), object storage (MinIO), FreeCAD/CAMotics/FFmpeg utilities. Configure secure env, base logging, and CI/CD.",
        "details": "Stack choices (direct path, mature tooling):\n- Backend: Python 3.11, FastAPI ^0.115, Pydantic v2, SQLAlchemy 2.0, Alembic 1.13, Uvicorn, structlog 24.x\n- Workers/Queue: Celery 5.4 + RabbitMQ 3.13 (DLQ via x-dead-letter-exchange), Celery Beat for scheduled jobs (license reminders)\n- Cache/Rate-limit/CSRF: Redis 7.2\n- DB: PostgreSQL 17.6\n- Object Storage: MinIO RELEASE.2024-xx (MinIO Python SDK minio==7.2.7)\n- CAD/CAM: FreeCADCmd (FreeCAD 1.1.x) in isolated container; CAMotics 1.2.x CLI; FFmpeg 6.x; optional ClamAV for malware scan\n- Frontend: Next.js 15.4.0 (React 19, TypeScript 5), TanStack Query v5, react-hook-form v7, MUI v5, i18next, Zustand\n- SIEM/log shipping: JSON logs to stdout; Fluent Bit to SIEM (OpenSearch/Splunk) later\nRepo and infra:\n- Structure: /apps/api (FastAPI), /apps/workers (Celery tasks), /apps/web (Next.js), /infra (docker, compose, k8s manifests later)\n- docker-compose.dev.yml services: api, workers, beat, postgres, redis, rabbitmq, minio, createbuckets, freecad (with entry FreeCADCmd), camotics, ffmpeg (as utility), clamav (optional)\n- Security: no dev bypass in prod images; .env templates; secrets via .env + Doppler/Vault later. Enforce TLS at ingress in prod.\n- CI (GitHub Actions):\n  - Backend: ruff/mypy, pytest, coverage gate; build/push images with SBOM (syft) and signatures (cosign)\n  - Frontend: eslint, typecheck, vitest, Playwright e2e smoke\n- Pre-commit hooks: black, ruff, isort, prettier\n- Base logging: structlog JSON, request_id correlation middleware, OpenTelemetry SDK stubs.\nPseudocode (compose excerpt):\n- version: \"3.9\"\n- services:\n  - api: build apps/api, env_file .env, depends on postgres redis rabbitmq minio\n  - workers: build apps/workers, command: celery -A app.celery_app worker -Q default,cam,sim -O fair\n  - beat: celery beat for schedules\n  - postgres, redis, rabbitmq (with DLX), minio (MINIO_ROOT_USER/PASS), freecad image (FROM freecad/freecad:1.1.x), camotics (FROM camotics/camotics:1.2)\n",
        "testStrategy": "Smoke boot all containers locally; health endpoints return 200; verify MinIO bucket creation and presigned URL generation in a dry run. CI runs lint/tests on PR. Validate FreeCADCmd, CAMotics, FFmpeg versions available in containers. Security: Trivy scan images, check no secrets in repo. E2E: Playwright hits login page and fetches health.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Monorepo scaffolding, conventions, pre-commit and onboarding",
            "description": "Initialize monorepo structure (/apps, /infra), repository conventions, pre-commit hooks, EditorConfig, Makefile/Taskfile, and onboarding docs.",
            "dependencies": [],
            "details": "Goals:\n- Create a clean monorepo with /apps and /infra, consistent lint/format, and fast developer onboarding.\n\nSteps:\n- Create directories: /apps/api, /apps/workers, /apps/web, /infra/docker, /infra/compose, /infra/rabbitmq, /infra/minio, /scripts, /.github/workflows.\n- Add base files: README.md, LICENSE, .gitignore (Python, Node, Docker, OS), .editorconfig.\n- Pre-commit: install and configure black, ruff, isort, prettier, end-of-file-fixer, trailing-whitespace.\n- Add Makefile/Taskfile with common targets (bootstrap, compose up/down, fmt, lint, test, ci-local, sbom, sign, scan, smoke).\n- Add CODEOWNERS (optional) and CONTRIBUTING.md.\n\nFiles to create/change:\n- README.md (top-level), CONTRIBUTING.md, /scripts/bootstrap.sh, Makefile and/or Taskfile.yml, .gitignore, .editorconfig, .pre-commit-config.yaml, CODEOWNERS.\n\nImages/tags: n/a (scaffolding only).\n\nSecurity hardening:\n- Add .gitignore entries for .env*, node_modules, venv, dist, .next, coverage, .pytest_cache.\n- Add pre-commit hook detect-private-key (optional) and check for secrets with gitleaks as an extra hook (optional).\n\nCaching and CI performance tips:\n- Centralize .cache/ dirs in repo root to improve local dev performance.\n- Document use of local Docker BuildKit cache mounts.\n\nAcceptance criteria:\n- Monorepo tree exists with agreed structure and baseline configs committed.\n- pre-commit install works and hooks run locally.\n- make help shows targets.\n\nVerification commands:\n- pre-commit install && pre-commit run --all-files\n- tree -L 3\n- make help",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": ".env templates and secret handling strategy",
            "description": "Provide environment variable templates, validation, and guidance for Doppler/Vault integration later.",
            "dependencies": [
              "1.1"
            ],
            "details": "Goals:\n- Standardize environment configuration across services; avoid committing secrets; pave path to Doppler/Vault later.\n\nSteps:\n- Create .env.example (root) with shared values and service-specific sections.\n- Create /apps/api/.env.example, /apps/workers/.env.example, /apps/web/.env.local.example when needed.\n- Add pydantic BaseSettings schema stubs in backend for validation.\n- Document secret loading order and plan for Doppler/Vault.\n\nFiles to create/change:\n- /.env.example\n- /apps/api/.env.example, /apps/workers/.env.example, /apps/web/.env.local.example\n- /apps/api/app/core/settings.py (pydantic v2 settings model)\n- /docs/secrets.md\n\nImages/tags: n/a.\n\nSecurity hardening:\n- Ensure .env* is ignored in .gitignore except *.example templates.\n- Document production: inject via CI/CD or secret store; never bake secrets in images.\n\nCaching and CI performance tips:\n- Use dotenv-linter in pre-commit (optional) to avoid drift.\n\nAcceptance criteria:\n- Example env files exist and load successfully in dev; missing required vars fail fast.\n\nVerification commands:\n- python -c \"from app.core.settings import Settings; print(Settings().model_dump())\" (from /apps/api)\n- grep -R \"SECRET\" -n . to ensure no real secrets present",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "FastAPI backend skeleton with health and DB/Alembic base",
            "description": "Create FastAPI app with /healthz, basic routes structure, SQLAlchemy/Alembic setup, Uvicorn config.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Goals:\n- Bootable API with health endpoint; ready for DB integration and logging/tracing later.\n\nSteps:\n- Initialize /apps/api with pyproject.toml (Python 3.11), dependencies: fastapi ^0.115, uvicorn[standard], pydantic v2, sqlalchemy 2.x, alembic 1.13, psycopg[binary], structlog, opentelemetry-api/sdk (stubs), httpx (tests), pytest.\n- Create app package: app/main.py (FastAPI instance), app/api/routes.py, app/core/settings.py, app/db/session.py (SQLAlchemy engine/session), app/db/base.py, app/middleware/__init__.py.\n- Add Alembic: alembic.ini, /apps/api/alembic/ env.py, versions/ README; base migration.\n- Add /healthz route returning 200 and build info (git sha env injected later).\n- Add pytest scaffolding and a simple test_health.py.\n\nFiles to create/change:\n- /apps/api/pyproject.toml, /apps/api/app/... (as above), /apps/api/alembic/..., /apps/api/tests/test_health.py\n\nImages/tags: n/a (built in Dockerfiles task).\n\nSecurity hardening:\n- Enforce uvicorn --proxy-headers and forwarded allow list to avoid header spoofing in prod configs.\n- Validate settings with strict Pydantic model; default to secure values.\n\nCaching and CI performance tips:\n- Use uv loop and httptools only in prod; dev can hot-reload outside container.\n\nAcceptance criteria:\n- FastAPI boots locally (uvicorn app.main:app) and GET /healthz returns 200 JSON.\n- Alembic upgrade head runs on empty DB.\n\nVerification commands:\n- cd apps/api && uvicorn app.main:app --reload\n- curl http://localhost:8000/healthz\n- alembic upgrade head",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Celery workers + Celery Beat wiring",
            "description": "Set up Celery app for workers and scheduled jobs (Beat), queues, and basic task placeholders.",
            "dependencies": [
              "1.1",
              "1.2",
              "1.3"
            ],
            "details": "Goals:\n- Workers and Beat processes that connect to RabbitMQ/Redis with placeholder tasks and schedule.\n\nSteps:\n- In /apps/workers, create pyproject.toml with celery 5.4, kombu, redis, structlog, opentelemetry-api/sdk, pydantic.\n- Create app/celery_app.py configuring broker (amqp), backend (redis optional), task_queues for default, cam, sim, model, report, erp; enable acks_late and prefetch; JSON serialization only.\n- Create tasks modules with a sample noop task and version check task.\n- Configure Celery Beat with a sample schedule (e.g., license reminders placeholder) in app/beat.py.\n- Add unit tests for task import and simple run.\n\nFiles to create/change:\n- /apps/workers/app/celery_app.py, app/tasks/__init__.py, app/tasks/sample.py, app/beat.py, /apps/workers/pyproject.toml, /apps/workers/tests/test_tasks.py\n\nImages/tags: n/a (built in Dockerfiles task).\n\nSecurity hardening:\n- Force task_routes and queue names explicitly; only allow JSON content.\n- Set broker heartbeat and TCP keepalive; configure visibility_timeout adequate for tasks.\n\nCaching and CI performance tips:\n- Separate requirements into base/dev to keep prod images minimal.\n\nAcceptance criteria:\n- Celery can start (worker and beat) and register queues without error.\n\nVerification commands:\n- cd apps/workers && python -c \"from app.celery_app import celery_app; print(celery_app.control.ping())\"",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Next.js frontend skeleton with health page",
            "description": "Initialize Next.js 15.4.0 app with TypeScript, ESLint, TanStack Query, MUI, i18next, Zustand, and a /healthz route.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Goals:\n- Bootable Next.js app with baseline tooling and a health page for smoke tests.\n\nSteps:\n- Create /apps/web with Next.js 15.4.0 (create-next-app with TS) and add dependencies: @tanstack/react-query, @mui/material, i18next, react-i18next, zustand, zod, eslint configs, vitest, playwright.\n- Add /app/healthz/route.ts returning 200 JSON {status:\"ok\"} or a simple page /healthz.\n- Configure ESLint and TypeScript strict mode; set up vitest config and a trivial test.\n\nFiles to create/change:\n- /apps/web/package.json, tsconfig.json, next.config.js, .eslintrc.js, vitest.config.ts, playwright.config.ts, app/healthz/page.tsx or route.ts.\n\nImages/tags: n/a (built in Dockerfiles task).\n\nSecurity hardening:\n- Set React StrictMode; configure Content Security Policy headers in next.config.js (stubs).\n\nCaching and CI performance tips:\n- Use .npmrc with npm ci and cache in CI; consider pnpm for speed later.\n\nAcceptance criteria:\n- next dev runs and /healthz returns 200 in dev or renders page.\n\nVerification commands:\n- cd apps/web && npm install && npm run dev\n- curl -I http://localhost:3000/healthz",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Base logging (structlog JSON), request_id correlation, OpenTelemetry stubs",
            "description": "Implement JSON logging with structlog, request ID middleware, and OTel SDK stubs for api and workers.",
            "dependencies": [
              "1.3",
              "1.4"
            ],
            "details": "Goals:\n- Consistent JSON logs with request/task correlation IDs and future-ready tracing hooks.\n\nSteps:\n- In /apps/api: add middleware to inject X-Request-ID (generate if missing), log incoming/outgoing with structlog; configure uvicorn to use JSON.\n- In /apps/workers: configure structlog processors, attach task_id/request_id where present; add logging in task base class.\n- Add OTel stubs: initialize tracer provider from env; no exporter by default; integrate with FastAPI and Celery via instrumentation stubs disabled by default.\n\nFiles to create/change:\n- /apps/api/app/core/logging.py, app/middleware/request_id.py, updates to app/main.py.\n- /apps/workers/app/core/logging.py, updates to app/celery_app.py.\n\nImages/tags: n/a.\n\nSecurity hardening:\n- Redact sensitive fields in logs; use structlog filtering processor.\n\nCaching and CI performance tips:\n- Avoid logging noise in tests; set log level via env.\n\nAcceptance criteria:\n- Logs are JSON, include request_id and service name; workers include task_id.\n\nVerification commands:\n- Run API and curl a route with header X-Request-ID=abc; observe logs.\n- Trigger a Celery task and inspect stdout logs.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Dockerfiles for API and Workers (Python 3.11) with hardening",
            "description": "Create secure, multi-stage Dockerfiles for FastAPI (Uvicorn) and Celery workers/beat.",
            "dependencies": [
              "1.3",
              "1.4",
              "1.6"
            ],
            "details": "Goals:\n- Build minimal, non-root images for api, workers, and beat with cache-efficient layers.\n\nSteps:\n- Use python:3.11-slim-bookworm as base; enable BuildKit; multi-stage: builder (install build deps, compile wheels) -> runtime.\n- Add system deps: gcc, build-essential (builder), libpq5, curl, netcat; clean apt lists.\n- Copy pyproject.toml and lock; pip install --no-cache-dir with --require-hashes (lock recommended) to /usr/local.\n- Create non-root user app:app; set WORKDIR /app; copy source; set proper permissions; set PYTHONDONTWRITEBYTECODE=1, PYTHONUNBUFFERED=1.\n- API CMD: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 2.\n- Workers CMD: celery -A app.celery_app worker -Q default,cam,sim,model,report,erp -O fair; Beat CMD: celery -A app.celery_app beat.\n\nFiles to create/change:\n- /apps/api/Dockerfile\n- /apps/workers/Dockerfile\n\nImages/tags:\n- apps-api: local build from python:3.11-slim-bookworm\n- apps-workers: local build from python:3.11-slim-bookworm\n\nSecurity hardening:\n- Run as non-root; drop capabilities; set read-only root FS with writable /tmp if possible; do not expose dev-only env in prod image.\n- Pin package versions; verify wheels; no SSH keys copied.\n\nCaching and CI performance tips:\n- Separate requirements install from source copy; use pip cache mount: --mount=type=cache,target=/root/.cache/pip.\n\nAcceptance criteria:\n- Images build successfully; containers start and respond; hadolint passes.\n\nVerification commands:\n- docker build -t apps-api ./apps/api\n- docker run --rm -p 8000:8000 apps-api curl -s localhost:8000/healthz\n- docker build -t apps-workers ./apps/workers",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Dockerfile for Web (Next.js 15.4.0) with hardening",
            "description": "Create multi-stage Dockerfile for Next.js app with separate builder/runtime and non-root execution.",
            "dependencies": [
              "1.5"
            ],
            "details": "Goals:\n- Produce small runtime image using Next.js standalone output; run as non-root.\n\nSteps:\n- Builder: node:20-alpine; install deps with npm ci; run next build; output standalone.\n- Runner: node:20-alpine; add non-root node user; copy .next/standalone and .next/static and public; set PORT=3000; CMD: node server.js.\n- Add healthcheck route mapping to /healthz.\n\nFiles to create/change:\n- /apps/web/Dockerfile\n\nImages/tags:\n- apps-web: local build from node:20-alpine\n\nSecurity hardening:\n- Run as non-root; set NODE_ENV=production; enable read-only root FS if possible; set sensible ulimit in compose.\n\nCaching and CI performance tips:\n- Cache node_modules via Docker build cache (mount=cache) and CI cache.\n\nAcceptance criteria:\n- Image builds and serves app; /healthz returns 200.\n\nVerification commands:\n- docker build -t apps-web ./apps/web\n- docker run --rm -p 3000:3000 apps-web curl -I localhost:3000/healthz",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Utility images: FreeCADCmd, CAMotics, FFmpeg, optional ClamAV",
            "description": "Reference or wrap utility images for FreeCAD 1.1.x (FreeCADCmd), CAMotics 1.2.x, FFmpeg 6.x, and optional ClamAV.",
            "dependencies": [
              "1.1"
            ],
            "details": "Goals:\n- Provide containerized tooling images accessible to workers via compose.\n\nSteps:\n- FreeCAD: Dockerfile FROM freecad/freecad:1.1.x; set ENTRYPOINT [\"FreeCADCmd\"]; include a non-root user; document usage.\n- CAMotics: use camotics/camotics:1.2 directly or thin wrapper setting ENTRYPOINT [\"camotics\"]\n- FFmpeg: use jrottenberg/ffmpeg:6-slim; ensure hardware-agnostic; ENTRYPOINT [\"ffmpeg\"].\n- ClamAV (optional): clamav/clamav:1.3; expose freshclam and clamscan; mount DB volume.\n\nFiles to create/change:\n- /infra/docker/freecad/Dockerfile\n- /infra/docker/camotics/Dockerfile (optional)\n- /infra/docker/ffmpeg/Dockerfile (optional if using upstream)\n\nImages/tags:\n- freecad/freecad:1.1.x (ENTRY FreeCADCmd)\n- camotics/camotics:1.2\n- jrottenberg/ffmpeg:6-slim\n- clamav/clamav:1.3\n\nSecurity hardening:\n- Run as non-root where feasible; restrict capabilities; read-only FS; scan images with Trivy in CI.\n\nCaching and CI performance tips:\n- Pin tags; avoid latest; leverage Buildx cache for wrappers.\n\nAcceptance criteria:\n- Containers run and print version info.\n\nVerification commands:\n- docker run --rm freecad/freecad:1.1.x FreeCADCmd --version\n- docker run --rm camotics/camotics:1.2 camotics --version\n- docker run --rm jrottenberg/ffmpeg:6-slim -version\n- docker run --rm clamav/clamav:1.3 clamscan --version",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "docker-compose.dev.yml with services, networks, healthchecks, and smoke script",
            "description": "Compose file wiring all services with healthchecks, networks, volumes, and a smoke test script to validate the stack.",
            "dependencies": [
              "1.2",
              "1.7",
              "1.8",
              "1.9",
              "1.11",
              "1.12"
            ],
            "details": "Goals:\n- One-command local dev stack up; automated smoke validation.\n\nSteps:\n- Create /infra/compose/docker-compose.dev.yml version \"3.9\" with services: api, workers, beat, postgres, redis, rabbitmq (management), minio, createbuckets, freecad, camotics, ffmpeg, clamav (optional), web.\n- Define networks: backend, frontend; attach api/workers to backend; web to frontend (and backend if it calls api).\n- Add volumes: pg_data, minio_data, minio_config, clamav_db.\n- Add env_file: .env; set restart, user: non-root where applicable; resource hints (mem_limit) for dev.\n- Healthchecks:\n  - postgres: pg_isready -U $POSTGRES_USER\n  - redis: redis-cli ping\n  - rabbitmq: rabbitmq-diagnostics -q ping\n  - minio: curl -sf http://localhost:9000/minio/health/ready inside container\n  - api: curl -sf http://localhost:8000/healthz\n  - web: curl -sf http://localhost:3000/healthz\n  - workers/beat use CMD-SHELL true to ensure process stays healthy\n- Add depends_on with condition: service_healthy where supported.\n- Add /scripts/smoke.sh that waits for health and exercises key calls (healthz, MinIO presign dry-run, version checks for utilities).\n\nFiles to create/change:\n- /infra/compose/docker-compose.dev.yml\n- /scripts/smoke.sh (executable)\n\nImages/tags:\n- postgres:16\n- redis:7.2-alpine\n- rabbitmq:3.13-management\n- minio/minio:RELEASE.2024-XX\n- minio/mc:RELEASE.2024-XX (for createbuckets)\n- utility images from prior task; apps-api, apps-workers, apps-web\n\nSecurity hardening:\n- No prod-only bypasses; expose only necessary ports; set RABBITMQ_DEFAULT_USER/PASS and MINIO_ROOT_USER/PASS from .env; read-only FS for api/workers where possible; drop ALL capabilities.\n\nCaching and CI performance tips:\n- Use docker compose build --parallel; enable BuildKit; mount bind volumes for fast iteration.\n\nAcceptance criteria:\n- docker compose up -d brings all services healthy; smoke script passes.\n\nVerification commands:\n- docker compose -f infra/compose/docker-compose.dev.yml up -d --build\n- ./scripts/smoke.sh",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "RabbitMQ with DLX configuration for Celery queues",
            "description": "Configure RabbitMQ policies/definitions for dead-letter exchanges and Celery queues.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Goals:\n- Ensure each queue has a DLX and *_dlq dead-letter queue for failures.\n\nSteps:\n- Use rabbitmq:3.13-management.\n- Create /infra/rabbitmq/definitions.json defining exchanges/queues:\n  - Exchanges: default, cam, sim, model, report, erp (direct); *_dlx (fanout or direct) and *_dlq queues bound to *_dlx.\n  - Queues: each primary with arguments: x-dead-letter-exchange: <queue>.dlx.\n- Mount definitions.json and load on boot via RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS: -rabbitmq_management load_definitions \"/etc/rabbitmq/definitions.json\".\n- Document Celery queue names and ensure app.celery_app aligns.\n\nFiles to create/change:\n- /infra/rabbitmq/definitions.json\n- Update compose service rabbitmq to mount file and set env.\n\nImages/tags:\n- rabbitmq:3.13-management\n\nSecurity hardening:\n- Create non-default user from .env; disable guest; restrict management UI to local network in compose.\n\nCaching and CI performance tips:\n- Keep definitions minimal; use CLI rabbitmqadmin for updates if needed.\n\nAcceptance criteria:\n- Queues and DLQs exist; messages dead-letter on rejection.\n\nVerification commands:\n- docker compose exec rabbitmq rabbitmqctl list_queues name arguments\n- Publish a test message with x-death by rejecting and see it in *_dlq",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "MinIO and bucket bootstrap job",
            "description": "Set up MinIO service and a bootstrap job to create buckets and policies, verify presigned URL generation.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Goals:\n- Have artefacts/logs/reports/invoices buckets with versioning and lifecycle placeholders; demonstrate presign.\n\nSteps:\n- MinIO service using minio/minio:RELEASE.2024-XX with MINIO_ROOT_USER/PASS from .env.\n- Create createbuckets job using minio/mc:RELEASE.2024-XX that waits for MinIO, then:\n  - mc alias set local http://minio:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD\n  - mc mb --ignore-existing local/artefacts local/logs local/reports local/invoices\n  - mc version enable local/artefacts\n  - (optional) mc ilm add for lifecycle rules placeholders\n- In /apps/api add a tiny util to generate a presigned PUT and GET using minio==7.2.7; dry-run in smoke script.\n\nFiles to create/change:\n- /infra/minio/createbuckets.sh (entrypoint for createbuckets container)\n- /apps/api/app/services/s3.py (MinIO client wrapper)\n\nImages/tags:\n- minio/minio:RELEASE.2024-XX\n- minio/mc:RELEASE.2024-XX\n\nSecurity hardening:\n- Use unique creds per environment; do not expose MinIO publicly in dev; TLS termination at ingress in prod (documented).\n\nCaching and CI performance tips:\n- Keep mc script idempotent; skip work if buckets exist.\n\nAcceptance criteria:\n- Buckets exist; presigned URL generation works.\n\nVerification commands:\n- docker compose exec minio mc ls local/\n- python -c \"from app.services.s3 import client; print(client.presign_put('artefacts','smoke.txt',60))\" (adjust path)",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "GitHub Actions: Backend CI/CD with SBOM, signing, and scanning",
            "description": "Set up backend workflows: lint (ruff), format check (black), typecheck (mypy), tests (pytest) with coverage gate, build/push images, generate SBOM (syft), sign (cosign), scan (Trivy).",
            "dependencies": [
              "1.3",
              "1.4",
              "1.7"
            ],
            "details": "Goals:\n- Automated quality gates and secure supply chain for API and Workers images.\n\nSteps:\n- Create .github/workflows/backend.yml with jobs:\n  - lint-type-test: setup-python 3.11, cache pip, ruff check, black --check, mypy, pytest -q --cov with threshold (e.g., 80%).\n  - build-and-push: on main tag/push; setup QEMU/Buildx; docker/login; docker buildx build for apps/api and apps/workers with tags ghcr.io/ORG/api:SHA and workers:SHA; push.\n  - sbom-sign-scan: run anchore/syft to generate SBOMs; store as artifacts; cosign sign (keyless OIDC) images; aquasecurity/trivy image scan with --severity HIGH,CRITICAL and fail on critical.\n- Use actions/cache for pip and Docker Buildx inline cache.\n\nFiles to create/change:\n- /.github/workflows/backend.yml\n\nImages/tags:\n- Uses python:3.11 for CI; builds apps-api and apps-workers images; syft: latest; cosign: latest; trivy: latest.\n\nSecurity hardening:\n- Enable GitHub OIDC for cosign keyless; restrict workflow permissions: contents: read, id-token: write, packages: write.\n- Fail builds on critical vulnerabilities; allow override via allowlist file if needed.\n\nCaching and CI performance tips:\n- actions/setup-python with cache: 'pip'; docker buildx --cache-from/--cache-to.\n\nAcceptance criteria:\n- PRs run lint/type/tests; main builds and pushes images; SBOM artifacts published; images signed; Trivy scan passes or fails with clear output.\n\nVerification commands:\n- gh run watch (from PR)\n- cosign verify ghcr.io/ORG/api:SHA",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "GitHub Actions: Frontend CI with lint, typecheck, tests, and Playwright smoke",
            "description": "Set up frontend workflow: ESLint, typecheck, vitest, build, and Playwright e2e smoke using service containers.",
            "dependencies": [
              "1.5",
              "1.8"
            ],
            "details": "Goals:\n- Automated quality gates and basic e2e smoke for web.\n\nSteps:\n- Create .github/workflows/frontend.yml with jobs:\n  - lint-type-test: setup Node 20; cache npm; npm ci; npm run lint; npm run typecheck; npm run test:unit (vitest --run).\n  - build: npm run build to ensure prod build works.\n  - e2e-smoke: use Playwright; start app (npm run start) and run a simple test hitting /healthz; optionally start API with a lightweight mock or use Next route.\n\nFiles to create/change:\n- /.github/workflows/frontend.yml\n- Ensure package.json has scripts: lint, typecheck, test:unit, build, start, test:e2e\n\nImages/tags:\n- node:20 in CI; browsers via playwright action.\n\nSecurity hardening:\n- Pin action versions; least-privileged GHA permissions; sanitize PR logs.\n\nCaching and CI performance tips:\n- actions/setup-node with cache: 'npm'; reuse Playwright browser cache with actions/download-artifact (optional).\n\nAcceptance criteria:\n- PRs run ESLint/typecheck/tests; e2e smoke passes.\n\nVerification commands:\n- gh run watch (from PR)\n- npm run test:e2e locally to replicate",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Database Schema and Migrations",
        "description": "Implement PostgreSQL schema per PRD with indices, constraints, and seed data (tools). Add audit hash-chain capability and idempotency constraints.",
        "details": "Use Alembic for migrations; SQLAlchemy models mirror PRD tables: users, sessions, licenses, invoices, payments, models, jobs, cam_runs, sim_runs, artefacts, machines, materials, notifications, erp_mes_sync, audit_logs, security_events, tools.\nKey constraints/indexes:\n- Unique: users.email, users.phone; sessions.refresh_token_hash; jobs.idempotency_key; artefacts.s3_key; invoices.number; payments.provider_ref\n- FKs with ON DELETE: RESTRICT for most, artefacts (CASCADE on job delete)\n- Indices per PRD (e.g., jobs: user_id, type, status, created_at; licences: user_id, status, ends_at)\n- JSONB columns with GIN indexes where filtered often (e.g., jobs.metrics, params if needed)\n- Check constraints (currency in ['TRY', …] when multi-currency flag enabled)\n- tools table enum types for tool.type and material\nAudit chain:\n- audit_logs.chain_hash = sha256(prev_chain_hash || canonical_json(record))\n- Store prev hash (last hash overall or per user scope). Maintain in app layer within transaction.\nSeeds:\n- machines/materials minimal; tools: '6mm Carbide Endmill (4F)' and '10mm Drill HSS'\nPseudocode (Alembic example):\n- op.create_table('jobs', ... idempotency_key=sa.String, sa.UniqueConstraint('idempotency_key'))\n- op.create_index('ix_jobs_status_created', 'jobs', ['status','created_at'])\n- Seed tools via data migration inserting JSON specified in PRD.\n",
        "testStrategy": "Run alembic upgrade head on clean DB. Verify constraints by attempting duplicates (expect errors). Confirm FK behaviors. Check hash-chain creation by inserting two audit logs and verifying deterministic chain. Query plans show indexes used. Unit tests for models; migration downgrade/upgrade cycle validated.",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Finalize ERD, enums, and canonicalization rules",
            "description": "Produce the final PostgreSQL ERD and enum/type definitions per PRD, and define canonical JSON and audit hash-chain scopes.",
            "dependencies": [],
            "details": "- Deliverables: ERD (tables/columns/FKs), enum specs for tools.type and tools.material, JSONB usage map, timestamp and soft-delete policy, naming conventions.\n- Map PRD entities: users, sessions, licenses, invoices, payments, models, jobs, cam_runs, sim_runs, artefacts, machines, materials, notifications, erp_mes_sync, audit_logs, security_events, tools.\n- Define JSONB fields (e.g., jobs.metrics, jobs.params) and which get GIN indexes.\n- Enumerate unique constraints and indexes per PRD (users.email/phone, sessions.refresh_token_hash, jobs.idempotency_key, artefacts.s3_key, invoices.number, payments.provider_ref; indexes on jobs user_id/type/status/created_at; licenses user_id/status/ends_at).\n- Audit chain scope: global or per-scope (e.g., per user or job). Specify fields scope_type and scope_id semantics.\n- Canonical JSON rules: UTF-8; stable key sort; no whitespace; numbers as strings with fixed precision if required or use JSONB deterministic dumps; exclude non-deterministic fields; nulls explicit.\n- Idempotency key policy for jobs (string length, charset, uniqueness, retry semantics).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Alembic setup and base migration",
            "description": "Set up Alembic for PostgreSQL 17.6 with SQLAlchemy 2.0 models and naming conventions; create base revision.",
            "dependencies": [
              "2.1"
            ],
            "details": "- Configure alembic.ini and env.py (UTC timestamps, timezone-aware, async not required).\n- Apply naming convention for constraints and indexes for reproducible diffs.\n- Wire metadata from SQLAlchemy models; enable render_as_batch=False for Postgres.\n- Create initial empty base revision (\"base\") to anchor subsequent DDL.\n- Add helper utilities for creating enums, GIN indexes, and check constraints.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create core tables: users, sessions, licenses, models, jobs",
            "description": "Implement Alembic migration to create core domain tables with required PKs, FKs, uniques, and base indexes.",
            "dependencies": [
              "2.2"
            ],
            "details": "- users: id, email (unique), phone (unique), role, status, locale, created_at/updated_at.\n- sessions: id, user_id FK (RESTRICT), refresh_token_hash (unique), device_fingerprint, last_used_at, expires_at; index on user_id, expires_at.\n- licenses: id, user_id FK (RESTRICT), plan, status, starts_at, ends_at; index on (user_id), (status, ends_at).\n- models: id, user_id FK (RESTRICT), type, params JSONB, metrics JSONB, created_at; optional GIN on params if filtered.\n- jobs: id, user_id FK (RESTRICT), type, status, params JSONB, metrics JSONB, idempotency_key (unique), created_at, updated_at; indexes: (status, created_at), user_id, type; GIN on metrics (and params if needed).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create operational tables: cam_runs, sim_runs, artefacts, notifications, erp_mes_sync",
            "description": "Add execution/operations-related tables with correct FKs and cascade behaviors.",
            "dependencies": [
              "2.3"
            ],
            "details": "- cam_runs: id, job_id FK (RESTRICT), machine_id FK (RESTRICT), params JSONB, metrics JSONB, status, created_at; indexes: job_id, status.\n- sim_runs: id, job_id FK (RESTRICT), params JSONB, metrics JSONB, status, created_at; indexes: job_id, status.\n- artefacts: id, job_id FK (CASCADE on job delete), type, s3_key (unique), size_bytes, sha256, mime, meta JSONB, created_at; indexes: job_id, type; optional GIN on meta.\n- notifications: id, user_id FK (RESTRICT), type, payload JSONB, read_at, created_at; indexes: user_id, type, read_at.\n- erp_mes_sync: id, external_id, entity_type, entity_id, status, last_sync_at, payload JSONB; indexes: entity_type, status.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create billing tables: invoices and payments",
            "description": "Implement billing-related tables with monetary and provider references and currency checks.",
            "dependencies": [
              "2.3"
            ],
            "details": "- invoices: id, user_id FK (RESTRICT), number (unique), amount_cents, currency, status, issued_at, due_at, meta JSONB; indexes: user_id, status, issued_at.\n- payments: id, invoice_id FK (RESTRICT), provider, provider_ref (unique), amount_cents, currency, status, paid_at, meta JSONB; indexes: invoice_id, status, paid_at.\n- Currency check constraint: currency in allowed set; allow multi-currency when current_setting('app.multi_currency', true) = 'on' else enforce 'TRY'.\n- Ensure consistent money precision and non-negative checks.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Create security and audit tables with hash-chain",
            "description": "Add audit_logs (hash-chain) and security_events tables and supporting constraints/indexes.",
            "dependencies": [
              "2.2"
            ],
            "details": "- audit_logs: id, scope_type, scope_id, actor_user_id FK (RESTRICT, nullable), event_type, payload JSONB, prev_chain_hash, chain_hash, created_at.\n- Constraints: chain_hash and prev_chain_hash are 64-char hex; NOT NULL for chain_hash; CHECK on hex format.\n- Indexes: (scope_type, scope_id, created_at), event_type; GIN on payload if filtered.\n- App responsibility: compute chain_hash = sha256(prev_chain_hash || canonical_json(payload)) within the same transaction; store prev_chain_hash from last scope record (or global) as per ERD rules.\n- security_events: id, user_id FK (RESTRICT), type, ip, ua, created_at; indexes: user_id, type, created_at.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Apply global constraints and performance indexes",
            "description": "Ensure all uniques, FKs with correct ON DELETE rules, JSONB GIN, and check constraints are in place.",
            "dependencies": [
              "2.3",
              "2.4",
              "2.5",
              "2.6"
            ],
            "details": "- Uniques: users.email, users.phone, sessions.refresh_token_hash, jobs.idempotency_key, artefacts.s3_key, invoices.number, payments.provider_ref.\n- FKs: RESTRICT by default; artefacts.job_id CASCADE; verify others match PRD.\n- Indexes: jobs(user_id), jobs(type), jobs(status, created_at); licenses(user_id), licenses(status, ends_at); add missing per PRD.\n- JSONB GIN indexes: jobs.metrics, jobs.params (if filtered), models.params, artefacts.meta as needed.\n- Check constraints: currency rules; non-negative amounts; any domain-specific checks per PRD.\n- Add comments to schema objects for maintainability.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Seed data via data migration",
            "description": "Create idempotent data migration to seed machines, materials, and tools.",
            "dependencies": [
              "2.7"
            ],
            "details": "- Minimal machines and materials per PRD; ensure stable primary keys or natural keys.\n- tools: insert '6mm Carbide Endmill (4F)' and '10mm Drill HSS' with proper enums (type, material) and metadata.\n- Use INSERT ... ON CONFLICT DO NOTHING or upsert on natural key to make seeding idempotent.\n- Wrap in a separate migration file; provide downgrade that deletes only seeded rows by natural key.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Migration and integrity test suite",
            "description": "Implement tests for upgrade/downgrade, constraints, FK behaviors, audit-chain determinism, and query plans.",
            "dependencies": [
              "2.6",
              "2.7",
              "2.8"
            ],
            "details": "- Run alembic upgrade head on clean DB and downgrade to base; ensure no residual objects.\n- Constraints: attempt duplicates for each unique; expect errors. Validate currency checks with GUC app.multi_currency on/off.\n- FKs: verify RESTRICT on delete for users with sessions; verify CASCADE for artefacts on job delete.\n- Audit chain: insert two audit_logs within a tx; verify chain_hash = sha256(prev || canonical_json(payload)) deterministically.\n- Query plans: EXPLAIN (ANALYZE, BUFFERS) on key queries to confirm index usage (jobs status+created_at; licenses status+ends_at; JSONB GIN probe).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "SQLAlchemy model parity and documentation",
            "description": "Ensure SQLAlchemy models mirror the DB schema and document idempotency, canonical JSON, and rollback strategy.",
            "dependencies": [
              "2.7",
              "2.9"
            ],
            "details": "- Validate ORM models against DB via autogenerate producing empty diffs.\n- Add model-level constraints/validators for idempotency_key, enums, and JSONB fields.\n- Document canonical JSON rules and audit chain responsibilities; include sample helper in app layer.\n- Provide rollback strategy notes for each migration and test fixtures for core entities (users, jobs, audit logs).\n- Include query examples demonstrating index usage and idempotent job creation semantics.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Auth, Sessions, RBAC, MFA, OIDC, Magic Link + Frontend Guards",
        "description": "Implement registration, login (email/password, Google OIDC, magic link), JWT access (30m) and refresh tokens (7d with rotation/revocation), MFA (TOTP for admin/critical actions), RBAC (user/admin). Frontend route guards, CSRF/XSS protections, rate limiting, session idle logout.",
        "details": "Backend (FastAPI):\n- Passwords: argon2-cffi 23.x with pepper and unique salts; strong policy check\n- JWT: PyJWT 2.8; access token (30m) in Authorization Bearer; refresh token as httpOnly, Secure, SameSite=Strict cookie; rotation on refresh with sessions table (refresh_token_hash unique), device_fingerprint stored\n- RBAC: role in JWT claims; scope checks per endpoint via dependency\n- OIDC (Google): Authlib 1.3; PKCE + state; store oidc sub mapping to user; logs oidc_login\n- Magic link: single-use signed token (itsdangerous or custom HMAC) TTL 15m; logged as magic_link_issued/consumed\n- MFA: pyotp + qrcode; enforce for admin or sensitive actions; backup codes table optional\n- CSRF: double-submit token for browser POST/PUT/DELETE: set csrf cookie + require X-CSRF-Token; validate when Authorization header present\n- Rate limit: fastapi-limiter + Redis (e.g., 5/min login, 30/min AI prompt)\n- Security headers: Starlette middleware (CSP, HSTS, X-Frame-Options deny); sanitize inputs; SQLAlchemy ORM prevents SQLi; PII masking in logs\n- Dev mode: env flag; if dev, bypass guards and tag outputs dev_mode=true; disabled in prod via config\n- Logging: all auth events to audit_logs + security_events with masked PII\nFrontend (Next.js):\n- i18next Turkish default; localized errors\n- React Hook Form dynamic validation\n- Auth pages for login/register, OIDC callback, magic link consumption\n- Route guard (middleware.ts): if no access token or /license/me shows expired → redirect to license page; show banner with remaining days; idle timer auto-logout\nPseudocode (refresh rotation):\n- POST /auth/token/refresh:\n  - read refresh cookie → verify, lookup session by hash → if revoked: 401\n  - issue new access + new refresh; mark old session revoked_at; insert new session\n  - set-cookie new refresh; return access\n",
        "testStrategy": "Unit: password policy, JWT create/verify, refresh rotation, MFA verification, CSRF checks. Integration: OIDC login flow with mocked Google, magic link single-use. Rate limit enforced (429). Frontend: Playwright tests for route guard redirects, idle logout, Turkish UI strings. Security: ZAP scan for XSS/CSRF; brute-force throttling verified.",
        "priority": "high",
        "dependencies": [
          1,
          2
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Password Authentication (Argon2 + Pepper + Policy)",
            "description": "Implement email/password registration and login with Argon2 hashing, pepper, unique salts, and strong password policy enforcement.",
            "dependencies": [],
            "details": "Design:\n- Use argon2-cffi 23.x (argon2id) with per-user salts and a global pepper (ENV: AUTH_PASSWORD_PEPPER).\n- Enforce strong password policy: length >= 12, upper/lower/number/symbol, block top-10k and repeated patterns.\n- Optional account lockout counter (e.g., soft lock for 15m after 10 failed attempts) in addition to rate limiting.\nAPIs:\n- POST /auth/register {email, password, name?} → 201 {user_id}. 409 if email exists.\n- POST /auth/login {email, password, device_fingerprint?, mfa_code?} → 200 {access_token, expires_in, mfa_required?}. Sets refresh cookie if MFA satisfied.\n- POST /auth/password/strength {password} → 200 {score, ok, feedback}.\n- POST /auth/password/forgot {email} → 202 {}.\n- POST /auth/password/reset {token, new_password} → 200 {}.\nCookies: N/A (refresh set by Subtask 3).\nErrors:\n- 400 ERR-AUTH-INVALID-BODY, 401 ERR-AUTH-INVALID-CREDS, 423 ERR-AUTH-LOCKED, 409 ERR-AUTH-EMAIL-TAKEN.\nAcceptance:\n- Hash uses argon2id with configured parameters; pepper required; passwords failing policy rejected.\n- Login succeeds with correct creds; fails with incorrect; lockout triggers after threshold; does not leak timing/PII.\nLogging:\n- audit_logs: user_registered, login_succeeded, login_failed (mask email), password_reset_requested, password_reset_completed.\n- security_events: excessive_login_failures, account_locked.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Sessions Table and Device Fingerprint",
            "description": "Create sessions persistence for refresh rotation with hashed refresh_token, device metadata, and revocation fields.",
            "dependencies": [],
            "details": "Design (DB/model):\n- Table sessions: id (uuid), user_id (fk users), refresh_token_hash (unique, sha256/HMAC), device_fingerprint (string), ip, user_agent, created_at, last_used_at, expires_at (7d), revoked_at, rotated_from (fk sessions.id), reason.\n- Indexes: user_id, (user_id, revoked_at IS NULL), expires_at.\n- Store only hash of refresh token; never store plaintext.\nAPIs: N/A (internal usage by auth flows).\nCookies: N/A.\nErrors: N/A.\nAcceptance:\n- Unique constraint enforced on refresh_token_hash; revoked_at prevents reuse.\n- Rotation chain (rotated_from) preserved; device_fingerprint stored when provided.\nLogging:\n- audit_logs: session_created, session_rotated, session_revoked (with session_id, masked ua/ip).\n- security_events: refresh_reuse_detected, anomalous_device_detected.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "JWT Access/Refresh Tokens with Rotation & Revocation",
            "description": "Issue 30m JWT access tokens and 7d refresh tokens stored in httpOnly cookies; implement refresh rotation, revocation, and logout endpoints.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Design:\n- PyJWT 2.8; access token: 30m exp; claims: sub (user_id), role, scopes, sid (session id), iat, exp.\n- Refresh token: random 256-bit opaque; 7d TTL; stored as httpOnly cookie; rotation on each refresh; detect reuse → revoke all in chain.\nAPIs:\n- POST /auth/token/refresh → reads refresh cookie; 200 {access_token, expires_in}; sets new refresh cookie; 401 if invalid/revoked.\n- POST /auth/logout → revoke current session; 204; clears refresh cookie.\n- POST /auth/logout/all → revoke all sessions for user; 204; clears cookie.\nCookies:\n- Name: rt; Path=/; HttpOnly; Secure; SameSite=Strict; Max-Age=604800; Domain configurable; Same attributes on rotation; clear on logout.\nErrors:\n- 401 ERR-TOKEN-INVALID, ERR-TOKEN-EXPIRED, ERR-TOKEN-REVOKED, ERR-REFRESH-REUSE.\n- 400 ERR-TOKEN-MALFORMED.\nAcceptance:\n- Refresh rotates: old session revoked_at set; new session inserted; new cookie set with correct attributes.\n- Reuse of an already-rotated refresh cookie triggers global revocation and 401.\n- Access token in Authorization: Bearer <jwt> works for protected routes.\nLogging:\n- audit_logs: token_refreshed, logout, logout_all.\n- security_events: refresh_reuse, invalid_refresh_cookie, suspicious_refresh_ip_change.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "RBAC Enforcement via FastAPI Dependencies",
            "description": "Add role- and scope-based authorization using dependency injectors that read JWT claims.",
            "dependencies": [
              "3.3"
            ],
            "details": "Design:\n- Roles: user, admin; scopes per endpoint (e.g., designs:read, designs:write).\n- Dependencies: require_auth(), require_role('admin'), require_scopes('x','y'). Reject if missing.\nAPIs:\n- Example protected: GET /admin/users (admin only), GET /me (user).\nCookies: N/A.\nErrors:\n- 401 ERR-AUTH-REQUIRED (no/invalid token), 403 ERR-RBAC-FORBIDDEN (insufficient role/scope).\nAcceptance:\n- Endpoints annotated with dependencies enforce correct access; admin-only endpoints deny user role.\n- Scope mismatch returns 403 without leaking resource existence.\nLogging:\n- security_events: rbac_forbidden (endpoint, required, provided), missing_auth_header.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Google OIDC (Authlib) with PKCE/State",
            "description": "Integrate Google OIDC sign-in with PKCE and state validation; map oidc sub to user and issue session/tokens.",
            "dependencies": [
              "3.3",
              "3.2"
            ],
            "details": "Design:\n- Authlib 1.3 OAuth2Client; use PKCE (S256) and state in server-side store; nonce in auth request.\n- Persist mapping: oidc_accounts(user_id, provider='google', sub, email_verified, picture, created_at).\nAPIs:\n- GET /auth/oidc/google/start → 302 redirect to Google (sets pkce_verifier in server store).\n- GET /auth/oidc/google/callback?code&state → 302 to FE; on success set refresh cookie and return short-lived page that exchanges for access token if needed.\nCookies:\n- Sets refresh cookie (rt) per Subtask 3 upon successful OIDC login.\nErrors:\n- 400 ERR-OIDC-STATE, 400 ERR-OIDC-NONCE, 401 ERR-OIDC-TOKEN-EXCHANGE, 409 ERR-OIDC-EMAIL-CONFLICT (if email claimed by another flow and policy forbids auto-link).\nAcceptance:\n- PKCE verifier and state validated; code exchange succeeds; new or existing user linked via sub.\n- Audit log contains oidc_login with provider, sub; PII masked.\nLogging:\n- audit_logs: oidc_login_started, oidc_login_succeeded, oidc_login_linked.\n- security_events: oidc_state_mismatch, oidc_token_exchange_failed.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Magic Link Issuance and Consumption",
            "description": "Provide passwordless login via single-use, 15-minute signed magic links with issuance and consumption endpoints.",
            "dependencies": [
              "3.3"
            ],
            "details": "Design:\n- Sign tokens with itsdangerous (TimedJSONWebSignatureSerializer) or HMAC; payload: {email, nonce, iat}.\n- Persist single-use: magic_links(id, email, nonce, issued_at, consumed_at, ip, ua).\nAPIs:\n- POST /auth/magic-link/request {email} → 202 {}; send email with URL (/auth/magic-link/consume?token=...). Rate limit applied.\n- POST /auth/magic-link/consume {token, device_fingerprint?} → 200 {access_token, expires_in}; sets refresh cookie.\nCookies:\n- On consume, set refresh cookie (rt) with Secure, HttpOnly, SameSite=Strict, Max-Age=7d.\nErrors:\n- 400 ERR-ML-MALFORMED, 401 ERR-ML-INVALID, 401 ERR-ML-EXPIRED, 409 ERR-ML-ALREADY-USED.\nAcceptance:\n- Token expires at 15m; cannot be reused; consuming creates session and returns access token.\n- Email enumeration safe: request endpoint always 202.\nLogging:\n- audit_logs: magic_link_issued, magic_link_consumed.\n- security_events: magic_link_invalid, magic_link_reuse_attempt.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "TOTP MFA (pyotp) Setup, Verify, and Backup Codes",
            "description": "Implement TOTP MFA for admins/sensitive actions with setup, verification during login, and optional backup codes.",
            "dependencies": [
              "3.1",
              "3.3"
            ],
            "details": "Design:\n- pyotp TOTP (period=30, digits=6); store mfa_secret encrypted at rest; enforce for admin role and step-up on sensitive endpoints.\n- Backup codes: 10 single-use codes (sha256 hashed) in mfa_backup_codes table.\nAPIs:\n- POST /auth/mfa/setup/start → 200 {secret_masked, otpauth_url, qr_png_base64}.\n- POST /auth/mfa/setup/verify {code} → 200 {} enables MFA.\n- POST /auth/mfa/disable {code} → 200 {}.\n- POST /auth/mfa/challenge {code or backup_code} → 200 {access_token} when login required step-up.\n- GET /auth/mfa/backup-codes → 200 {codes_plaintext_once} (generate/regenerate).\nCookies:\n- Uses refresh cookie issuance from Subtask 3 when MFA satisfied.\nErrors:\n- 401 ERR-MFA-REQUIRED, 401 ERR-MFA-INVALID, 409 ERR-MFA-ALREADY-ENABLED, 400 ERR-MFA-NOT-ENABLED.\nAcceptance:\n- Admin login without MFA triggers mfa_required; providing valid TOTP returns tokens.\n- Backup codes consume once; disabling MFA requires valid TOTP.\nLogging:\n- audit_logs: mfa_enabled, mfa_challenge_succeeded, mfa_disabled.\n- security_events: mfa_challenge_failed, backup_code_used.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "CSRF Double-Submit Protection",
            "description": "Add double-submit CSRF for browser POST/PUT/PATCH/DELETE when Authorization header present.",
            "dependencies": [
              "3.3"
            ],
            "details": "Design:\n- Issue CSRF cookie and require X-CSRF-Token header matching cookie value for state-changing browser requests.\n- Skip for non-browser clients (no cookies) and for idempotent GET/HEAD.\nAPIs:\n- GET /auth/csrf-token → 200 {} and sets csrf cookie; FE reads cookie and mirrors into header.\nCookies:\n- Name: csrf; HttpOnly=false; Secure=true; SameSite=Strict; Path=/; Max-Age=7200.\nErrors:\n- 403 ERR-CSRF-MISSING, 403 ERR-CSRF-MISMATCH.\nAcceptance:\n- Requests with valid X-CSRF-Token pass; mismatched or missing token returns 403.\n- Token rotates periodically or on login; works with refresh cookie and Authorization header.\nLogging:\n- security_events: csrf_missing, csrf_mismatch (method, path, ua masked).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Rate Limiting with Redis",
            "description": "Configure fastapi-limiter with Redis and apply per-route policies, including login 5/min and AI prompt 30/min.",
            "dependencies": [],
            "details": "Design:\n- fastapi-limiter with IP + user keying; trust X-Forwarded-For when behind proxy.\n- Policies: /auth/login 5/min, /auth/magic-link/request 3/min, /auth/token/refresh 60/min per session, AI prompt endpoints 30/min per user.\nAPIs: N/A (decorators/middleware on endpoints).\nCookies: N/A.\nErrors:\n- 429 ERR-RATE-LIMIT with Retry-After header.\nAcceptance:\n- Exceeding thresholds yields 429; counters reset after window; per-user limits respect JWT sub.\nLogging:\n- security_events: rate_limited (route, key), potential_bruteforce_detected.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Security Headers and Input Sanitization",
            "description": "Add Starlette middleware for CSP, HSTS, frame-ancestors deny, and sanitize inputs to mitigate XSS/Injection.",
            "dependencies": [],
            "details": "Design:\n- Headers: Content-Security-Policy (default-src 'self'; frame-ancestors 'none'; object-src 'none'), Strict-Transport-Security (max-age=31536000; includeSubDomains), X-Frame-Options: DENY, X-Content-Type-Options: nosniff, Referrer-Policy: no-referrer, Permissions-Policy minimal.\n- Input sanitization: pydantic validation; strip HTML from text fields where applicable; encode outputs.\nAPIs: N/A.\nCookies: Ensure Secure and SameSite per auth cookies.\nErrors: N/A.\nAcceptance:\n- Responses include required headers; sample reflective XSS payload is neutralized.\n- SQLAlchemy ORM used to prevent SQLi patterns in queries.\nLogging:\n- security_events: csp_violation_report (if enabled), xss_attempt_detected.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Audit and Security Event Logging with PII Masking",
            "description": "Implement structured audit and security logs with masking and correlation IDs; persist to audit_logs and security_events.",
            "dependencies": [],
            "details": "Design:\n- Log schema: {event_type, user_id?, session_id?, resource?, ip_masked, ua_masked, metadata, created_at, chain_hash} with optional hash-chaining.\n- PII masking: emails partially masked (a***@d***), IP truncated.\n- Correlation-ID per request; include in all logs and responses.\nAPIs:\n- GET /admin/logs (admin) with filters and pagination.\nCookies: N/A.\nErrors: 403 ERR-RBAC-FORBIDDEN for non-admin access.\nAcceptance:\n- All auth events generate audited entries; sensitive fields masked; chain hash verifies integrity.\n- Logs queryable by correlation ID.\nLogging:\n- Applies across all other subtasks; provides emitters/utilities for consistent logging.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Dev-Mode Toggles and Production Hardening",
            "description": "Introduce dev-mode behaviors and enforce production-only security settings.",
            "dependencies": [
              "3.3"
            ],
            "details": "Design:\n- ENV DEV_MODE=true enables relaxed guards for local development (e.g., skip CSRF for localhost), annotate responses dev_mode=true; never enabled in prod.\n- Enforce Secure cookies in prod; reject HTTP (redirect to HTTPS); disable detailed error traces in prod.\n- Feature flags for test OIDC provider.\nAPIs: N/A.\nCookies:\n- Validate cookie attributes vary by environment (Secure, SameSite strict in prod).\nErrors: N/A.\nAcceptance:\n- In dev: convenience features active and flagged; in prod: strict headers and cookies enforced; misconfiguration logs warnings and refuses to start if critical secrets missing.\nLogging:\n- audit_logs: config_loaded (env summary sans secrets).\n- security_events: insecure_config_detected (prod with insecure flags).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Frontend Auth Pages and Turkish i18n",
            "description": "Build Next.js pages for login/register, OIDC callback, and magic link; integrate React Hook Form and i18next (TR default).",
            "dependencies": [
              "3.1",
              "3.3",
              "3.5",
              "3.6",
              "3.8"
            ],
            "details": "Design:\n- Pages: /login, /register, /auth/oidc/callback, /auth/magic-link.\n- React Hook Form with dynamic validation mirroring backend policy; i18next with TR default and EN fallback; localized error handling.\n- Fetch flow: obtain CSRF token on app load; submit forms with X-CSRF-Token; store access token in memory (e.g., React state) and refresh via cookie.\nAPIs:\n- Calls backend: /auth/register, /auth/login, /auth/oidc/google/start, /auth/magic-link/request, /auth/magic-link/consume, /auth/csrf-token.\nCookies:\n- Reads CSRF cookie; backend manages refresh cookie; FE never stores refresh token.\nErrors:\n- Display localized messages for 401/403/429; map backend error codes to strings.\nAcceptance:\n- Turkish UI strings shown by default; validation errors inline; OIDC redirects correctly; magic link flow confirms email sent and consumes link successfully.\nLogging:\n- Client console/info logs minimized; no PII; send client telemetry (optional) with correlation ID if enabled.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Frontend Route Guards and Idle Logout",
            "description": "Implement Next.js middleware and client guards to enforce auth and license checks; add idle timer auto-logout and banner.",
            "dependencies": [
              "3.3",
              "3.13"
            ],
            "details": "Design:\n- middleware.ts: check for access token presence/validity (e.g., via lightweight endpoint or JWT decode) and redirect to /login; call /license/me to detect expiry → redirect to /license with banner of remaining days.\n- Idle timer: logout after configurable inactivity (e.g., 15–30m); warn before logout; clear memory tokens and call /auth/logout.\nAPIs:\n- GET /license/me → {status: active|expired, days_remaining}.\n- POST /auth/logout.\nCookies:\n- None handled directly; access token held in memory; refresh cookie managed by backend.\nErrors:\n- Redirect on 401; display localized toast on ERR-RBAC-FORBIDDEN when navigating to admin routes.\nAcceptance:\n- Unauthenticated user is redirected; expired license redirects with banner; idle logout triggers and clears session.\nLogging:\n- Client-side event logs (non-PII) for guard redirects and idle logout (optional analytics).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 15,
            "title": "End-to-End and Security Tests",
            "description": "Automate E2E and security tests: Playwright flows, mocked OIDC, ZAP scan, CSRF/XSS/rate-limit verification.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4",
              "3.5",
              "3.6",
              "3.7",
              "3.8",
              "3.9",
              "3.10",
              "3.11",
              "3.12",
              "3.13",
              "3.14"
            ],
            "details": "Design:\n- Playwright: register→login (pwd), MFA challenge, OIDC login via mocked Google, magic link request→consume, refresh rotation, logout, license guard redirects, idle logout.\n- Security: OWASP ZAP active scan against staging; CSRF negative tests; XSS reflection tests; rate-limit assertions (429 with Retry-After).\nAPIs: Exercise all relevant endpoints from other subtasks.\nCookies: Validate rt cookie attributes and CSRF cookie behaviors.\nErrors:\n- Assert specific codes/messages for failure paths (ERR-* from backend).\nAcceptance:\n- All happy paths pass; all negative paths return correct status and codes; ZAP reports no high/medium issues.\nLogging:\n- Verify audit/security events emitted for key actions and correlate via test-specific correlation IDs.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 16,
            "title": "Documentation and Operational Playbooks",
            "description": "Produce API docs, security notes, and runbooks for keys rotation, incidents, and day-2 operations.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4",
              "3.5",
              "3.6",
              "3.7",
              "3.8",
              "3.9",
              "3.10",
              "3.11",
              "3.12",
              "3.13",
              "3.14",
              "3.15"
            ],
            "details": "Design:\n- API reference with schemas, error codes, and cookie attributes; sequence diagrams for login, refresh rotation, OIDC, magic link, MFA.\n- Security guide: CSRF model, XSS defenses, header policies, RBAC model, rate-limits.\n- Playbooks: JWT signing key rotation (kid, JWKS), refresh token compromise response (revoke chains), password pepper rotation strategy, OIDC client secret rotation, incident response steps, audit log review, backup/restore for auth tables.\nAPIs: N/A (documentation deliverables).\nCookies: Document rt and csrf attributes and lifetimes.\nErrors: Document canonical ERR-* codes.\nAcceptance:\n- Docs reviewed and approved; includes examples and cURL snippets; on-call runbooks actionable with checklists.\nLogging:\n- Include logging taxonomy and examples; mapping of events to alerting.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Licensing, Billing, Invoices, Expiry Enforcement and Notifications",
        "description": "Build license assignment/extend/cancel APIs, invoice generation with numbering and VAT, payment provider integration stub, strict license enforcement (grace=0), and D-7/3/1 notifications via email/SMS.",
        "details": "Backend:\n- Endpoints: POST /license/assign|extend|cancel, GET /license/me\n- License types: ENUM 3m/6m/12m, scope JSON, ends_at. On extend, append duration; on cancel, set status and reason, audit trail\n- Enforcement middleware licenseGuard: if expired → 403 LIC_EXPIRED across all routes; on initial check for UI gating use /license/me; also revoke all sessions on expiry (set sessions.revoked_at)\n- Edge cases: running jobs on expire → mark cancel_requested and gracefully stop workers\n- Invoices: numbering 'YYYYMM-SEQ-CNCAI'; fields (amount, vat=amount*0.20, total). Currency: TRY (multi-currency behind feature flag)\n- PDF generation: WeasyPrint or reportlab; store pdf_url in S3/MinIO with immutable tag; signed GET URLs to deliver\n- Payments: provider-agnostic interface (e.g., Stripe Payment Intents or local PSP) with webhook to update payments.status and invoices.paid_status; audit all responses\n- Notifications: SMTP via provider (e.g., Postmark) and SMS via provider (e.g., Twilio/Vonage). ENV: SMTP_URL, SMS_API_KEY, SMS_SENDER, EMAIL_SENDER\n- Scheduler: Celery Beat daily 02:00 UTC: query licenses with remaining days 7/3/1 → enqueue notification jobs; persist notifications table (success/fail, provider_id)\n- Failover: try primary SMTP then fallback; SMS provider A→B fallback\nPseudocode (notification scan):\n- def scan_licenses():\n  - for lic in licenses where ends_at::date - now()::date in (7,3,1) and status=active:\n    - enqueue send_email_sms(user, lic)\n- def licenseGuard(request):\n  - lic = get_active_license(user)\n  - if not lic or lic.ends_at < now(): raise HTTPException(403, 'LIC_EXPIRED')\n",
        "testStrategy": "Unit: license date math, invoice numbering uniqueness under concurrency, VAT calc, licenseGuard. Integration: Webhook updates invoice/payment states; PDF upload to MinIO and immutability flag. Scheduler: freeze time to hit 7/3/1 and assert notifications enqueued and persisted. E2E: expired license blocks API/UI immediately; extending license re-enables access instantly.",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "License domain model and state transitions",
            "description": "Design license schema, enums, constraints, and lifecycle for assign/extend/cancel/expire with auditability.",
            "dependencies": [],
            "details": "Scope:\n- Define DB tables: licenses, license_audit.\n- licenses fields: id, user_id (FK), type ENUM('3m','6m','12m'), scope JSONB, status ENUM('active','expired','canceled'), reason TEXT NULL, starts_at TIMESTAMPTZ DEFAULT now(), ends_at TIMESTAMPTZ NOT NULL, canceled_at TIMESTAMPTZ NULL, created_at, updated_at.\n- Constraints: one active license per user (partial unique index on (user_id) WHERE status='active' AND ends_at>now()); indexes on (status, ends_at).\n- State transitions:\n  - assign: create active license with ends_at = starts_at + duration (3/6/12 months). Audit event 'license_assigned'.\n  - extend: only if status='active' and ends_at>=now(). On extend, ends_at += duration months (append, not reset). Audit 'license_extended' with delta.\n  - cancel: set status='canceled', reason, canceled_at=now(). Audit 'license_canceled'.\n  - expire: when now()>ends_at treat as expired (status may be updated lazily by middleware or via scheduled task). Audit 'license_expired'.\n- Invariants: cannot have overlapping active licenses per user; canceled/expired licenses are immutable except audit.\nFailure modes:\n- Invalid type, scope not JSON, attempt to assign when active exists, extend non-active, cancel already canceled.\nAcceptance criteria:\n- Data model migrates successfully; constraints enforce single active license; date math verified for 3/6/12 months; audit rows exist for each transition.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "License APIs (assign/extend/cancel, me) with audit",
            "description": "Implement POST /license/assign|extend|cancel and GET /license/me with request/response schemas and audit logging.",
            "dependencies": [
              "4.1"
            ],
            "details": "Endpoints:\n- POST /license/assign\n  - Auth: admin can assign to any user; user can self-assign if allowed by business rules.\n  - Body: { user_id?: UUID, type: '3m'|'6m'|'12m', scope: object, starts_at?: RFC3339 }\n  - Headers: Idempotency-Key (optional but supported).\n  - Response: { license: { id, type, scope, status, starts_at, ends_at } }\n  - Errors: 409 ACTIVE_LICENSE_EXISTS, 400 INVALID_TYPE, 403 FORBIDDEN.\n- POST /license/extend\n  - Body: { user_id?: UUID, license_id?: UUID, type: '3m'|'6m'|'12m' }\n  - Response: { license_id, previous_ends_at, new_ends_at, added_months }\n  - Errors: 409 LIC_NOT_ACTIVE, 404 NOT_FOUND.\n- POST /license/cancel\n  - Body: { user_id?: UUID, license_id?: UUID, reason: string }\n  - Response: { license_id, status: 'canceled', canceled_at, reason }\n  - Errors: 409 ALREADY_CANCELED, 404 NOT_FOUND.\n- GET /license/me\n  - Response: { status, type?, ends_at?, remaining_days?, scope? }\nAudit:\n- Write audit entries for each API call with actor, target, before/after diffs, request_id.\nSecurity/RBAC:\n- Enforce role-based access; validate ownership for self-service.\nAcceptance criteria:\n- Schemas documented via OpenAPI; success and error responses match; audit rows created for all state changes; idempotent retry (same Idempotency-Key) does not duplicate operations.\n<info added on 2025-08-18T18:26:44.021Z>\nImplementation completed and integrated:\n\n- Schemas (apps/api/app/schemas/license.py): LicenseAssignRequest/Response, LicenseExtendRequest/Response, LicenseCancelRequest/Response, LicenseMeResponse, LicenseErrorResponse with Turkish-localized messages and strict validation/error codes\n- Router (apps/api/app/routers/license.py): POST /license/assign, POST /license/extend, POST /license/cancel, GET /license/me with admin vs user RBAC and business-rule checks\n- Audit: full audit trail for all operations via LicenseService (actor, target, before/after diffs, request_id, unique operation IDs)\n- Idempotency: Idempotency-Key header supported (placeholder implementation) to prevent duplicate operations on retries\n- Security & Compliance: authorization enforced; client IP anonymization for KVKV; Turkish status/error localization\n- Integration: uses LicenseService from 4.1; router registered in main.py; works with existing JWT auth middleware\n- Spec alignment: request/response validation via Pydantic; error responses and codes match spec; OpenAPI reflects the schemas\n\nNote: Idempotency is a basic placeholder and can be hardened in a follow-up if needed.\n</info added on 2025-08-18T18:26:44.021Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Enforcement middleware and session revocation on expiry",
            "description": "Add global licenseGuard to protect routes, return 403 LIC_EXPIRED, and revoke all sessions upon expiry detection.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Behavior:\n- Implement licenseGuard(request): fetch get_active_license(user). If none or ends_at<now() then raise HTTP 403 with code 'LIC_EXPIRED'.\n- Apply to all protected routes; exclude auth/health/webhook endpoints as needed.\n- On first detection of expiry per user, revoke sessions: update sessions set revoked_at=now() where user_id=? and revoked_at is null; emit audit 'sessions_revoked_license_expired'.\n- Provide helper GET /license/me for UI gating; include remaining_days calculation.\n- Ensure thread-safe single revocation per user via DB condition.\nFailure modes:\n- Clock skew; missing user; DB unavailable (fail closed with 403 unless route is excluded?).\nAcceptance criteria:\n- Expired users receive 403 LIC_EXPIRED across protected endpoints; sessions.revoked_at set; subsequent requests remain blocked; non-expired users unaffected; logs include request_id and user_id.\n<info added on 2025-08-18T19:17:01.129Z>\n- Implemented global license middleware with path exclusions (/auth, /health, /webhook) that invokes LicenseService.get_active_license(user_id); if missing or ends_at < now, responds 403 with code LIC_EXPIRED and localized TR message; fail-closed on DB errors\n- Integrated middleware into main stack after environment middleware and before other security middleware; continues to rely on JWT auth for user extraction\n- On first expiry detection per user, revoke sessions via SessionService.revoke_all_user_sessions(user_id) which updates sessions.revoked_at=now() where revoked_at IS NULL; guarded by an in-process processed set with lock plus DB condition to ensure single revocation\n- Emit audit event sessions_revoked_license_expired with user_id, license_id, request_id, and anonymized client IP; logs include request_id and user_id on all relevant code paths\n- Compliance enhancements: IP addresses in audit metadata are anonymized; LIC_EXPIRED responses are localized to Turkish\n- Helper GET /license/me remains available for UI gating and includes remaining_days\n- Test coverage added: middleware initialization and ordering; path exclusion behavior; expired vs active license responses; fail-closed behavior on DB errors; single revocation under concurrency; audit event emission with IP anonymization; integration with session and audit services\n- Verified acceptance criteria: expired users receive 403 LIC_EXPIRED across protected endpoints; sessions are marked revoked once and subsequent requests remain blocked; non-expired users unaffected; logs contain request_id and user_id\n</info added on 2025-08-18T19:17:01.129Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Invoice numbering scheme, VAT calculation, and invoice model",
            "description": "Create invoice schema with numbering 'YYYYMM-SEQ-CNCAI', VAT 20%, TRY currency, and associations to licenses/users.",
            "dependencies": [
              "4.1"
            ],
            "details": "Data model:\n- invoices fields: id, user_id (FK), license_id (FK), number UNIQUE, amount NUMERIC(12,2), currency CHAR(3) DEFAULT 'TRY', vat NUMERIC(12,2), total NUMERIC(12,2), paid_status ENUM('unpaid','pending','paid','failed','refunded') DEFAULT 'unpaid', issued_at TIMESTAMPTZ DEFAULT now(), pdf_url TEXT NULL, provider_payment_id TEXT NULL, created_at, updated_at.\nNumbering:\n- Format: 'YYYYMM-SEQ-CNCAI' where YYYYMM = issued_at in UTC; SEQ is per-month incremental integer zero-padded (e.g., 000123). CNCAI is static suffix.\nCalculations:\n- vat = round(amount * 0.20, 2); total = amount + vat. Rounding mode: half up.\nLinkage:\n- One invoice per assign/extend event; store pointers to license and user.\nAcceptance criteria:\n- Invoices created with correct number format and amounts; currency fixed to TRY unless feature flag enables multi-currency; paid_status defaults to 'unpaid'.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Invoice PDF rendering and MinIO storage with immutability",
            "description": "Render invoice PDFs (WeasyPrint/ReportLab), store in S3/MinIO with immutable tag, and deliver via short-lived signed URLs.",
            "dependencies": [
              "4.4"
            ],
            "details": "Rendering:\n- Default renderer: WeasyPrint (HTML template with CSS); fallback to ReportLab on failure.\nStorage:\n- Bucket: invoices/. Object key: invoices/{YYYY}/{MM}/{number}.pdf. Set object immutability (legal hold or retention policy) and content-type application/pdf.\n- Persist pdf_url in invoices table.\nDelivery:\n- Generate presigned GET URLs (TTL 2 minutes) for download endpoint GET /invoices/:id/pdf.\nTemplate:\n- Include invoice number, dates, seller/buyer info, line items (license type and duration), amount, VAT, total, currency 'TRY'.\nAudit:\n- Log PDF generated and uploaded with checksum.\nAcceptance criteria:\n- PDF renders identically across engines for sample data; object stored and marked immutable; presigned URL works and expires; pdf_url persisted and points to stored object.\n<info added on 2025-08-19T07:18:14.325Z>\n- Implementation complete with dual-renderer PDFService (WeasyPrint primary, ReportLab fallback) and Turkish-localized HTML/CSS templates\n- MinIO storage uses immutability metadata with legal hold attempts; checksum captured and recorded in audit logs; invoices.pdf_url persisted\n- API endpoints added and integrated into FastAPI: GET /api/v1/invoices/{id}/pdf and POST /api/v1/invoices/{id}/pdf/regenerate\n- Presigned GET URLs issued with 2-minute TTL; comprehensive error handling and structured audit logging for generation and upload events\n- Monetary values rendered using Decimal for exact precision\n- KVKK compliance observed (minimized PII in logs, short-lived URLs)\n- Test suite covers renderer fallback and equivalence for sample data, storage immutability, presigned URL expiry, audit checksum, and endpoint behaviors; all acceptance criteria validated and ready for deployment\n</info added on 2025-08-19T07:18:14.325Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Payments provider abstraction and webhook handling with idempotency",
            "description": "Implement provider-agnostic payments interface (Stripe/local PSP style), intents, and webhook to update payment and invoice states.",
            "dependencies": [
              "4.4"
            ],
            "details": "Abstraction:\n- Interface PaymentProvider: create_intent(amount, currency, metadata), retrieve(id), confirm(id, params), verify_webhook(sig, payload).\n- payments table: id, invoice_id, provider, provider_payment_id, amount, currency, status ENUM('requires_action','processing','succeeded','failed','canceled','refunded'), raw_request/response JSONB, created_at, updated_at.\nAPI/Webhook:\n- POST /payments/intents { invoice_id } -> { client_secret, provider, provider_payment_id }.\n- POST /payments/webhook: verify signature, parse events (payment_intent.succeeded/failed/refunded). Idempotency by event_id with unique index.\n- On succeeded: set payments.status='succeeded', invoices.paid_status='paid', audit 'payment_succeeded'. On failed: set 'failed', invoices.paid_status='failed'.\n- Log and persist all provider responses (for audit).\nFailure modes:\n- Signature invalid -> 400; unknown invoice -> 404; duplicate webhook -> 200 no-op.\nAcceptance criteria:\n- Creating an intent returns usable client params; webhook updates invoice/payment states correctly; duplicate webhooks are ignored; all interactions audited.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Notification service (email/SMS) with provider fallback and templating",
            "description": "Build notification layer with SMTP + SMS providers, fallback logic, templating for D-7/3/1 reminders, and persistence.",
            "dependencies": [
              "4.2"
            ],
            "details": "Providers:\n- Email: primary SMTP (e.g., Postmark via SMTP_URL), fallback SMTP or API client.\n- SMS: primary (Twilio) via SMS_API_KEY, fallback (Vonage).\nTemplating:\n- Templates for D-7, D-3, D-1 with variables: {user_name, days_remaining, ends_at, renewal_link}. SMS within 160 chars; Email HTML + plain text.\nPersistence:\n- notifications table: id, user_id, license_id, channel ENUM('email','sms'), template_id, days_out INT, status ENUM('queued','sent','failed'), provider, provider_id, error_text, created_at, sent_at.\nFallback:\n- On provider failure, try fallback provider once, record both attempts.\nConfig:\n- ENV: SMTP_URL, EMAIL_SENDER, SMS_API_KEY, SMS_SENDER.\nAcceptance criteria:\n- Sending works with primary provider; on simulated failure, fallback is used; notifications rows capture success/failure, provider_id, and timing; templates render with correct substitutions.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Scheduler via Celery Beat for D-7/3/1 scans",
            "description": "Run daily scan at 02:00 UTC to enqueue and persist D-7/3/1 email/SMS notifications for active licenses.",
            "dependencies": [
              "4.1",
              "4.7"
            ],
            "details": "Scheduling:\n- Celery Beat job daily 02:00 UTC: scan_licenses().\nLogic:\n- Query licenses where status='active' and (DATE(ends_at) - DATE(now())) IN (7,3,1).\n- For each, enqueue send_email_sms(user, license, days_out) task; insert notifications row with status='queued'. Prevent duplicates per (license_id, days_out, date) via unique key.\nIdempotency:\n- Use transaction + ON CONFLICT DO NOTHING to avoid duplicate enqueues.\nObservability:\n- Log counts by days_out; metrics for queued/sent/failed.\nAcceptance criteria:\n- With time frozen to D-7/3/1, notifications are enqueued once per license; reruns do not duplicate; notifications persisted with correct days_out.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Edge-case handling for running jobs on expiry",
            "description": "On license expiry, flag running jobs cancel_requested and ensure workers stop gracefully.",
            "dependencies": [
              "4.3",
              "4.2"
            ],
            "details": "Behavior:\n- On detecting expiry in licenseGuard or a background watcher: update jobs set cancel_requested=true where user_id=? and status IN ('running','pending'); enqueue lightweight cancellation signal.\n- Workers: periodically check cancel_requested and stop after safe checkpoint; mark job as 'canceled' with reason 'license_expired'.\n- Persist audit events: 'license_expired_jobs_cancel_requested' with affected job IDs.\n- Provide admin endpoint GET /licenses/:id/impacted-jobs to review state.\nAcceptance criteria:\n- When a license expires, active jobs transition to canceling -> canceled without abrupt termination; audit lists impacted jobs; new job submissions are blocked by licenseGuard.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Observability and audit trail across licensing, billing, payments, notifications",
            "description": "Implement structured logs, metrics, traces, and comprehensive audit entries with correlation IDs.",
            "dependencies": [
              "4.2",
              "4.4",
              "4.6",
              "4.7",
              "4.8"
            ],
            "details": "Observability:\n- OpenTelemetry tracing on API endpoints, Celery tasks, webhook handler; propagate X-Request-ID / trace-id.\n- Metrics: counters/gauges for licenses_active, license_expired_events, invoices_created, payments_succeeded/failed, notifications_sent/failed.\nAudit:\n- audit_log table: id, actor_id, subject_type, subject_id, action, before JSONB, after JSONB, request_id, created_at.\n- Actions captured: license_assigned/extended/canceled/expired, sessions_revoked, invoice_created/pdf_generated, payment_intent_created/succeeded/failed, notification_sent/failed.\nRetention:\n- Configure log retention and PII redaction for sensitive fields.\nAcceptance criteria:\n- Each state change produces an audit record with subject and action; traces span HTTP->DB->queue; metrics visible in test run; correlation IDs consistent across related events.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Concurrency and uniqueness guards (invoice numbers, idempotency, webhooks)",
            "description": "Ensure atomic, conflict-free invoice numbering and deduplication for API idempotency and webhooks.",
            "dependencies": [
              "4.4",
              "4.6"
            ],
            "details": "Invoice numbering:\n- Use DB sequence per month or a (period, seq) allocator with advisory locks. Compose number = YYYYMM + '-' + LPAD(seq,5,'0') + '-CNCAI'. Unique index on number.\n- Retry on conflict with bounded backoff.\nAPI idempotency:\n- Respect Idempotency-Key for /license/assign and /license/extend: store request hash keyed by (user_id, key) with response snapshot; return same result on retry.\nWebhooks:\n- Deduplicate via unique event_id table; process-once semantics.\nOther guards:\n- Unique active license per user enforced by partial unique index.\nAcceptance criteria:\n- Under concurrent 100x invoice creations in same month, zero duplicate numbers; repeated Idempotency-Key returns same response without side effects; duplicate webhook deliveries are no-ops.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Test matrix and acceptance criteria (unit/integration/e2e/time-freeze)",
            "description": "Define and implement tests for license flows, enforcement, invoicing, payments, notifications, scheduler, and concurrency.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4",
              "4.5",
              "4.6",
              "4.7",
              "4.8",
              "4.9",
              "4.10",
              "4.11"
            ],
            "details": "Unit tests:\n- License date math (3/6/12 months append), single active constraint, status transitions.\n- VAT calc and rounding; invoice number formatter.\n- licenseGuard behavior; session revocation.\n- Notification template rendering and fallback selection.\nIntegration tests:\n- Webhook updates payment/invoice states with signature verification and idempotency.\n- PDF generation and MinIO upload; immutability/tag presence; presigned GET works and expires.\n- Idempotent assign/extend under retries.\nScheduler/time-freeze:\n- Freeze time to D-7/3/1 and assert notifications enqueued once and persisted.\nE2E:\n- User without license blocked (403 LIC_EXPIRED); after assign, access allowed; after expiry, blocked and sessions revoked; invoice created; payment success flips paid_status; reminders sent at D-7/3/1.\nConcurrency:\n- Hammer invoice creation to verify unique numbering; duplicate webhooks ignored.\nAcceptance criteria:\n- All tests pass; OpenAPI docs validate; no flakiness under concurrency stress; audit entries present for all relevant flows.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "S3/MinIO File Service, Signed URLs, Upload Validation and Artefacts",
        "description": "Implement file service for uploads/downloads with MinIO presigned URLs, type/size/sha256 validation, optional malware scan, and artefact management tied to jobs.",
        "details": "Buckets/classes: artefacts (FCStd/STEP/STL/GLB/G-code/video), logs, reports, invoices with versioning + lifecycle (hot→cold). Object tags: job_id, machine, post.\nAPIs:\n- POST /files/upload/init {type, size, sha256, mime} → presigned PUT (TTL 5m, optional IP binding)\n- POST /files/upload/finalize {key} → verify object exists, size <= 200MB, compute sha256 server-side (stream) and compare; optional ClamAV scan; persist artefacts row\n- GET /files/:id → presigned GET (TTL 2m)\nValidation:\n- Allow: .step .stl .fcstd .glb .nc .tap .gcode .mp4 .gif; enforce MIME; reject otherwise (415)\n- On upload: limit size 200MB; block double extensions\nSecurity:\n- Signed URLs use least privilege; audit access events; store sha256, size; set object lock for invoices\n- Client uploads directly to S3/MinIO; backend never stores raw file on disk\nFrontend:\n- Uploader with progress; compute client-side sha256 (Web Workers) to include in init\nPseudocode (finalize):\n- def finalize(key):\n  - stat = minio.stat_object(bucket, key)\n  - assert stat.size <= 200*1024*1024\n  - hasher = sha256(); for chunk in minio.get_object_stream(...): hasher.update(chunk)\n  - if hasher.hexdigest()!=expected: delete object; 422\n  - if scan_enabled and clamav.detect(key): delete; 422\n  - insert artefact(job_id, key, size, sha256)\n",
        "testStrategy": "Integration: init→upload→finalize happy path; wrong sha256 rejected; wrong MIME rejected; oversize rejected; presigned URLs expire. Malware scan mock returns detection gets blocked. Artefact rows created with correct metadata. Download presigned GET works and is short-lived.",
        "priority": "high",
        "dependencies": [
          1,
          2,
          3
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "MinIO client configuration and credentials management",
            "description": "Set up secure MinIO client, environment config, credentials rotation, and connection hardening.",
            "dependencies": [],
            "details": "- Env vars: MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY, MINIO_REGION (optional), MINIO_SECURE=true, MINIO_BUCKET_ARTEFACTS, MINIO_BUCKET_LOGS, MINIO_BUCKET_REPORTS, MINIO_BUCKET_INVOICES.\n- Client: MinIO Python SDK (minio==7.2.7); timeouts (connect/read 10s/60s), retries w/ backoff (3 attempts), HTTP/HTTPS support, certificate pinning or CA bundle configurable.\n- Credentials: dedicated service user(s) with least-privilege policies; document rotation procedure; disallow root credentials in non-local envs.\n- Key naming strategy: artefacts/{job_id}/{uuid}.{ext}, logs/{date}/{uuid}.log, reports/{date}/{uuid}.pdf, invoices/{year}/{invoice_no}.pdf.\n- Security: never persist raw files to local disk; stream I/O only; sanitize key components; enforce URL signing exclusively for client access.\n- Acceptance tests:\n  - Can connect to MinIO and list buckets using service credentials.\n  - Network failures retry and surface clear 503 STORAGE_UNAVAILABLE.\n  - TLS misconfig results in 502 STORAGE_TLS_ERROR with safe message.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Buckets, policies, versioning, lifecycle and object lock",
            "description": "Create buckets with versioning and lifecycle hot→cold, set policies and object lock for invoices.",
            "dependencies": [
              "5.1"
            ],
            "details": "- Buckets: artefacts, logs, reports, invoices.\n- Enable versioning on all buckets.\n- Lifecycle: transition non-current versions to cold storage after 30 days (env-configurable), expire incomplete multipart uploads after 7 days; delete logs after 90 days (configurable); reports after 365 days; artefacts retain per policy; invoices retain indefinitely.\n- Object Lock: Enable on invoices bucket (COMPLIANCE mode) with retention period via env (e.g., INVOICE_RETENTION_YEARS=7); legal hold supported via admin tools.\n- Policies: deny ListBucket to public; service account(s) restricted to specific prefixes; only PUT/GET/HEAD allowed for artefact paths; deny DeleteObject on invoices bucket.\n- Presign constraints (enforced via conditions where supported): content-length-range ≤ 200MB; content-type must match allowed mime; optional x-amz-tagging for job_id/machine/post.\n- Object tags standard: job_id, machine, post.\n- Acceptance tests:\n  - Versioning enabled and new object gets version ID.\n  - Lifecycle rules exist and are validated via MinIO client.\n  - Invoices cannot be deleted due to object lock (expect 403/AccessDenied).\n  - Upload >200MB rejected at presign or PUT stage.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Upload init/finalize and download APIs with presigned URLs",
            "description": "Implement POST /files/upload/init, POST /files/upload/finalize, GET /files/:id with least-privilege presigned URLs.",
            "dependencies": [
              "5.1",
              "5.2"
            ],
            "details": "- POST /files/upload/init {type, size, sha256, mime, job_id, machine?, post?}\n  - Validate inputs (basic): size ≤ 200MB, type/mime prelim allowed list.\n  - Generate server-side key: artefacts/{job_id}/{uuid}.{ext}.\n  - Presign PUT URL (TTL 5m), optionally bind client IP; include conditions for content-type and content-length-range; optionally include tagging for job_id/machine/post.\n  - Response: {key, upload_url, expires_in:300, headers:{Content-Type,...}}.\n  - Errors: 400 INVALID_INPUT, 401 UNAUTHORIZED, 415 UNSUPPORTED_MEDIA_TYPE, 413 PAYLOAD_TOO_LARGE, 429 RATE_LIMITED.\n- POST /files/upload/finalize {key}\n  - Look up expected metadata from init (size, sha256, mime, job_id...)\n  - Stream-verify existence and SHA256 (see subtask 5); optional malware scan (subtask 6); persist artefact (subtask 7).\n  - Errors: 404 NOT_FOUND, 409 UPLOAD_INCOMPLETE, 413 PAYLOAD_TOO_LARGE, 422 HASH_MISMATCH or MALWARE_DETECTED, 503 SCAN_UNAVAILABLE, 500 STORAGE_ERROR.\n- GET /files/:id → presigned GET (TTL 2m)\n  - Authorize access by artefact ownership/role; log audit; return {download_url, expires_in:120}.\n  - For invoices ensure object lock is respected.\n- Security: presigned URLs single-operation; minimal TTLs; restrict headers; no backend file writes.\n- Acceptance tests:\n  - Happy path: init→PUT→finalize→GET works; TTLs honored (PUT 5m, GET 2m).\n  - Unauthorized user gets 401/403; wrong job_id cannot access artefacts; expired URL fails with 403 SignatureDoesNotMatch.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Server-side validation for type, MIME, size, and double-extension guard",
            "description": "Enforce strict validation for uploads including allowed types/MIME, size limit, and extension checks.",
            "dependencies": [
              "5.3"
            ],
            "details": "- Allowed extensions: .step .stl .fcstd .glb .nc .tap .gcode .mp4 .gif.\n- Allowed MIME (examples): model/step, model/stl, application/vnd.freecad, model/gltf-binary, text/plain (G-code), video/mp4, image/gif. Map per extension; reject mismatches.\n- Size limit: ≤ 200MB enforced at init and finalize; use content-length-range condition on presign and recheck via stat.\n- Double-extension guard: reject filenames where the last extension is allowed but the penultimate extension exists and is not identical or is in blacklist [exe, js, sh, bat, cmd, com, dll, zip, rar, 7z].\n- Filename sanitization: normalize to server-generated UUID; ignore client-provided names to avoid traversal and UTF-8 tricks.\n- Error codes: 415 for unsupported type/MIME, 413 for oversize, 400 for malformed input.\n- Acceptance tests:\n  - .exe or .stl.exe rejected with 415.\n  - MIME mismatch (e.g., .stl with video/mp4) rejected with 415.\n  - >200MB rejected at init (413) and finalize (413).",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Streaming SHA256 computation and comparison",
            "description": "Compute SHA256 server-side by streaming from MinIO, compare to client-provided hash, and handle mismatches.",
            "dependencies": [
              "5.3",
              "5.4"
            ],
            "details": "- Implementation: use stat_object to confirm size, then get_object with ranged streaming; hash incrementally (chunk e.g., 8MB) to keep memory low; consider multipart objects.\n- Compare computed digest to expected from init; on mismatch: delete object, emit audit event, return 422 HASH_MISMATCH.\n- On success: attach metadata/etag (cannot overwrite etag), store computed sha256 in DB (subtask 7).\n- Timeouts: 60s read timeout; abort on slowloris; idempotent finalize allowed (re-verify same hash and return success).\n- Security: do not trust client headers for hash; only server-side compute; ensure path/key matches init record.\n- Acceptance tests:\n  - Happy path matches hash for a ~100MB file within memory constraints (<64MB RSS spike).\n  - Altered upload triggers deletion and 422.\n  - Finalize called twice returns 200 idempotently.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Optional ClamAV integration and failure handling",
            "description": "Integrate ClamAV (clamd) to scan uploaded objects post-hash with robust error handling.",
            "dependencies": [
              "5.3",
              "5.5"
            ],
            "details": "- clamd: connect via TCP or Unix socket; timeouts 10s connect / 60s scan; stream-scan from MinIO (avoid local disk) using chunk bridge.\n- Scan policy: execute only for configured types (e.g., non-G-code CAD and videos) or if scan_enabled; on detection: delete object, 422 MALWARE_DETECTED with remediation hint.\n- Failure mode: fail closed if scan_enabled=true and clamd unreachable → 503 SCAN_UNAVAILABLE; log security_event.\n- Rate limiting: cap concurrent scans to protect resources.\n- Acceptance tests:\n  - EICAR string upload triggers 422 MALWARE_DETECTED and object removed.\n  - clamd down → finalize returns 503 SCAN_UNAVAILABLE when scan_enabled.\n  - scan_enabled=false → finalize succeeds without scan.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Artefact persistence, tagging, and audit logging",
            "description": "Persist artefact metadata in DB, apply S3 object tags, and audit all access events.",
            "dependencies": [
              "5.3",
              "5.5",
              "5.6"
            ],
            "details": "- DB insert into artefacts: job_id (FK), s3_bucket, s3_key, size, sha256, mime, type, created_by, machine?, post?, version_id; unique on s3_key.\n- S3 object tags: set/merge job_id, machine, post after successful finalize; retry on tag failures; ensure least-privilege.\n- Invoices: when type=invoice, set object retention (if supported at object level) and verify lock active.\n- Audit logs: write events for upload_init, upload_finalize_success/failure, download_url_issued with user_id, job_id, ip, user_agent, hash chain link.\n- Security: do not expose direct S3 keys publicly; enforce authz checks on GET /files/:id by job ownership/role; log every presign issuance.\n- Acceptance tests:\n  - Finalize persists artefact row with correct size and sha256; tags present on object.\n  - Download presign issues audit entry; unauthorized access denied with 403.\n  - Invoice artefact shows retention/lock metadata.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Frontend uploader with Web Worker SHA256 and progress",
            "description": "Build uploader UI that computes SHA256 client-side via Web Worker, performs presigned PUT, and finalizes.",
            "dependencies": [
              "5.3",
              "5.4"
            ],
            "details": "- Flow: user selects file → worker computes SHA256 (streaming via File.slice) → call /files/upload/init → PUT to presigned URL with correct Content-Type and Content-Length → on 200/204, call /files/upload/finalize.\n- Progress: upload progress via XHR/fetch streams; show speed, ETA; handle retries/backoff on transient 5xx.\n- Validation UX: enforce allowed extensions, size ≤200MB, double-extension warning before init; show clear errors for 415, 413, 422.\n- TTL handling: warn if PUT URL close to expiry; if expired (403), re-init and retry automatically once.\n- Security: no file reads by main thread beyond hashing; sandbox worker; never send file to backend; respect IP binding if enabled.\n- Acceptance tests:\n  - Happy path completes and shows success state; finalize returns artefact metadata.\n  - Wrong hash simulation (flip byte) yields 422 with user-facing remediation and auto-cleanup shown.\n  - Expired URL triggers re-init and successful retry; wrong MIME blocked with inline error.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Job Orchestration, Idempotency, DLQ and Audit Chain",
        "description": "Provide job APIs, queue topology, idempotency keys, retries/backoff with jitter, cancellation, progress updates, and append-only audit log with hash-chain.",
        "details": "Celery configuration:\n- Queues: default (light), model, cam, sim, report, erp; DLX for each (dead-letter exchange) routing to *_dlq queues\n- Retries: exponential backoff with jitter; max retries per task type (AI 3, model 5, cam 5, sim 5, erp 5)\nAPIs:\n- POST /jobs (type, params, idempotency_key): if exists → return existing; else create 'pending' and enqueue\n- GET /jobs/:id: status, progress, artefacts\n- Cancellation: POST /jobs/:id/cancel → set cancel_requested; workers check cooperative cancel between steps\n- Events: job.status.changed published internally and used for ERP outbound\nIdempotency:\n- jobs.idempotency_key unique; transactional check-create pattern to avoid races\nAudit:\n- Every state transition writes audit_logs with chain_hash computed as sha256(prev_hash || canonical_json)\nRate limit + DLQ replay:\n- Admin DLQ replay API gated by MFA\nPseudocode (create job):\n- def create_job(req):\n  - with tx:\n    - if job by idempotency_key exists: return it\n    - job = insert jobs(... status='pending')\n  - celery.send_task(queue=req.type, args=[job.id])\n  - return job\n",
        "testStrategy": "Unit: idempotent creation, chain_hash consistency, cancel flag handling. Integration: enqueue tasks, simulate failures to DLQ and replay. Concurrency: two simultaneous requests with same idempotency_key result in one job. Progress updates visible via GET /jobs/:id. Audit entries ordered and hash-linked.",
        "priority": "high",
        "dependencies": [
          2,
          3,
          5
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Celery and RabbitMQ topology with DLX/DLQ",
            "description": "Define Celery app and RabbitMQ resources for job orchestration with per-queue dead-lettering.",
            "dependencies": [],
            "details": "Implement Celery 5.4 app configuration. Declare durable direct exchanges and queues: primary queues: default, model, cam, sim, report, erp; for each, create a dedicated dead-letter exchange <queue>.dlx and a DLQ queue <queue>_dlq. Bindings: primary queues bound to exchange jobs.direct with routing keys: default, model, cam, sim, report, erp. Each primary queue sets x-dead-letter-exchange to <queue>.dlx; each DLQ queue bound to its DLX with routing key '#'. Use quorum queues for primaries, classic (lazy) for DLQs. Set basic_qos prefetch=8; acks_late=True. Define Celery task_queues using kombu Queue objects; configure task_routes mapping type→queue. Routing keys: jobs.ai→default, jobs.model→model, jobs.cam→cam, jobs.sim→sim, jobs.report→report, jobs.erp→erp. Policies: durable, lazy-mode for DLQs, message size limit 10MB, enforce publisher confirms. Acceptance: publishing a message with each routing key delivers to the correct primary queue; rejecting with requeue=False routes to the corresponding *_dlq; Celery workers consume from their expected queues.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Retry strategy, backoff with jitter, and error taxonomy",
            "description": "Configure per-task-type retry policy with exponential backoff and jitter, and define error classes for retry vs DLQ.",
            "dependencies": [
              "6.1"
            ],
            "details": "Set max retries: AI 3 (uses default queue), model 5, cam 5, sim 5, erp 5, report 5. Use exponential backoff with full jitter: delay_n = min(cap, base * 2^n) * random.uniform(0.5, 1.5); base=2s, caps: AI 20s, model/cam/sim 60s, report/erp 45s. Configure Celery autoretry_for on retryable exceptions and retry_kwargs per task type. Error taxonomy (examples): Retryable: TransientExternalError, RateLimitedError, NetworkError; Non-retryable: ValidationError, UnauthorizedError, QuotaExceededError; Cancellation: JobCancelledError (no retry); Fatal: IntegrityError (send to DLQ immediately). Enable task_acks_late, reject_on_worker_lost=True, time_limit/soft_time_limit per type (e.g., model 900/840s). On exceeding retries, nack with requeue=False to DLQ. Include attempt count and last_exception in task headers for observability. Acceptance: forced Retryable errors show increasing backoff with jitter and cap; Non-retryable errors go directly to DLQ; attempt counts align with configured maxima; tasks respect time limits.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Job type routing, payload contracts, and validation",
            "description": "Define job type enumeration, routing rules to queues, and canonical task payload schema with validation.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "Job types: ai (routes to default), model, cam, sim, report, erp. Routing map: type→routing_key (jobs.<type>) → queue (matching the type, ai→default). Canonical task payload: { job_id: uuid, type: enum, params: object, submitted_by: user_id, attempt: int, created_at: iso8601 }. Validate params per type with Pydantic schemas; reject invalid with ERR-JOB-422. Enforce a max payload size of 256KB; large artefacts are referenced via object storage keys, not embedded. Acceptance: publishing with each type selects the correct queue; invalid types yield ERR-JOB-400; params validation errors are non-retryable.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "POST /jobs with transactional idempotency and enqueue",
            "description": "Implement job creation API with idempotency_key handling, DB transactionality, enqueue to Celery, and rate limits.",
            "dependencies": [
              "6.1",
              "6.3"
            ],
            "details": "FastAPI endpoint: POST /jobs accepts {type, params, idempotency_key}. In a DB transaction: lookup by idempotency_key; if found, return existing (200). Else insert jobs row with status='pending', cancel_requested=false, attempts=0, progress=0, idempotency_key unique. Use unique index on jobs.idempotency_key; implement insert-on-conflict pattern to avoid races and then select existing. After commit, publish task to appropriate queue with payload contract. Rate limiting: per-user 60/min and global 500/min via Redis token bucket; return 429 ERR-JOB-RATE-LIMIT when exceeded. Responses: 201 for new with Location header; 200 for existing idempotent hit. Errors: invalid type/params → 422; database conflict → 409; unknown → 500. Acceptance: two concurrent requests with same idempotency_key yield a single job; metrics show rate-limit counters; task appears on correct queue.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "GET /jobs/:id for status, progress, and artefacts",
            "description": "Expose job read API returning current status, progress, attempts, errors, and artefact references.",
            "dependencies": [
              "6.4"
            ],
            "details": "FastAPI endpoint: GET /jobs/:id returns {id, type, status, progress: {percent, step, message, updated_at}, attempts, cancel_requested, created_at, updated_at, artefacts: [{id, kind, s3_key, sha256, size}], last_error: {code, message}}. Authorize access by owner or admin. Support ETag/If-None-Match to reduce polling bandwidth. Acceptance: progress updates issued by workers are visible within 1s; artefact list reflects persisted outputs; 404 for missing or unauthorized jobs; ETag changes when progress changes.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Cancellation API and cooperative worker cancellation",
            "description": "Implement POST /jobs/:id/cancel and worker-side cooperative checks to stop work safely.",
            "dependencies": [
              "6.4",
              "6.1"
            ],
            "details": "API: POST /jobs/:id/cancel sets jobs.cancel_requested=true and writes an audit entry. Workers must call check_cancel(job_id) between major steps; check reads a cached flag (Redis) backed by DB and raises JobCancelledError if set. On cancellation: mark job status='cancelled', persist final progress, do not retry, and release resources. Ensure idempotent cancel endpoint responses (200 even if already cancelled). Acceptance: cancelling during a long-running job stops work within a step boundary; subsequent GET shows status=cancelled; cancelled tasks are not retried or sent to DLQ.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Worker progress update conventions and status change events",
            "description": "Define progress reporting API for workers and publish job.status.changed events for internal consumers and ERP outbound.",
            "dependencies": [
              "6.4",
              "6.1",
              "6.2",
              "6.6"
            ],
            "details": "Provide a worker helper progress(job_id, percent, step, message, metrics) that updates jobs.progress fields and emits a job.status.changed event with payload {job_id, status, progress, attempt, timestamp}. Throttle writes to at most once per 2s per job to reduce DB load (coalesce). On state transitions (queued, started, running, retrying, succeeded, failed, cancelled), publish the event to an internal topic exchange events.jobs with routing key job.status.changed and fanout to ERP outbound bridge. Acceptance: progress in DB advances monotonically and is visible via GET; events for each transition are published exactly once per transition and consumed by ERP bridge.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Append-only audit log with hash-chain for state transitions",
            "description": "Record every job state change in audit_logs with a tamper-evident chain_hash.",
            "dependencies": [
              "6.4",
              "6.7"
            ],
            "details": "On each state change (created, queued, started, progress, retrying, cancelled, failed, succeeded, dlq_replayed), write an audit_logs row with fields: id, job_id, event_type, actor (system/user id), ts, payload (canonical JSON), prev_hash, chain_hash. Compute chain_hash=sha256(prev_hash || canonical_json(payload)) using stable key ordering and normalized floats/ints. prev_hash is the chain_hash of the last audit entry for the job, or 32 zero-bytes for the first. Enforce append-only at application level; never update existing rows. Provide a verification routine to recompute and validate the chain for a job. Acceptance: inserting two consecutive audit events yields deterministic chain_hash; modifying any prior audit row causes verification to fail; all API and worker transitions create corresponding audit entries.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Admin DLQ inspection and replay API with MFA and auditing",
            "description": "Implement secured endpoints to list DLQs, inspect messages, and replay to origin queues with audit trail.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.8"
            ],
            "details": "Endpoints: GET /admin/dlq lists *_dlq queues with depths; GET /admin/dlq/{queue}/peek?limit=n previews messages (headers include original routing key, job_id if present); POST /admin/dlq/{queue}/replay re-publishes up to N messages to their original exchange/routing key. Security: admin role + enforced MFA check; rate limit 30/min; require justification string recorded in audit. Replay policy: only messages whose original exchange/routing key match known routes; preserve headers; backoff between batches to avoid thundering herd. Errors: unauthorized (ERR-DLQ-401), invalid queue (ERR-DLQ-404), replay limit exceeded (ERR-DLQ-429). Acceptance: replayed messages land on primary queues and are processed; all admin actions are written to audit_logs with event_type=dlq_replayed; MFA is required for access.",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Observability (metrics, tracing, logs) and comprehensive test suite",
            "description": "Add structured logs, metrics, tracing across APIs/workers, and implement tests for idempotency, retries, DLQ, audit chain, and cancellation.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3",
              "6.4",
              "6.5",
              "6.6",
              "6.7",
              "6.8",
              "6.9"
            ],
            "details": "Logging: structlog with fields request_id, trace_id, job_id, idempotency_key, attempt, queue, routing_key, error_code; ensure PII masking. Metrics (Prometheus): job_create_total (labels type, status), job_in_progress gauge, job_duration_seconds (histogram), retries_total (labels type, error_code), dlq_depth (gauge per queue), dlq_replay_total, cancellation_total, progress_update_total. Tracing: OpenTelemetry for FastAPI and Celery (link spans via job_id); export to OTLP. Dashboards: Grafana panels for queue depths, success/failure rates, retry distribution, DLQ replay outcomes. Tests: unit tests for idempotent creation race (simulated concurrent POST with same idempotency_key), audit chain determinism and tamper detection, cancellation behavior, progress throttling, error taxonomy routing to retry/DLQ; integration tests that enqueue tasks, force failures into DLQ, and verify admin replay; performance test for 1k concurrent job creates meeting rate-limit behavior. Acceptance: >=90% coverage for job orchestration module; dashboards show live metrics; traces link API request to worker execution; all specified test scenarios pass.",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Model Generation Flows (Prompt, Parametric, Upload, Assembly4) with FreeCAD",
        "description": "Implement endpoints and Celery workers to generate models via FreeCADCmd for prompt-driven (AI pre-processing), parametric forms, file upload normalization, and Assembly4 assemblies. Produce FCStd, STEP, STL, GLB preview and metrics.",
        "details": "Endpoints:\n- POST /designs/prompt, /designs/params, /designs/upload, /assemblies/a4 (all JWT + licenseGuard + rate limit)\nPrompt (AI):\n- Adapter interface (e.g., OpenAI/Azure) with limits: max tokens ~2k, timeout 20s, retries 3; per-user 30/min; mask PII in logs; store raw masked in ai_suggestions table (part of models.params)\n- normalize() and validate() deterministic; ambiguous → 425; missing → ERR-AI-422 AI_HINT_REQUIRED\nValidation rules:\n- Required fields (dimensions, units, material, machine); ranges (min wall, inner radius); material↔machine compatibility\nFreeCAD worker (subprocess):\n- Launch per job: `FreeCADCmd -c worker_script.py --args ...`; set ulimit or cgroups for CPU/RAM\n- Parametric example (prism with hole) pseudo-Python:\n  - import FreeCAD as App, Part\n  - doc=App.newDocument(); b=Part.makeBox(L,W,H); c=Part.makeCylinder(d/2,H)\n  - b=b.cut(c.translate(App.Vector(L/2,W/2,0)))\n  - Part.show(b); doc.recompute(); doc.saveAs(fcstd_path)\n  - export STEP/STL via Import/Export; generate STL then GLB using trimesh(export('glb')) for preview\n- Upload flow: on finalize, run normalization (unit conversion, orientation); optional manifold fix (trimesh.repair)\n- Assembly4: parse parts and placement constraints; build hierarchy; collision check (basic bounding box first)\nOutputs:\n- Artefacts: FCStd, STEP, STL, GLB; metrics: solids/faces/edges counts, duration; logs with request_id\n- Errors: catch FreeCAD exceptions → job failed + suggestions\n",
        "testStrategy": "Unit: normalize/validate for several sample prompts/params; ambiguous prompt returns proper code. Integration: run FreeCAD worker in container to produce FCStd/STEP/STL/GLB; verify artefacts saved and sha256 logged. Upload: corrupted STEP returns 422 with remediation hints. Assembly: conflicting constraints detect and reported.",
        "priority": "high",
        "dependencies": [
          2,
          5,
          6
        ],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Define API contracts and guards for model generation endpoints",
            "description": "Design and document POST /designs/prompt, /designs/params, /designs/upload, /assemblies/a4 with JWT, licenseGuard, RBAC scope checks, and per-user rate limits.",
            "dependencies": [],
            "details": "Deliver OpenAPI schemas and Pydantic models for inputs/outputs; apply guards: JWT Bearer, licenseGuard (active license), RBAC scope 'models:write'; rate limits: global 60/min per user and AI prompt-specific 30/min; support Idempotency-Key header (stored to jobs.idempotency_key) returning 409 on reuse with conflicting body; responses: 202 Accepted with job_id and request_id; error codes include 401/403/429; content types: application/json (prompt, params, a4) and application/json with object storage reference for upload; include GET /jobs/:id and GET /jobs/:id/artefacts to poll status and list artefacts; acceptance: OpenAPI generated, guards enforced in integration stub, rate limit returns 429 with Retry-After, idempotency validated.\n<info added on 2025-08-24T15:55:59.641Z>\n- JWT payload requirements: user_id (UUID), license_id (UUID), tenant_id (UUID), scopes (array; must include models:write or models:read as appropriate), exp (Unix timestamp seconds), iat (Unix timestamp seconds), jti (UUID used for revocation). Tokens must be rejected if jti is present in the revocation store; tenant_id is enforced in downstream access checks.\n- Rate limiting implementation: Redis sliding window using Lua for atomic prune-count-increment. Keys: rate:{user_id}:{endpoint}:{window}. Use sorted sets with millisecond timestamps; prune older-than-window, count current, compare against limit, then add current event and set key TTL to window seconds. Endpoints identifiers: global and prompt. Limits enforced as specified (global 60/min, prompt 30/min). 429 includes Retry-After with seconds until window elapses and X-RateLimit headers (limit, remaining, reset).\n- API versioning: all endpoints under /api/v1/designs/*; support version negotiation via Accept header. Supported formats: application/json; version=1 and application/vnd.designs.v1+json. If an unsupported version is requested, return 406. Include API-Version: 1 in responses.\n- Pydantic v2: use Pydantic v2 models with field validators and constrained types for dimensions, units, and materials; strict types for UUIDs and enums for units/materials. Design input models use a discriminated union with discriminator field type having values prompt, params, upload, a4. OpenAPI must reflect oneOf with discriminator mapping. Add validators to normalize units, ensure positive dimensions, and cross-field checks (e.g., material compatible with process).\n</info added on 2025-08-24T15:55:59.641Z>\n<info added on 2025-08-25T20:21:06.935Z>\nFreeCAD core setup and worker constraints:\n- Version and headless: use FreeCAD 1.1.0 invoked via FreeCADCmd only (no GUI). Each model job runs FreeCADCmd in a subprocess; never embed FreeCAD in a multithreaded host.\n- Single-instance per process: one FreeCAD runtime per OS process. For parallelism, spawn isolated subprocesses (Celery worker -> fork/spawn child per job) with timeouts and stdout/stderr capture; kill on timeout and ensure no zombie processes.\n- Document management: create deterministic document names using job_id (e.g., Design_{job_id}); set as ActiveDocument; call doc.openTransaction(\"build\") before operations and doc.commitTransaction() or doc.abortTransaction() on failure; FreeCAD.ActiveDocument.recompute() before export; always App.closeDocument(name) and FreeCAD.setActiveDocument(\"\") after saving.\n- Deterministic artefact naming: artefacts saved as {job_id}_{artefact}.{ext} (e.g., 1234_model.fcstd, 1234_solid.step, 1234_mesh.stl).\n- File formats and modules:\n  - FCStd (native): doc.saveAs(path)\n  - STEP/IGES: Import.export([shape], path)\n  - STL/OBJ: Mesh.export([object], path)\n  - DXF/DWG: Draft.export([objects], path) (DWG requires system support if available)\n  - IFC: BIM/Ifc (IfcOpenShell-backed) export if module present\n- Configuration (persistent): use FreeCAD.ParamGet() to set required prefs (e.g., BaseApp/Preferences/Units, Import/Export options) at worker start; read for diagnostics in job logs.\n- Memory and cleanup: drop references to document objects, close documents, invoke gc.collect() after each job; ensure temporary files are deleted on both success and failure paths.\n- Error handling: catch FreeCAD.Base.FreeCADError and map to job errors (e.g., 422 for invalid geometry/unsupported format, 500 for unexpected engine errors) with masked details in logs; include first meaningful message from exception in job audit.\n- Transactions: wrap geometry creation/modification inside Document.openTransaction()/commitTransaction() blocks; on exceptions, abort and rollback before closing the document.\n- Acceptance: two concurrent model jobs run in separate FreeCADCmd subprocesses without interference, produce FCStd/STEP/STL (and others when applicable), and all documents are closed with memory reclaimed and logs containing deterministic document and file names.\n</info added on 2025-08-25T20:21:06.935Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement AI adapter (OpenAI/Azure) with timeouts, retries, PII masking, storage",
            "description": "Create a provider-agnostic AI adapter with configured limits and compliant logging/storage for prompt preprocessing.",
            "dependencies": [
              "7.1"
            ],
            "details": "Adapter interface: suggest_params(prompt:str, context:dict, max_tokens<=2000, timeout<=20s, retries=3 with exponential backoff+jitter); providers: OpenAI Chat Completions and Azure OpenAI; PII masking (emails, phones, names, addresses) before logging/storage; store masked raw prompt and response in ai_suggestions table (linked to models.params/job_id, request_id); enforce per-user 30/min in adapter as defense-in-depth; circuit breaker on repeated timeouts; configurable via env (provider, model, api_base, api_key, timeouts); acceptance: masked content in logs and DB, retries stop at 3 with backoff, timeouts cancel correctly, adapter returns deterministic schema or raises Ambiguous/Missing with codes.\n<info added on 2025-08-15T21:02:01.754Z>\nTurkish optimization and GPT4FreeCAD-inspired setup:\n\n- System prompt (inspired by revhappy/GPT4FreeCAD; tailored for FreeCAD Python API):\n  Sen FreeCAD Python API uzmanısın. Türkçe CAD tasarım isteklerini FreeCAD scriptlerine dönüştür. Part, Sketcher, Draft modüllerini kullan. Birimler mm, koordinat sistemi sağ el kuralı. Türkçe CAD terimlerini sözlükle İngilizce karşılıklarına eşle: vida→screw, flanş→flange, mil→shaft, yatak→bearing, dişli→gear. Belirsizlik varsa güvenli varsayılanları belirt ve requires_clarification=true alanını doldur. ÇIKTI KURALI: Yalnızca JSON döndür (markdown yok) ve alanlar: {language:\"tr\", units:\"mm\", intent:\"freecad_script\", glossary_used:true|false, parameters:{}, script_py:\"<FreeCAD Python>\", warnings:[], requires_clarification:true|false}.\n\n- Turkish CAD glossary (used in preprocessing and normalization):\n  vida→screw; flanş→flange; mil→shaft; yatak→bearing; dişli→gear.\n  Apply as case-insensitive hints; do not over-rewrite variable names inside code.\n\n- Few-shot exemplars (Turkish prompt → minimal FreeCAD Python script; adapter will wrap these as assistant JSON per schema during real requests):\n  1) Prompt: M8 vida deliği olan 20mm flanş\n     Script:\n     import FreeCAD as App, Part\n     doc = App.newDocument(\"flange\")\n     outer_d = 20.0\n     thickness = 5.0\n     hole_d = 8.5  # M8 clearance\n     body = Part.makeCylinder(outer_d/2.0, thickness)\n     hole = Part.makeCylinder(hole_d/2.0, thickness)\n     result = body.cut(hole)\n     Part.show(result)\n     doc.recompute()\n  2) Prompt: 3 adet dişli ile güç aktarım sistemi\n     Script:\n     import FreeCAD as App, Part, Base\n     doc = App.newDocument(\"gear_train\")\n     module = 1.0\n     teeth = [20, 40, 20]\n     thickness = 10.0\n     radii = [module*t/2.0 for t in teeth]\n     centers = [(0,0,0), (radii[0]+radii[1],0,0), (radii[0]+2*radii[1],0,0)]\n     for i,(r,c) in enumerate(zip(radii, centers)):\n         cyl = Part.makeCylinder(r, thickness)\n         cyl.translate(Base.Vector(*c))\n         Part.show(cyl)\n     doc.recompute()\n  3) Prompt: Ayarlanabilir mil çapı ve uzunluğu\n     Script:\n     import FreeCAD as App, Part\n     def make_shaft(d=12.0, L=80.0):\n         doc = App.ActiveDocument or App.newDocument(\"shaft\")\n         body = Part.makeCylinder(d/2.0, L)\n         Part.show(body)\n         doc.recompute()\n         return body\n     make_shaft()\n\n- OpenAI/Azure client configuration for this adapter path:\n  timeout=20s, retries=3 with exponential backoff + jitter; model and api_base configurable via env; pass user_locale=tr-TR; include system prompt above; enforce JSON-only response via response_format or output parser.\n\n- Response parser and normalization:\n  Expect JSON with fields: language, units, intent, glossary_used, parameters, script_py, warnings, requires_clarification.\n  Steps:\n  1) Strip markdown and non-JSON pre/post text.\n  2) Validate presence and types; else raise Missing(code=ERR-AI-422) or Ambiguous(code=ERR-AI-425).\n  3) Enforce units=mm; convert numeric dims if user used cm (detect tokens like “cm”, “metre”).\n  4) Apply glossary to parameters/intent only; do not mutate script identifiers.\n  5) Lint script_py: must import FreeCAD and use Part/Sketcher/Draft only; reject file I/O and os/system calls.\n  6) Attach warnings for assumed defaults (e.g., thickness=5mm, M8 clearance=8.5mm).\n  7) Store masked prompt/response for audit; persist normalized JSON and derived script.\n\n- PII masking and audit:\n  Mask emails, phones, names, addresses in Turkish/English before storage/logging; ensure masked content is what lands in audit; retain request_id and job linkage.\n\n- Test scenarios (Turkish):\n  - Basit geometri: \"M8 vida deliği olan 20mm flanş\" → script returns a 20mm disk with 8.5mm through-hole; units=mm; warnings may note default thickness.\n  - Karmaşık montaj: \"3 adet dişli ile güç aktarım sistemi\" → script creates 3 cylinders positioned by pitch radii; warnings note simplification (no involute teeth).\n  - Parametrik model: \"Ayarlanabilir mil çapı ve uzunluğu\" → script exposes parameters d and L; requires_clarification=false when both provided, true otherwise.\n\n- Acceptance additions for Turkish flow:\n  - adapter applies glossary and locale; model returns JSON-only; parser yields valid FreeCAD script_py or raises explicit Ambiguous/Missing with codes.\n  - timeouts and retries respected for tr-TR prompts.\n  - masked audit rows include original Turkish prompt and JSON response.\n</info added on 2025-08-15T21:02:01.754Z>\n<info added on 2025-08-25T19:59:12.750Z>\nFreeCAD 1.1 integration for AI adapter and parser:\n\n- Runtime version pin: verify FreeCADCmd == 1.1.0 at startup and before execution (parse `FreeCADCmd --version`); on mismatch fail fast with CONFIG_FREECAD_VERSION_MISMATCH (503) and skip callouts.\n- Headless support baseline: generate scripts compatible with FreeCAD 1.1 headless using modules Part, PartDesign, Sketcher, Import, Mesh; Assembly4 workbench constructs allowed via App/Link usage (no GUI).\n- System prompt update (tr-TR): emphasize FreeCAD 1.1 Python API expertise, Assembly4 and OndselSolver awareness, glossary mapping (vida→screw, flanş→flange, mil→shaft, yatak→bearing, dişli→gear), mm units, right-handed CS. Instruct JSON-only output with minimal schema: {language:\"tr\", units:\"mm\", script_py:\"<FreeCAD Python>\", parameters:{...}}. Keep few-shot examples (Turkish) and add an Assembly4/link-based placement exemplar.\n- Parser normalization: accept both legacy schema and the new minimal v1.1 schema; normalize to internal canonical structure (units=mm enforced; cm/metre tokens converted to mm). If minimal schema is received, backfill optional fields to defaults.\n- Script security (AST-based): whitelist imports FreeCAD, Part, PartDesign, Sketcher, Draft, Import, Mesh, math, numpy (restricted to ndarray creation/basic ops); forbid __import__, exec, eval, open, os, subprocess and any file/network I/O; reject Attribute/Call usage that reaches forbidden modules or builtins. Enforce numeric dimension literals and parameter-provided dimensions within 0.1–1000 mm; violations return SecurityViolation (ERR-AI-451).\n- Dimension guards: scan numeric constants in AST and parameters; auto-add parser warnings when defaults are assumed; clamp only in validation preview, never mutate silently.\n- Assembly4 and solver: allow creation of App::Link and placement transforms; do not require direct OndselSolver imports in scripts—solver integration occurs in execution environment. Reject any GUI-only API usage.\n- Standard parts integration (adapter-facing): permit parameters to reference DIN/ISO parts (e.g., parameters.standard_parts:[{code:\"DIN933\", size:\"M8x20\"}]) and FCStd template IDs (parameters.templates:[{id:\"flange_v3\", s3:\"s3://...\"}]); the generator/worker will resolve these via StandardPartsLibrary (no library imports inside script_py).\n- Manufacturing validation hooks: allow optional parameters.manufacturing_checks (min_wall_mm, min_draft_deg, tool_access=true|false). The adapter does not enforce geometry; it forwards to GeometryValidator in the worker.\n- Rate limits, retries, masking unchanged; ensure masked prompt/response include the new minimal schema JSON.\n- Implementation notes and integration points:\n  - Create apps/api/app/services/freecad/ with FreeCADScriptGenerator, GeometryValidator, StandardPartsLibrary; AI adapter will call FreeCADScriptGenerator only after parser/AST pass.\n  - Expose adapter profile \"freecad-1.1\" selecting the updated system prompt, schema, and AST rules.\n- Acceptance additions:\n  - FreeCADCmd version check passes; mismatch returns 503 without external calls.\n  - Adapter returns JSON-only in minimal schema for tr-TR; legacy schema still accepted; both normalize successfully.\n  - AST validator rejects forbidden imports/calls and out-of-bounds dimensions with ERR-AI-451; allowed modules pass.\n  - Scripts run headless in 1.1 with Part/PartDesign/Sketcher/Import/Mesh only; no file I/O detected.\n  - Parameters may include standard parts/templates and manufacturing_checks; values are preserved to downstream services.\n</info added on 2025-08-25T19:59:12.750Z>\n<info added on 2025-08-25T20:22:40.715Z>\nFreeCAD Part Workbench coverage for adapter profile \"freecad-1.1\"\n\n- System prompt enrichment (tr-TR): instruct the model to generate FreeCAD 1.1 headless scripts using Part/Sketcher/Draft that cover:\n  - Primitives: Box, Cylinder, Sphere, Cone, Torus, Wedge with parametric controls (dimensions in mm; valid range 0.1–1000).\n  - Boolean ops: Part.Shape.cut(), fuse(), common() with try/except and post-op validation (result.isValid(), not result.isNull()) and warnings on non-manifold/invalid results.\n  - Loft/Sweep: Part.makeLoft(profiles, solid=True/False, ruled=False/True) and sweeps using profiles (wires) and a path; create profiles via Sketcher or Part wires; avoid GUI.\n  - Chamfer/Fillet: shape.makeChamfer(r, edges) and shape.makeFillet(r, edges) with deterministic edge selection (by index list or geometric filters); no GUI selection.\n  - Thickness/Shell: shape.makeThickness(faces, t, join=0, tolerance=1e-3, makeSolid=True) for thin walls; document inward/outward via sign of t.\n  - Shape checks: isValid(), isClosed(), isNull(); attach warnings accordingly.\n  - Properties access: use shape.Volume, shape.Area, shape.CenterOfMass to compute metrics.\n  - Units: default mm; if user specifies inches (tokens: in, inch, inches, \"), convert to mm (×25.4) explicitly in parameters or pre-compute in script constants.\n\n- Parser/normalization updates:\n  - Accept minimal v1.1 schema; backfill legacy fields. When script references inches, normalize parameters to mm; record a units_conversion warning.\n  - Extend unit detection to inches (in/inch/inches/\") and composite tokens (e.g., 2in, 2\", 2 in).\n  - Normalize requested operations into parameters.operations list when present (e.g., [{op:\"fillet\", r:2, edges:[1,3]}], [{op:\"cut\", a:\"body\", b:\"hole\"}]).\n  - Introduce parameters.exports and parameters.mesh:\n    - exports: [{format:\"STEP\", ap:\"AP203\"|\"AP214\"} | {format:\"IGES\"} | {format:\"BREP\"}]\n    - mesh: {linear_deflection:float, angular_deflection_deg:float, relative:true|false}\n  - Profile/path schema for loft/sweep:\n    - profiles: [\"sketch:profile1\", \"wire:auto\"] and path: \"wire:path1\" or \"edge:auto\"; adapter does not resolve geometry—worker maps identifiers to created objects.\n  - Edge selection schema:\n    - edges: indices [int] or selectors [{by:\"length_gt\", value:mm}, {by:\"name\", value:\"Edge1\"}]; worker resolves against final shape.\n\n- AST/security enforcement (unchanged core rules):\n  - Allow Part, Sketcher, Draft, Import, Mesh; still forbid any file/network I/O and dangerous builtins. Explicitly reject Import.export/Import.exportStep/Import.exportIGES usage inside script (ERR-AI-451); exports occur only in worker based on parameters.exports.\n  - Permit shape.tessellate() for preview-only; forbid MeshPart and any disk writes. Mesh file generation happens in worker.\n\n- Worker integration expectations (adapter-facing parameters pass-through):\n  - Boolean validity: after executing script, worker runs GeometryValidator (isValid/isNull), rejects non-manifold/invalid results with ERR-AI-451 or attaches warnings depending on endpoint policy.\n  - Exports: worker performs STEP (AP203/AP214), IGES, BREP via Import in a controlled path; script never writes files.\n  - Meshing: worker generates meshes with specified tessellation; no MeshPart import required if using shape.tessellate or controlled utilities.\n  - Metrics: worker computes and stores Volume, Area, CenterOfMass if not provided by script.\n\n- Acceptance additions:\n  - AI outputs scripts that create requested Part primitives and apply booleans, loft/sweep, chamfer/fillet, and thickness without GUI APIs; scripts pass AST validator.\n  - Non-manifold/invalid results are detected; either warnings are included (legacy schema) or worker returns a structured validation error; adapter does not silently accept invalid shapes.\n  - Inch inputs are correctly converted to mm during normalization; units=mm enforced in stored canonical JSON.\n  - parameters.exports and parameters.mesh are preserved to downstream; scripts contain no file I/O; any attempt to call Import.export is blocked with ERR-AI-451.\n  - Properties access via .Volume/.Area/.CenterOfMass is allowed and does not trigger security violations.\n\n- Test scenarios:\n  - Primitives: Box(10×20×5 mm), Cylinder(d=12 mm, h=30 mm), Wedge with top/bottom offsets; validate volumes.\n  - Boolean: Fuse two cylinders, cut through-hole; invalid self-intersection triggers warning/error.\n  - Loft: Two circular profiles at z=0 and z=30; solid=True; result isValid() and isClosed() true.\n  - Sweep: Rectangular profile along S-curve path; result passes validator.\n  - Fillet/Chamfer: Apply r=2 mm on edges [1,3,5]; edges exist and operation succeeds.\n  - Thickness: Shell a box outward by 1.5 mm excluding top face; result solid and valid.\n  - Export: Request STEP AP214 and IGES via parameters.exports; worker produces files; script has no export calls.\n  - Mesh: Request linear_deflection=0.1, angular_deflection=15; mesh artifact generated by worker; no MeshPart import in script.\n  - Units: Input \"2 inch pipe with 1/8 inch wall\" normalizes to mm and passes bounds check.\n</info added on 2025-08-25T20:22:40.715Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build normalize() and validate() deterministic rules engine",
            "description": "Implement canonicalization and validation for prompts and parametric inputs with strict error codes.",
            "dependencies": [
              "7.1"
            ],
            "details": "Normalization: units→mm (exact factors), ordered keys, numeric rounding (e.g., 1e-6), default units/material if provided context, string trims, enum casing; Validation: required fields (dimensions L,W,H or relevant set, units, material, machine), ranges (min wall thickness, inner radius > 0), material↔machine compatibility; Ambiguity detection (multiple interpretations) → HTTP 425; missing essential info → 422 with code ERR-AI-422 AI_HINT_REQUIRED; output canonical_params JSON used by workers and cache keys; acceptance: unit tests cover valid, ambiguous, and missing cases returning exact codes and messages deterministically.\n<info added on 2025-08-15T21:04:01.108Z>\nExtend normalize()/validate() with GPT4FreeCAD-style “script mode” for AI-generated FreeCAD Python scripts (Turkish CAD script security).\n\nScope\n- Applies when input contains a FreeCAD Python script (from prompt adapter or upload). Produces canonical_script and script_meta alongside canonical_params. Deterministic outputs drive worker cache keys (script_hash = sha256(canonical_script)).\n\nAllowed/forbidden\n- Allowed imports: FreeCAD (aliased as App), Part, Sketcher, Draft, math, numpy (numeric-only).\n- Forbidden names/operations: __import__, exec, eval, open, file, os, subprocess, sys.exit (any access), dynamic import patterns, writing files, spawning processes.\n- Numpy allowlist (attribute access limited to): array, asarray, linspace, arange, zeros, ones, sqrt, sin, cos, tan, pi, dot, cross, clip, maximum, minimum, abs, floor, ceil, round. Any other numpy attribute → SECURITY_VIOLATION.\n\nNormalization (deterministic, idempotent)\n1) Ensure imports (prepend if missing, keep single copies):\n   - import FreeCAD as App\n   - import Part\n   - Optional pass-through (present but not added): import Sketcher, import Draft, import math, import numpy as np or import numpy\n2) Ensure document:\n   - If no new/active doc used: insert doc = App.newDocument()\n   - Else if a document exists but no variable doc: insert doc = App.ActiveDocument\n3) Ensure display:\n   - If a top-level variable named result is created and is used as a shape/object, append Part.show(result) if missing\n4) Ensure recompute:\n   - Append doc.recompute() if missing at end of script\n5) Unit normalization to mm:\n   - Recognize and convert the following to mm, rounding to 1e-6:\n     a) Identifier suffixes: *_cm → value*10; *_inch or *_in → value*25.4\n     b) Inline unit comments on assignments/call args: “… = 12  # cm/inch/in”\n     c) Helper-like calls cm(x), inch(x) → replaced with numeric mm literal (if present)\n   - Update variable names by removing unit suffixes after conversion\n6) Comment translation:\n   - Translate Turkish comments to English via glossary-based replacement (code unchanged). Minimal glossary: uzunluk→length, genişlik→width, yükseklik→height, yarıçap→radius, duvar kalınlığı→wall thickness, birim→unit, mm→mm, cm→cm, inç→inch, hata→error, uyarı→warning\n7) Key ordering and whitespace:\n   - Stable import ordering (FreeCAD/Part first), strip trailing spaces, ensure newline at EOF\n\nValidation\n1) Syntax (AST):\n   - Parse with Python AST. On SyntaxError → INVALID_SYNTAX\n2) Security (AST-based, name/attr/use):\n   - Reject forbidden builtins/names and any access to os, subprocess, sys.exit (direct or via alias)\n   - Reject dynamic code execution (exec, eval, __import__)\n   - Enforce import allowlist; any other import → SECURITY_VIOLATION\n   - Enforce numpy allowlist; any disallowed attribute access on numpy/np → SECURITY_VIOLATION\n3) FreeCAD API compatibility (version target: FreeCAD 1.1.x):\n   - Resolve attribute chains (e.g., Part.makeBox). If attribute missing → API_NOT_FOUND\n   - Deprecated methods produce non-fatal warnings (API_DEPRECATED) with suggested replacements (maintain internal deprecation map); still pass validate unless also missing\n4) Dimension limits (mm):\n   - Extract lengths from known constructors and operations: Part.makeBox(L,W,H), Part.makeCylinder(r,h), Part.Face/Edge creation with lengths, Sketcher distances where literal\n   - After unit normalization, each positive dimension must be 0.1 ≤ value_mm ≤ 1000; else → DIMENSION_ERROR\n5) Timeout:\n   - Sandbox execution budget 20s CPU/wall (worker enforces). Exceeding budget → TIMEOUT_ERROR\n\nError codes, messages (Turkish, deterministic)\n- INVALID_SYNTAX (HTTP 400): Python sözdizimi hatası: {details}. Çözüm: satır {lineno} yakınındaki hatayı düzeltin.\n- SECURITY_VIOLATION (HTTP 403): Güvenlik ihlali: yasaklı komut/modül kullanımı tespit edildi: {symbol}. Öneri: yalnızca izin verilen modülleri (FreeCAD, Part, Sketcher, Draft, math, numpy) ve güvenli API’leri kullanın.\n- API_NOT_FOUND (HTTP 422): API bulunamadı: {qualname} FreeCAD {version} içinde yok veya erişilemez. Öneri: güncel API’yi kullanın: {suggestion}.\n- DIMENSION_ERROR (HTTP 422): Boyut limiti aşıldı: {name}={value_mm} mm (izin: 0.1–1000 mm). Öneri: değeri aralığa çekin.\n- TIMEOUT_ERROR (HTTP 504): Zaman aşımı: script 20 saniyeyi aştı. Öneri: hesaplamayı basitleştirin veya yinelemeyi sınırlandırın.\n\nOutputs on success (augment canonical_params)\n- canonical_script: normalized script text\n- script_meta: {\n  modules_used: [...],\n  conversions_applied: [{from_unit, to_unit, before, after, location}],\n  api_warnings: [API_DEPRECATED …],\n  dims_mm: {L, W, H, r, h, … when inferable},\n  script_hash: sha256(canonical_script)\n}\n\nDeterminism\n- All normalization edits are structural (AST-to-source or regex with anchor rules) and idempotent.\n- Error messages include fixed templates and stable field ordering; numbers rounded to 1e-6.\n\nTesting (expand unit tests)\n- Syntax error sample → INVALID_SYNTAX with exact Turkish template\n- Forbidden exec/eval/open/os/subprocess/sys.exit → SECURITY_VIOLATION\n- Disallowed import (e.g., json) or numpy attribute (e.g., np.linalg.solve) → SECURITY_VIOLATION\n- Missing imports/doc/show/recompute auto-inserted; idempotent on second run\n- Unit conversions: _cm, _inch suffixes and inline comments converted to mm; values rounded; variable names normalized\n- API missing (e.g., Part.makeBoxx) → API_NOT_FOUND; deprecated method → warning only\n- Dimension limits: values outside [0.1, 1000] mm → DIMENSION_ERROR; boundary values pass\n- Timeout enforced in worker harness → TIMEOUT_ERROR\n- Comments translated TR→EN in output; code semantics unchanged\n- Snapshot of canonical_script hashed; cache key stable across runs with same logical script\n</info added on 2025-08-15T21:04:01.108Z>\n<info added on 2025-08-25T20:24:31.095Z>\nExtend script mode to support FreeCAD PartDesign Workbench (Bodies, Sketcher constraints, features, patterns, datums, booleans) with deterministic normalization/validation.\n\nScope and allowlist\n- Detect PartDesign usage when script creates/uses any of: PartDesign::Body, PartDesign::Pad, PartDesign::Pocket, PartDesign::Revolution, PartDesign::Groove, PartDesign::AdditiveLoft, PartDesign::SubtractiveLoft, PartDesign::AdditivePipe, PartDesign::SubtractivePipe, PartDesign::LinearPattern, PartDesign::PolarPattern, PartDesign::Mirrored, PartDesign::Boolean, PartDesign::DatumPlane, PartDesign::DatumLine, PartDesign::DatumPoint, Sketcher::SketchObject and Sketcher.Constraint.\n- Allowed Sketcher constraint types (allowlist): Distance, Angle, Coincident, Parallel, Perpendicular, Horizontal, Vertical, Tangent, Equal, Symmetric. Any other constraint name → CONSTRAINT_UNSUPPORTED.\n- Multibody is allowed at document level; enforce PartDesign “single solid per Body” rule for each Body independently.\n\nNormalization (idempotent)\n- Ensure a document Body context: if PartDesign features are used and no active Body exists, insert body = App.ActiveDocument.addObject('PartDesign::Body','Body') and activate if needed; do not move existing objects.\n- Ensure recompute at end still applies; if a Body exists and has features, set body.ViewObject.Visibility = True is not forced (visual only); we do not insert GUI calls.\n- If a PartDesign Body is created and a “Tip” is not set by script, set it to the last created PartDesign feature in that Body (deterministically by creation order) to keep a stable resulting solid.\n- Units: continue mm normalization for all linear properties on PartDesign features (Pad.Length, Pocket.Length, Pocket.Offset, Revolution.Axis placement distances, Loft/Pipe section-driven numeric distances, Datum offsets). Round to 1e-6. Angles are left in degrees; see validation for ranges.\n- Sketcher comments translation (TR→EN) still applies; constraint names in comments are not modified.\n- Stable ordering: keep imports first, then document/body creation, then sketches, then features, then patterns/booleans; do not reorder user logic beyond minimal insertions above.\n\nValidation (deterministic)\n- Syntax and security rules unchanged; reject any disallowed imports or dynamic code.\n- API compatibility: verify addObject class names exist in FreeCAD 1.1.x; missing → API_NOT_FOUND.\n- Sketch constraints: verify each added constraint type is in the allowlist; else → CONSTRAINT_UNSUPPORTED (HTTP 422).\n- Sketch ambiguity: compute degrees of freedom (if accessible via SketchObject.DegreeOfFreedom or solver result after recompute). If DoF > 0 → SKETCH_UNDERCONSTRAINED (HTTP 425). If overconstrained or conflicting → INVALID_SYNTAX if thrown by solver, else SKETCH_UNDERCONSTRAINED with details.\n- Feature parameter extraction and limits (post mm normalization):\n  - Pad/Pocket length, Groove/pipe/loft section distances: 0.1 ≤ value_mm ≤ 1000; else → DIMENSION_ERROR.\n  - Draft/taper angles on Pad/Pocket/Pipe/Loft: -45 ≤ angle_deg ≤ 45; else → ANGLE_ERROR.\n  - Revolution total angle: 0 < angle_deg ≤ 360; else → ANGLE_ERROR.\n  - Pattern features:\n    - LinearPattern occurrences: 1–1000; spacing_mm: 0.1–1000; else → PATTERN_ERROR.\n    - PolarPattern occurrences: 1–1000; angle sweep: 0–360; else → PATTERN_ERROR.\n    - Mirrored: requires valid mirror plane/axis reference; missing/invalid → PATTERN_ERROR.\n- Feature tree integrity:\n  - Each PartDesign feature must reference a valid parent/support within the same Body (Sketch or previous feature). Missing parent or cross-body misuse (except Boolean) → FEATURE_DEPENDENCY_ERROR (HTTP 422).\n  - No cycles allowed in feature dependency graph; detect and reject → FEATURE_DEPENDENCY_ERROR.\n  - Tip must be a PartDesign feature in the Body; if set to non-feature or invalid → FEATURE_DEPENDENCY_ERROR.\n- Single solid rule:\n  - After recompute, evaluate each Body’s resultant shape; if a Body contains more than one separate solid at its Tip → SINGLE_SOLID_VIOLATION (HTTP 422).\n- Boolean operations between bodies:\n  - Only PartDesign::Boolean at Body level is accepted; confirm target Body and tool Body exist; operation result must maintain a single solid in the target Body → else BOOLEAN_BODY_ERROR (HTTP 422).\n\nError codes, messages (Turkish, deterministic)\n- CONSTRAINT_UNSUPPORTED (HTTP 422): Desteklenmeyen Sketcher kısıtı: {name}. Öneri: izin verilen kısıtları kullanın: Distance, Angle, Coincident, Parallel, Perpendicular, Horizontal, Vertical, Tangent, Equal, Symmetric.\n- SKETCH_UNDERCONSTRAINED (HTTP 425): Belirsizlik: eskiz yeterince kısıtlanmadı (DoF={dof}). Öneri: eksik kısıtları ekleyin veya boyutları netleştirin.\n- FEATURE_DEPENDENCY_ERROR (HTTP 422): Özellik bağımlılığı hatası: {feature} → {parent} eksik, geçersiz veya döngü tespit edildi. Öneri: özellikleri Body içinde sıralı inşa edin ve destekleri doğru ayarlayın.\n- SINGLE_SOLID_VIOLATION (HTTP 422): Tek katı kuralı ihlali: Body '{body}' birden fazla ayrı katı içeriyor. Öneri: özellikleri birleştirerek tek bir sürekli katı oluşturun veya ayrı Body kullanın.\n- ANGLE_ERROR (HTTP 422): Açı limiti aşıldı: {name}={value_deg}° (izin: {min_deg}–{max_deg}°). Öneri: değeri aralığa çekin.\n- PATTERN_ERROR (HTTP 422): Dizilim özelliği hatası: {param}={value}. Öneri: adet 1–1000, aralık 0.1–1000 mm, açı 0–360° olmalıdır.\n- BOOLEAN_BODY_ERROR (HTTP 422): Body boolean hatası: '{target}' ile '{tool}' arasında geçersiz işlem veya çoklu katı oluşumu. Öneri: Body düzeyinde Boolean kullanın ve tek katı kuralını koruyun.\n\nOutputs on success (augment script_meta)\n- modules_used includes PartDesign and Sketcher if present.\n- partdesign_features: [{name, class, body, params: {length_mm, depth_mm, angle_deg, draft_deg, spacing_mm, occurrences, sweep_angle_deg, ...}}]\n- sketches: [{name, body, constraint_counts: {Distance, Angle, Coincident, Parallel, Perpendicular, Horizontal, Vertical, Tangent, Equal, Symmetric}, dof}]\n- bodies: {count, names: [...], tip_map: {body: tip_name}, single_solid_ok: true/false}\n- datums: {planes, axes, points} with offsets_mm where applicable\n- booleans: [{target_body, tool_bodies: [...], operation: Fuse/Cut/Common, result_single_solid: true/false}]\n- dims_mm extended with feature-derived values (pad_length, pocket_depth, revolution_angle_deg, draft_deg, pattern_spacing, etc.)\n\nTesting (expand unit tests)\n- Underconstrained sketch (DoF>0) → SKETCH_UNDERCONSTRAINED (HTTP 425) with exact DoF in message.\n- Unsupported constraint (e.g., Concentric) → CONSTRAINT_UNSUPPORTED.\n- Pad length out of bounds and taper angle out of bounds → DIMENSION_ERROR and ANGLE_ERROR respectively.\n- Feature dependency missing/cycle → FEATURE_DEPENDENCY_ERROR.\n- Single solid violation in a Body (e.g., two separate pads on parallel sketches) → SINGLE_SOLID_VIOLATION.\n- LinearPattern invalid spacing or occurrences → PATTERN_ERROR; PolarPattern angle >360 → PATTERN_ERROR.\n- Boolean between bodies that yields multiple solids in target → BOOLEAN_BODY_ERROR; valid fuse → success.\n- Tip auto-set when missing; idempotent normalization on second run.\n- Metadata captures features, constraints, bodies, datums, booleans; numbers rounded to 1e-6.\n</info added on 2025-08-25T20:24:31.095Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Celery job orchestration and lifecycle for model flows",
            "description": "Wire API to Celery tasks, define queues, idempotency, structured logging, and job status transitions.",
            "dependencies": [
              "7.1"
            ],
            "details": "Queues: models.prompt, models.params, models.upload, assemblies.a4; task signature includes job_id, request_id, user_id, canonical_params/input_ref; Celery worker options: acks_late, task_time_limit and soft_time_limit, visibility_timeout; retries for transient storage/queue errors only; dead-letter via RabbitMQ DLX; structured logs with request_id; CLI: celery -A app.celery_app worker -Q models.prompt,models.params,models.upload,assemblies.a4 -Ofair -c 2 --prefetch-multiplier=1; acceptance: job created with idempotency, transitions queued→running→succeeded/failed, logs contain request_id, DLQ receives poisoned messages.\n<info added on 2025-08-25T20:31:29.929Z>\nFreeCAD FEM/Simulation implementation (FEM Workbench, “Sonlu Elemanlar Analizi”) to run headless under Celery and produce deterministic, reproducible results:\n\n- Supported analyses (Türkçe in parens):\n  - Linear static (Doğrusal statik)\n  - Modal/eigenfrequency (Modal/özfrekans)\n  - Linear buckling (Doğrusal burkulma)\n  - Steady-state thermal (Durulmuş ısıl)\n  - Transient thermal (Zamansal ısıl)\n  - Sequential thermo-structural coupling (Ardışık ısıl→mekanik bağlama)\n  - Optional: Nonlinear static with small-strain contact where feasible in CalculiX (Doğrusal olmayan statik)\n\n- Canonical inputs (normalize/validate applies before enqueue):\n  - model_ref or input_ref to shape(s); unit_system (SI default)\n  - analysis_type ∈ {static, modal, buckling, thermal_steady, thermal_transient, coupled_thermal_static}\n  - material set per body/part (malzeme: E, ν, ρ, α, k, c_p; Türkçe: Young modülü, Poisson oranı, yoğunluk, genleşme katsayısı, ısı iletkenliği, özgül ısı)\n  - constraints and loads (kısıtlar ve yükler) with magnitude, direction, faces/edges/nodes selection, coordinate system\n  - mesh settings (ağ/mesh): mesher, global_size, min_size, second_order, local_fields, growth_rate, quality_targets\n  - solver settings (çözücü): backend, max_steps, tolerance, nlgeom, time_steps (thermal transient), damping (modal), threads\n  - outputs: fields, probes, sections, images, vtk, csv, foS\n\n- Constraint and load types mapped to FreeCAD FEM objects (Türkçe):\n  - Fixed support (Sabit kısıt)\n  - Displacement/rotation BC (Yer değiştirme/Dönme kısıtı) with components\n  - Force (Kuvvet) vector or normal to face\n  - Pressure (Basınç) on faces\n  - Moment/Torque (Moment/Tork)\n  - Remote load/support with reference point (Uzak yük/destek)\n  - Gravity/Self-weight (Ağırlık)\n  - Symmetry plane (Simetri düzlemi) and Roller/Slider (Mafsallı/tekerlekli kısıt)\n  - Bearing load (Yatak yükü) for shafts\n  - Temperature (Sıcaklık), Heat flux (Isı akısı), Convection h·(T∞−T) (Taşınım sınır koşulu)\n  - Contact: Bonded/Tie (Yapışık), Frictionless (Sürtünmesiz) where supported by CalculiX\n\n- Mesh control (Gmsh preferred; Netgen fallback):\n  - Global target size from bounding-box heuristics unless specified\n  - Second-order elements (İkinci mertebe) on by default for structural; first-order for thermal unless gradients require second-order\n  - Curvature- and proximity-based refinement; local sizing by selection groups (delik kenarları/fillet bölgeleri)\n  - Growth rate limit, min/max element size, surface/volume refinement\n  - Quality checks: aspect ratio, minimum Jacobian; fail fast if below thresholds; auto-relax targeting if mesh fails twice\n  - Mesher selection: gmsh|netgen via canonical_params.mesh.mesher\n\n- Solver configuration (CalculiX ccx default; Elmer optional when installed):\n  - Static: linear by default; nlgeom toggled if large loads or contact; iterative control (tolerance, max_steps)\n  - Modal: number_of_modes, frequency_range; mass-scaling disabled by default\n  - Buckling: reference from converged static preload; extract critical load factors\n  - Thermal (steady/transient): time stepping (Δt, total_time), stabilization where needed\n  - Parallelism via OMP_NUM_THREADS; memory cap enforced; write .inp with node/element sets named by UUID for traceability\n  - Output frequency and fields tuned per analysis to balance size vs insight\n\n- Headless execution plan (FreeCADCmd pipeline):\n  - Load FCStd/STEP; rebuild shape; ensure units coherent (SI)\n  - Create Analysis container; attach Solver object (CalculiX)\n  - Assign materials (App::MaterialObject) to solids; verify density for gravity if used\n  - Build BCs/loads from canonical selections (by persistent SubElement names); auto-orient pressure normals\n  - Generate mesh (FemMeshGmsh); apply local regions; set secondOrder flag as required\n  - Write solver input; run ccx with bounded time and CPU; stream stdout/stderr to structured logs (request_id)\n  - Parse results (.frd/.dat or VTK); create FemResultObject; compute derived fields (Von Mises/Esnek gerilme, principal stresses, displacement magnitude)\n  - Export artefacts: FCStd with Analysis, .inp, .frd/.dat, .vtk/.vtu/.vtm, CSV probes, PNG snapshots (deformed, stress maps), JSON summary\n  - Cleanup temporary files; attach metrics; update progress across phases: setup→meshing→solve→post→archive\n\n- Results processing and metrics (Sonuç işleme):\n  - Summary JSON: max_von_mises, max_disp, max_temp, eigenfrequencies[], buckling_factors[], reaction_forces, energy, mass\n  - Factor of Safety (Emniyet katsayısı): if yield_strength provided, min(σ_y/σ_vM) over domain; else null with hint\n  - Probes: user-defined points/paths; sections: clip planes for images\n  - Image presets: undeformed + deformed (scale auto), stress/temperature contour with legend; unit labels in SI\n  - Validation: result sanity checks (nonzero stiffness, no rigid-body modes in constrained static); flag singular models with actionable messages\n\n- Multi-physics coupling (Çok-fizikli bağlama):\n  - Sequential thermal→structural: run thermal, map nodal temperatures to structural load; ensure matching meshes or interpolate\n  - Modal with pre-stress: optional static preload prior to eigenfrequency\n  - Elmer path reserved for future true multi-physics; current default is CalculiX sequential coupling\n\n- Celery integration for FEM (queue and lifecycle):\n  - Queue: sim.fem (DLX: sim.fem_dlx → sim.fem_dlq); soft_time_limit and hard time_limit sized by analysis_type and mesh size\n  - Task signature extends existing: job_id, request_id, user_id, model_ref, canonical_params.sim\n  - Retries only on transient I/O; solver/model errors are non-retryable; poisoned messages route to DLQ\n  - Progress updates at defined milestones; structured logs include analysis_type, model_ref, mesher, ccx_version\n\n- Deterministic normalize/validate for FEM inputs (Ön-doğrulama kuralları):\n  - Units normalized to SI; reject mixed unit systems\n  - Required: at least one BC fixing all rigid-body DOFs for static; temperature/convection well-posed for thermal\n  - Geometry selections must resolve to existing faces/edges; empty selections → 422 with hints\n  - Mesh: size bounds relative to smallest feature; min_size ≥ feature_min/10; second_order true for static/modal unless explicitly disabled\n  - Solver caps: max elements, max nodes; if exceeded → 409 SIM_LIMIT_EXCEEDED with recommendation\n  - Contact pairs require non-overlapping, compatible regions; otherwise demote to bonded with warning if allowed\n\n- Artefact storage and naming:\n  - sim/{job_id}/analysis.fcstd, model.inp, result.frd, result.dat, result.vtu|vtm, result.csv, report.json, preview_stress.png, preview_disp.png, logs.txt\n  - SHA256 recorded for each; signed URLs returned via job artefacts\n\n- Turkish terminology quick map for UI/logs:\n  - Constraint (Kısıt), Load (Yük), Boundary condition (Sınır koşulu), Mesh (Ağ), Element (Eleman), Node (Düğüm), Solver (Çözücü), Result (Sonuç), Stress (Gerilme), Strain (Şekil değiştirme), Displacement (Yer değiştirme), Temperature (Sıcaklık), Heat flux (Isı akısı), Convection (Taşınım), Contact (Temas), Buckling (Burkulma), Modal (Modal/Özdeğer), Factor of Safety (Emniyet katsayısı)\n\nAcceptance additions:\n  - Reproduce reference problems (cantilever beam static, 3D bracket modal, plate buckling, heat sink thermal) within tolerance vs analytical/benchmark\n  - Artefacts present and downloadable; JSON summary validates against sim_report schema; logs include request_id and solver metadata\n  - DLQ receives failed solves with structured error and minimal reproduction info (mesher, sizes, counts, ccx exit code)\n</info added on 2025-08-25T20:31:29.929Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "FreeCAD worker container image and execution harness",
            "description": "Create container with FreeCADCmd 1.1.x and a harness that executes modeling tasks under resource limits.",
            "dependencies": [
              "7.4"
            ],
            "details": "Dockerfile installs FreeCADCmd 1.1.x, Python 3.11, packages: numpy, trimesh, pygltflib (if used), minio SDK; non-root user; execution harness worker_script.py parses args, sets ulimit (CPU seconds) and cgroups (RAM), sets nice/ionice; invocation: FreeCADCmd -c /app/worker_script.py --flow {prompt|params|upload|a4} --input /work/input.json --outdir /work/out --request-id {uuid}; temp workspace isolated per job; acceptance: container builds in CI, FreeCADCmd available, harness enforces time/memory limits and exits with non-zero on violation.\n<info added on 2025-08-24T15:57:18.229Z>\n- Pin exact FreeCAD version: FreeCADCmd 1.1.0 (no wildcard). Lock via package manager pin or artifact checksum; verify at build with FreeCADCmd --version == 1.1.0.\n- requirements.txt (pinned):\n  - numpy==1.24.3\n  - trimesh==4.0.1\n  - pygltflib==1.16.1\n  - minio==7.2.0\n  - psutil==5.9.8\n- Health check server: lightweight HTTP endpoint exposed by the harness (threaded) on PORT (default 8080). GET /health/freecad returns 200 JSON including:\n  - freecad_version: “1.1.0” (from FreeCADCmd --version or FreeCAD.Version())\n  - python_version\n  - packages: {numpy, trimesh, pygltflib, minio, psutil} with installed versions\n  - status: “ok”\n  Enable with --health-server flag or HEALTH_SERVER=1; container EXPOSE 8080 for k8s liveness/readiness.\n- Resource monitoring: integrate psutil to sample per-process CPU percentage and RSS memory (MB) at a configurable interval (default 2s). Emit metrics into:\n  - periodic progress updates (jobs.progress.meta.resource: cpu_pct, rss_mb)\n  - final job metrics blob\n  Respect cgroup limits; if RSS approaches limit, emit WARN and throttle if configured; always include a final peak_rss_mb and avg_cpu_pct.\n- Multi-stage Docker build for size optimization:\n  - Stage “builder”: install build deps, fetch/install FreeCAD 1.1.0, build Python wheels (pip wheel -r requirements.txt).\n  - Stage “runtime” (slim base): copy FreeCAD runtime, site-packages from wheels, worker_script.py, and minimal shared libs; run as non-root.\n  - Use pip --no-cache-dir, remove *.a, tests, __pycache__, docs; apt-get clean and rm -rf /var/lib/apt/lists/*; strip binaries where safe.\n  - CI step asserts no compiler/build tools present in final image and validates FreeCADCmd --version == 1.1.0.\n- Acceptance additions:\n  - requirements.txt matches pinned versions above; pip freeze in CI contains those exact versions.\n  - GET /health/freecad returns 200 with freecad_version “1.1.0” and package versions.\n  - Logs/progress include psutil metrics cpu_pct and rss_mb (MB) during runs; final metrics include peak_rss_mb.\n  - Final image built via multi-stage; FreeCADCmd available; size reduced vs single-stage and contains no build toolchain.\n</info added on 2025-08-24T15:57:18.229Z>\n<info added on 2025-08-25T20:32:34.607Z>\nAdditions: comprehensive worker container implementation\n\nFreeCAD 1.1.0 pinning and runtime\n- Use FreeCAD 1.1.0 AppImage artifact (x86_64) in builder stage; verify SHA256 checksum and fail build if mismatch. Extract with: FreeCAD_1.1.0.AppImage --appimage-extract\n- Copy only extracted usr/ subtree into runtime; set PATH=/opt/freecad/usr/bin:$PATH and symlink /usr/local/bin/FreeCADCmd -> /opt/freecad/usr/bin/FreeCADCmd\n- Build arg FREECAD_SHA256 to lock artifact; CI validates FreeCADCmd --version == 1.1.0 exactly and checksum match\n- Headless settings in runtime: FC_NO_UI=1, QT_QPA_PLATFORM=offscreen (fallback to minimal), OMP_NUM_THREADS=1, OPENBLAS_NUM_THREADS=1 to avoid CPU spikes\n- Include minimal runtime libs for headless TechDraw (Qt, OCC, freetype, fonts); install basic fonts (e.g., fonts-dejavu-core) so TechDraw templates render correctly\n\nMulti-stage Dockerfile (concrete shape)\n- Stage builder:\n  - apt-get install only what’s needed to run AppImage extraction and build wheels\n  - curl -L FreeCAD_1.1.0.AppImage; echo \"$FREECAD_SHA256  file\" | sha256sum -c -\n  - chmod +x; ./FreeCAD_1.1.0.AppImage --appimage-extract; mv squashfs-root /opt/freecad\n  - pip wheel -r requirements.txt; strip symbols where safe\n- Stage runtime:\n  - base: debian:bookworm-slim (or distroless with glibc if feasible)\n  - apt-get install: libgl1, libxrender1, libxext6, libfontconfig1, libfreetype6, fonts-dejavu-core, ca-certificates (no compilers)\n  - COPY from builder: /opt/freecad, built wheels, worker_script.py, templates/\n  - pip install --no-cache-dir wheels/*; remove tests, __pycache__\n  - useradd -m freecad; USER freecad; WORKDIR /work; EXPOSE 8080\n  - ENTRYPOINT [\"FreeCADCmd\",\"-c\",\"/app/worker_script.py\"]\n- CI asserts:\n  - no gcc/clang/make in final image\n  - FreeCADCmd --version outputs exactly 1.1.0\n  - image size reduced vs single-stage baseline\n\nHealth checks (extended)\n- Harness flag/env: --health-server or HEALTH_SERVER=1 to enable\n- GET /health/freecad returns 200 JSON with:\n  - freecad_version, python_version, packages and versions (numpy, trimesh, pygltflib, minio, psutil)\n  - techdraw: true if `import TechDraw` succeeds\n  - headless_ok: true if `App.newDocument(); App.closeDocument()` works without GUI\n  - cgroups: {memory_limit_mb, cpu_quota, cpu_period}\n  - status: \"ok\" when all checks pass\n- Implementation: do not open any viewers; only module import and lightweight document create/close\n\nResource monitoring and limits\n- psutil loop at configurable interval (default 2s) capturing:\n  - cpu_pct (process + children), rss_mb, io_counters (optional), num_threads\n  - cgroup memory limit detection; warn when rss_mb > 85% of limit; throttle via time.sleep(backoff) if THROTTLE_ON_PRESSURE=1\n- Hard limits:\n  - RLIMIT_CPU set from --cpu-seconds; on softlimit signal, emit progress WARN and exit non-zero\n  - Enforce memory via cgroup; on OOM impending (rss > 95% limit) attempt graceful abort with clear error code ERR-OOM\n- Emit metrics in progress meta and final metrics: {avg_cpu_pct, peak_rss_mb, wall_ms}\n\nTechDraw integration (headless technical drawings)\n- Add optional drawing generation to all flows via flags or input.json:\n  - CLI flags: --techdraw=on|off (default on), --td-template=/app/templates/A4_Landscape.svg, --td-views=Front,Right,Top,Isometric, --td-scale=1.0, --td-dpi=300, --td-fmt=pdf,svg\n  - Input override (input.json): techdraw {enabled, template, views[], scale, dpi, formats[]}\n- Implementation sketch inside worker:\n  - Open or create FCStd; ensure Part/Body or Shape exist\n  - import TechDraw; doc = App.ActiveDocument or App.newDocument()\n  - page = doc.addObject(\"TechDraw::DrawPage\",\"Page\"); template = doc.addObject(\"TechDraw::DrawSVGTemplate\",\"Template\"); template.Template = td_template_path; page.Template = template\n  - For each requested view name: v = doc.addObject(\"TechDraw::DrawViewPart\",\"ViewX\"); v.Source = [targetObject]; v.Direction = preset (Front=[0,0,1], Right=[1,0,0], Top=[0,1,0], Isometric=[1,1,1]); v.Scale = td_scale; page.addView(v)\n  - doc.recompute()\n  - Export: pdf_path = outdir + \"/drawing.pdf\"; svg_path = outdir + \"/drawing.svg\"; TechDraw.writePageAsPdf(page, pdf_path); TechDraw.writePageAsSvg(page, svg_path) as per requested formats\n  - Save FCStd after adding TechDraw page so drawing is embedded\n- Headless robustness:\n  - Ensure QT_QPA_PLATFORM offscreen; ensure fonts are present; fall back to built-in template if provided template missing\n  - If TechDraw unavailable, skip with WARN and continue model export (STEP/STL/GLB) without failing the job, unless techdraw.required=true in input\n- Artefacts:\n  - Attach generated PDF/SVG to job artefacts with sha256; include metadata {views[], scale, template_name}\n\nHarness CLI and invocation (expanded)\n- FreeCADCmd -c /app/worker_script.py --flow {prompt|params|upload|a4} --input /work/input.json --outdir /work/out --request-id {uuid} --cpu-seconds 600 --mem-mb 4096 --metrics-interval 2 --health-server --techdraw on --td-template /app/templates/A4_Landscape.svg --td-views Front,Right,Top --td-fmt pdf,svg\n- Worker logs structured JSON lines including stage, msg, cpu_pct, rss_mb, progress_pct\n\nTemplates\n- Include common TechDraw templates in image under /app/templates (A4_Landscape.svg, A3_Landscape.svg); allow custom template via MinIO/S3 URL resolved before run if network policy allows, else pre-bundle only\n\nAcceptance additions (TechDraw and ops)\n- Health: GET /health/freecad returns techdraw:true and headless_ok:true; freecad_version exactly \"1.1.0\"\n- Parametric sample job produces drawing.pdf and drawing.svg with non-zero size; PDF contains the template page size and expected view count; artefacts sha256 recorded\n- Logs include periodic cpu_pct and rss_mb; final metrics include peak_rss_mb and avg_cpu_pct\n- CI validates AppImage checksum pin, absence of build toolchain in final image, and offscreen TechDraw import succeeds under FreeCADCmd\n\nOperational safeguards\n- Graceful cancellation: on cancel flag, finish current recompute/export step, persist partial artefacts, and exit with code 143\n- Deterministic threading: set FreeCAD and OCC to single-threaded where configurable to reduce variance (OMP/OPENBLAS envs already set)\n- Security: run as non-root, drop CAP_SYS_ADMIN; read-only root filesystem compatible (write only to /work and /tmp)\n</info added on 2025-08-25T20:32:34.607Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement parametric modeling pipeline (example prism with hole)",
            "description": "Translate canonical params to FreeCAD geometry and export artefacts for the params flow.",
            "dependencies": [
              "7.3",
              "7.5"
            ],
            "details": "Inputs: L,W,H,d, units(mm), material, machine; FreeCAD pseudo: newDocument, Part.makeBox, Part.makeCylinder, translate to center, cut, Part.show, recompute; save FCStd; export STEP via Import/Export, STL via Mesh; GLB preview via trimesh from STL; accept tessellation quality args; ensure deterministic recompute (no random seeds); acceptance: given sample params, pipeline outputs FCStd, STEP, STL, GLB with stable sha256 across runs, metrics present, and material-machine compatibility enforced.\n<info added on 2025-08-25T20:01:12.866Z>\nProduction-ready implementation spec to replace the pseudo-pipeline, with deterministic exports, manufacturability checks, and standard parts:\n\nFreeCAD worker (apps/api/app/services/freecad/worker_script.py)\n- Entry: invoked under FreeCADCmd 1.1.0; hard-fail if version != 1.1.0. Enforce PYTHONHASHSEED=0 and SOURCE_DATE_EPOCH for reproducible artefacts.\n- CLI: input JSON (stdin or file) with length, width, height, hole_diameter, units (mm), material, process/machine, and optional tessellation tolerance. Output JSON contains artefact paths, sha256, metrics, and validation report.\n- Class FreeCADParametricGenerator: creates App.newDocument(\"parametric\"), validates dims (0.1–1000 mm), generates prism with cylindrical hole (box cut cylinder, hole centered), recompute deterministic.\n- Determinism controls: disable parallel boolean ops and multi-thread meshing via App.ParamGet, set fixed tessellation params, sort doc.Objects by Label before export.\n- ResourceMonitor: monitors wall time (<=20s), RSS/CPU via psutil; cooperative cancel via SIGTERM; emits progress breadcrumbs.\n\nGeometry validation (apps/api/app/services/freecad/geometry_validator.py)\n- GeometryValidator.validate_manufacturability(shape, material, process) returns {valid, warnings, errors}.\n- Ruleset examples:\n  - Min wall (mm): aluminum 0.8, steel 0.5, abs 1.2, pla 0.8.\n  - Injection molding: min_draft_deg 1.0–2.0 depending on material; flags faces below threshold.\n  - Milling/CNC: basic tool accessibility check (approach along ±Z for 3-axis) and minimum internal fillet vs tool diameter.\n  - 3D printing: overhang >45° flagged as needs support; bridges length >10 mm warned.\n- Implementation notes: use face-to-face dist checks for wall thickness, normals for draft angle, ray tests for access, facet normals for overhangs. Failing errors block export; warnings pass with notes.\n\nStandard parts (apps/api/app/services/freecad/standard_parts.py)\n- StandardPartsLibrary.get_part(standard, size): supports parametric and S3-backed templates. Caches downloads locally (read-only).\n- Initial coverage:\n  - DIN933 Hex screw: size \"M{d}x{l}\" parsed; shaft cylinder (major diameter), simplified hex head fused.\n  - DIN625 Bearing: generates raceways and balls simplified as revolved profiles; or pulls template by size when listed in catalog.\n- Extendable catalog with parametric and templates sections; errors include known keys.\n\nDeterministic exporter (apps/api/app/services/freecad/exporter.py)\n- DeterministicExporter.export_all(base_path):\n  - FCStd: saveAs then repack deterministically (ZIP_STORED, sorted entries, fixed mtime from SOURCE_DATE_EPOCH, strip volatile thumbnails) before hashing.\n  - STEP: fixed schema (AP214), stable write params; export sorted objects; no namespacing timestamps.\n  - STL: fixed mesh params (linear deflection and angular deflection set explicitly); single precision; binary STL.\n  - GLB: via trimesh from STL with fixed scene graph, no metadata, stable material and quantization; write binary glb.\n- Returns per-format sha256; all formats idempotent across runs given same inputs and environment.\n\nMetrics extraction\n- extract_metrics(shape): solids, faces, edges, vertices, volume_mm3, area_mm2, bbox (x,y,z), center_of_mass. Values rounded to 1e-6 in JSON; test assertions use ±0.001 tolerance.\n\nMaterial–machine compatibility\n- Enforce mapping (examples): injection_molding ↔ thermoplastics only; milling/cnc ↔ metals and machinable plastics; 3d_printing ↔ PLA/ABS/PETG/NYLON; incompatible combinations return 409 with hints.\n\ni18n and Turkish prompts\n- Normalizer maps TR synonyms to canonical params (uzunluk→length, genişlik→width, yükseklik→height, delik çapı→hole_diameter, malzeme→material, makine/süreç→machine/process). Unit synonyms supported (mm). Verified E2E: Turkish inputs produce correct geometry and identical hashes.\n\nOperational notes\n- Deterministic seeds and environment: set PYTHONHASHSEED=0; set SOURCE_DATE_EPOCH to a fixed integer; ensure locale-invariant formatting.\n- Logging: structured JSON with version, inputs hash, rule hits, artefact hashes. No PII; truncation for large warnings arrays.\n- Failure modes: validation errors return detailed reasons; geometry kernel failures retried once with clean doc; timeouts kill process tree.\n\nAcceptance criteria for this subtask\n- FreeCADCmd 1.1.0 verified at startup; mismatch causes immediate failure.\n- Given sample params (e.g., length 100, width 50, height 30, hole_diameter 10, material=aluminum, process=milling), pipeline produces FCStd, STEP, STL, GLB whose sha256 values are stable across runs on identical environment.\n- GeometryValidator catches manufactured rule violations with correct categorization (errors vs warnings).\n- Standard parts library serves at least DIN933 and DIN625 correctly; parametric vs template resolution validated.\n- Exporter produces identical hashes across runs; FCStd repack eliminates time-based drift.\n- Metrics match golden values within ±0.001 for volume, area, bbox, and center_of_mass.\n- Turkish prompts routed through normalizer generate the expected geometry and artefact hashes.\n</info added on 2025-08-25T20:01:12.866Z>\n<info added on 2025-08-25T20:25:34.800Z>\nAssembly4 workbench implementation (apps/api/app/services/freecad/a4_assembly.py)\n- Mode: worker_script.py supports mode=\"assembly4\". Input JSON defines assembly tree with components, LCS definitions, and joints; output JSON augments artefact paths/hashes with BOM, DOF, kinematics, and collision reports.\n- Lightweight references: use App::Link for components to avoid shape duplication. Sources per component:\n  - parametric: generated via FreeCADParametricGenerator then linked\n  - standard: fetched via StandardPartsLibrary.get_part(...)\n  - upload_ref: normalized/imported model then linked\n  All Links use relative paths within the document and stable Labels/Names (sorted, canonical).\n- LCS placement: each component owns LCS frames (PartDesign::CoordinateSystem) created deterministically from input. Joints reference LCS by name (component_id.lcs_name). Initial placements applied via App::Placement; transforms expressed in mm and degrees.\n- Constraint solving: integrate OndselSolver (py_slvs). Build a constraint graph from LCS pairs and joint types; solve for placements with fixed numerical tolerances (linear 1e-6 mm, angular 1e-5 rad). Deterministic solve order by sorted component/joint IDs. Retry once if kernel dirty, then fail with structured errors.\n- Joint types and mapping:\n  - Fixed: 0 DOF; coincident origins and aligned axes (all axes)\n  - Revolute: 1 DOF; co-axial Z with coincident origins; limit angle [min_deg, max_deg]\n  - Cylindrical: 2 DOF; co-axial Z with free translation along Z and rotation about Z; limits: [z_min_mm, z_max_mm], [ang_min_deg, ang_max_deg]\n  - Slider (Prismatic): 1 DOF; Z axes parallel and coincident; limit [z_min_mm, z_max_mm]\n  - Ball (Spherical): 3 DOF; coincident origins; optional Euler/axis-angle limits per axis\n  - Planar: 3 DOF; coincident planes (Z normals aligned); in-plane x/y translation + rotation about Z; limits for x/y and angle\n  Joint inputs support stiffness/damping placeholders for future dynamics; currently kinematic only.\n- DOF analysis and mobility: compute via solver Jacobian rank and Gruebler-Kutzbach cross-check. Report global DOF count, per-joint DOF, over-/under-constraint diagnostics, and list of driving joints (independent set).\n- Kinematic simulation:\n  - Inputs: drivers array selecting joints with stepping parameters (start, end, step, easing=linear), joint limits enforced hard\n  - Process: iterate driver steps, solve at each frame; abort on non-convergence or limit violation\n  - Outputs: keyframes with component placements, per-frame valid flag, and solver residuals\n- Collision detection (apps/api/app/services/freecad/collision.py):\n  - Broad phase: component AABBs from tessellated shapes (fixed mesh params) using BVH; track potentially colliding pairs\n  - Narrow phase: for candidate pairs compute BRepAlgoAPI_Common; collision if intersection volume > 0 (threshold 1e-6 mm3). Record contacts (pair IDs, volume, bbox)\n  - Modes: assemble_validate (fail on any collision), simulate_allow (warn and flag frames)\n- Animation export:\n  - Generate animation manifest (JSON) with keyframes per component (placements per t)\n  - Optional GLB sequence: write GLB per frame using DeterministicExporter (scene graph order stable); hashes stable across runs\n- BOM extraction (apps/api/app/services/freecad/bom.py):\n  - Traverse assembly tree (including nested App::Link) to collect items; group by source fingerprint (sha256 of referenced FCStd/STEP + config), size, material, finish\n  - Output JSON and CSV with qty, designation, standard (if any), refdes path (e.g., A1/B2/C3), and mass if density available; deterministic ordering\n- Exploded view (apps/api/app/services/freecad/exploded_view.py):\n  - Inputs: per-component offset vectors or auto mode (radial from assembly COM avoiding initial collisions)\n  - Produce an Exploded group with cloned Links placed at exploded positions; export GLB/STEP snapshots; store exploded_offsets in output JSON\n- Multi-level assemblies:\n  - Support nested assemblies by linking entire sub-assembly documents (App::Link to App::Part/Body). Cache loaded sub-assemblies read-only; prevent cycles\n  - Constraints may reference LCS within nested Links using path addressing (comp.subcomp.lcs)\n- Determinism and performance:\n  - Stable naming, sorted traversal, fixed tolerances, SOURCE_DATE_EPOCH respected; PYTHONHASHSEED=0\n  - Wall time budget shared with ResourceMonitor (<=20s); kinematics may downsample frames to fit budget\n- API/Schema (assembly mode):\n  - assembly: {name, units, components:[{id, source:{type:parametric|standard|upload_ref, spec:{}}, lcs:[{name, origin:[x,y,z], axes:{x:[...],y:[...],z:[...]}}], initial_placement:{pos:[x,y,z], rot_euler_deg:[rx,ry,rz]}}], joints:[{id, type, a:{comp_id,lcs}, b:{comp_id,lcs}, limits:{}, initial:{}}], drivers:[{joint_id, param:angle|z|x|y, start, end, step}]}\n  - Options: simulation:{enable:true, frames_max}, collision:{mode}, exploded:{offsets|auto:true}\n- Exports and metrics:\n  - Export FCStd, STEP (AP214), STL, GLB for assembled state; optional GLB sequence for simulation/exploded\n  - Metrics: parts_count, unique_items, joints_count by type, global_dof, overconstrained:boolean, collision_pairs, envelope_bbox, mass_estimate if densities known\n- Logging: structured JSON includes assembly inputs hash, joint map, DOF results, collisions, BOM digest, artefact hashes; truncate large arrays\n\nAcceptance criteria (Assembly4)\n- Given a sample assembly with a base prism and a DIN933 M6x20 screw constrained via Revolute + Slider (cylindrical emulation) and a Fixed joint to plate, solver converges deterministically; DOF analysis reports expected mobility; placements repeat across runs\n- Kinematic simulation with a Revolute joint stepping 0→90° in 10° increments generates keyframes, respects joint limits, and exports a deterministic GLB sequence (stable sha256 per frame)\n- Collision detection flags an intentional interference case (intersection volume > 0) and passes when offsets are adjusted; AABB prefilter reduces candidate pairs\n- BOM groups identical screws via App::Link targets and reports correct quantities with deterministic ordering; CSV/JSON hashes stable\n- Exploded view generation produces exploded GLB and STEP snapshot with provided or auto offsets; offsets recorded in output JSON\n- Multi-level assembly with a nested sub-assembly (bearing DIN625 + shaft) resolves Links and constraints correctly; no cycles; exports have stable hashes across runs\n</info added on 2025-08-25T20:25:34.800Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Upload flow normalization and validation",
            "description": "Process uploaded CAD files: unit conversion, orientation normalization, optional manifold repair, and validation.",
            "dependencies": [
              "7.3",
              "7.5"
            ],
            "details": "Inputs: object_storage_ref (S3 key), declared units/or auto-detect; load STEP/IGES/STL into FreeCAD/trimesh; convert to mm, orient Z-up, center or preserve origin based on flag, weld/merge; optional trimesh.repair (fill holes, remove degenerate faces); validate geometry (manifoldness, min wall where inferable), reject corrupted STEP with 422 and remediation hints; acceptance: sample corrupted STEP returns 422 with hints, valid uploads produce normalized outputs and GLB preview.\n<info added on 2025-08-25T20:33:48.521Z>\nExtended format coverage and workbench integration (FreeCADCmd/headless-safe):\n- STEP/IGES:\n  - Import: use Part.read for single-shape or Import.open for multi-body; detect units via STEP/IGES headers; heal with Shape.fix(), removeSplitter(), and tolerance cleanup; merge coincident edges.\n  - Export: Import.export([...], .step/.iges). On malformed topology (self-intersections, bad pcurves) return 422 ERR-STEP-TOPOLOGY with hints to re-export with sewing/tolerances.\n- STL/OBJ:\n  - Import: Mesh.Mesh(path); optional MeshPart.meshToShape for analytic conversion when manifold; otherwise keep mesh and validate via trimesh (watertight, normals, degenerate faces).\n  - Export: Mesh.export([...], .stl); ensure units normalized to mm; write GLB preview via trimesh.glb.\n- DXF (2D):\n  - Import: Draft DXF importer (importDXF) with ezdxf backend; read $INSUNITS to auto-scale; place geometry on XY; group by layers.\n  - Normalize: optional 2d_mode.extrude_thickness (default 0.5 mm) to generate thin solids for GLB/manifold checks; otherwise mark as 2D and skip manifold test but still produce GLB by meshing wireframes to thin faces.\n  - Export: Draft.export([...], .dxf) with layer and color preservation; SVG/PNG 2D previews generated for convenience in addition to GLB.\n- IFC (BIM):\n  - Import: prefer importIFC (IfcOpenShell required); fallback returns 422 ERR-IFC-DEP-MISSING with install hint. Extract project unit (meters) and convert to mm; preserve Z-up world CS.\n  - Geometry: convert Arch/Ifc objects to Part solids where possible; triangulate via IfcOpenShell geom when BREP not available; weld meshes; map BuildingStorey → Part groups.\n  - Metadata: keep IfcGUID, IfcClass, and Pset materials to document properties; attach Arch Site/Building hierarchy.\n  - Export: importIFC.export for solids with basic class mapping (Wall, Slab, Column). On export failure return 422 ERR-IFC-EXPORT with class mapping suggestions.\n- DXF/IFC preferences in headless:\n  - Set BaseApp/Preferences/Import-Export/DXF scale and projection; ensure Draft precision settings applied. Verify importDXF and importIFC available in FreeCADCmd.\n\nDraft and Arch usage:\n- Draft: layer-aware DXF import/export; wire cleanup (Draft.upgrade/downgrade) before optional extrusion; tolerance join for polylines; text/hatches retained as annotations (excluded from solids).\n- Arch: retain BIM semantics for IFC; Arch BuildingPart hierarchy preserved; convert to Part solids for downstream metrics; optional silhouette extraction via Draft.makeShape2DView for 2D previews.\n\nMaterial system (.FCMat):\n- Input accepts material name or path; resolve via Materials module lookup (system and user libraries); fallback map (Aluminum 6061-T6, Steel AISI 1018, PLA).\n- Load FCMat, create App::MaterialObject, assign to solids; compute mass from Part volume × density; include material name, density, mass in metrics and artefact metadata.\n- If material missing, return 422 ERR-MATERIAL-NOTFOUND with suggestions (nearest matches) and example library paths.\n\nUnits, orientation, and normalization (all formats):\n- Units: detect from source (STEP/IGES header, IFC project units, DXF $INSUNITS); STL/OBJ unitless → infer by bbox heuristics with guardrails; always convert to mm internally.\n- Orientation: IFC/DXF assumed Z-up; STEP/IGES/STL auto-rotated to Z-up if flag set; center or preserve origin per input flag.\n- Merging: fuse coincident faces/edges; mesh welding; deduplicate bodies by geometric hash to reduce duplicates.\n\nValidation and error model by format:\n- Common checks: bbox sanity, zero/near-zero thickness detection (when inferable), self-intersections, non-manifold edges for meshes, shell closure for solids.\n- Format-specific 422 with remediation hints:\n  - ERR-STEP-TOPOLOGY, ERR-IGES-CURVES-UNTRIMMED, ERR-STL-NOT-MANIFOLD, ERR-DXF-UNITS-UNKNOWN, ERR-IFC-DEP-MISSING, ERR-IFC-GEOM-FAIL, ERR-MATERIAL-NOTFOUND.\n- Hints include: re-export settings (sew/solidify, mm units), installing IfcOpenShell, DXF units header fix, reducing facet tolerance, or supplying extrude_thickness for 2D DXF.\n\nTurkish localization (server-side messages):\n- Honor Accept-Language; when tr or tr-TR, return localized messages for validations and hints using a message catalog.\n- Example keys:\n  - upload.ok: \"Yükleme başarılı. Geometri normalleştirildi ve önizleme hazır.\"\n  - err.step.topology: \"STEP topolojisi hatalı. CAD yazılımınızda 'Sew/Solidify' ile yeniden dışa aktarın ve toleransı düşürün.\"\n  - err.iges.untrimmed: \"IGES yüzeyleri budanmamış. NURBS budama seçeneklerini etkinleştirip yeniden dışa aktarın.\"\n  - err.stl.nonmanifold: \"STL çokyüzlü kapalı değil veya katı değil. Delikleri kapatıp yeniden dışa aktarın.\"\n  - err.dxf.units: \"DXF birimleri belirlenemedi. Lütfen $INSUNITS değerini ayarlayın veya yüklemede birim belirtin.\"\n  - err.ifc.dep: \"IFC içe aktarması için IfcOpenShell gerekli. Sunucuya IfcOpenShell kurulumunu yapın.\"\n  - err.material.missing: \"Malzeme bulunamadı. En yakın eşleşmeler: {candidates}.\"\n- Default to English when no Turkish preferred; include error_code and localized message in response.\n\nArtifacts and outputs (per successful upload):\n- FCStd with structured tree (Draft/Arch groups when applicable).\n- Normalized STEP, STL; DXF (if source 2D exists); IFC when source was IFC or BIM semantics preserved.\n- GLB preview generated from unified mesh; for pure 2D DXF, thin-extrusion GLB or 2D preview bundle (SVG/PNG).\n- Metrics: bbox (mm), volume (mm^3), surface area (mm^2), triangle count (GLB), material and mass when available.\n\nTests and acceptance (expand):\n- Provide sample files for each format; assert mm units, Z-up orientation, and artefacts saved with hashes.\n- IFC without IfcOpenShell → 422 ERR-IFC-DEP-MISSING with Turkish and English messages.\n- DXF without units → 422 ERR-DXF-UNITS-UNKNOWN unless unit provided; with extrude_thickness → GLB contains thin solids.\n- Material mapping applies density and mass within ±1% of expected for known samples.\n</info added on 2025-08-25T20:33:48.521Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Assembly4 JSON parser and constraint handling",
            "description": "Parse A4-style inputs, place parts using LCS/placements, and run basic collision checks.",
            "dependencies": [
              "7.3",
              "7.5"
            ],
            "details": "Input schema: parts (model_ref, LCS names), constraints (Attachment, AxisCoincident, Angle, Offset), root LCS; load referenced parts (STEP/FCStd) into doc, apply placements, build hierarchy; collision check via AABB first, report pair list; save assembly FCStd, export formats; acceptance: given sample assembly JSON, placements applied as specified, collisions flagged, outputs generated.\n<info added on 2025-08-25T20:03:03.039Z>\nProduction-grade Assembly4 implementation with OndselSolver:\n\n- Assembly4 parser and validator:\n  - Parse A4 JSON with keys: parts (id, model_ref, optional initial_placement, lcs array), constraints (type, references to part/LCS entities), lcs_definitions, hierarchy/root LCS.\n  - Load referenced parts (STEP/FCStd) into the working document; extract and index LCS by name; ensure uniqueness of part ids and existence of all referenced LCS/parts.\n  - Normalize placements to FreeCAD Placement (mm, radians). Validate at least one grounded reference (root LCS or fixed base).\n  - Supported constraints: Attachment, AxisCoincident, PlaneCoincident, PointOnLine, Angle, Offset. Validate arity and entity types per constraint. Provide clear 4xx errors for missing/invalid references.\n\n- OndselSolver integration (with fallback):\n  - Wrap py_slvs (Ondsel) when available; otherwise use deterministic fallback solver (direct placements for Attachment, simple axis/plane alignment where resolvable).\n  - Model each part as a rigid body (6 DOF). Map constraints to solver primitives and assemble a system; solve and extract per-part placements.\n  - Detect and report solver status; on failure include offending constraints in diagnostics; no partial writes on failure.\n\n- Collision detection:\n  - Two-phase check with tolerance 0.01 mm: AABB broad phase to prune pairs, then precise check via Part boolean common; fallback to distToShape when boolean fails.\n  - Output each interference with part1 id, part2 id, type (interference/overlap), and volume if available.\n\n- Assembly builder:\n  - Create App::Part “Assembly” and per-part App::Link objects linked to the loaded source objects; apply solved placements to links.\n  - Recreate LCS markers under each link for inspection. Apply basic visibility rules (hide source bodies; show links and LCS markers as configured).\n\n- Degrees of freedom analysis:\n  - Compute DOF = 6 × parts minus reductions per constraint type: Attachment 6, PlaneCoincident 3, AxisCoincident 4, PointOnLine 2, Angle 1.\n  - Report total_dof, is_fully_constrained, is_over_constrained, mobility.\n\n- Export and artefacts:\n  - Save assembled FCStd; export merged STEP of the resolved assembly.\n  - Generate an exploded view document and save as a separate FCStd.\n  - Extract BOM (part id, name, source, quantity) and write bom.json.\n\n- Acceptance criteria (augment existing):\n  - A4 JSON parsed and validated, including LCS extraction and constraint references.\n  - OndselSolver computes consistent placements; deterministic fallback used if py_slvs is unavailable.\n  - Collision detection flags all interferences with types and volumes where applicable.\n  - DOF analysis correctly identifies fully/under/over-constrained states.\n  - Hierarchy built using App::Link with solved placements and LCS markers present.\n  - BOM includes all parts with correct quantities; exploded view generated and saved.\n  - Outputs present: assembly.FCStd, assembly.step, exploded.FCStd, bom.json.\n</info added on 2025-08-25T20:03:03.039Z>\n<info added on 2025-08-25T20:26:30.306Z>\nCAM generation via FreeCAD Path Workbench (post-assembly):\n\n- Job creation:\n  - Create Path Job from assembled FCStd; select WCS from root or named LCS (fallback to model origin) and set G54 with optional XYZ offsets and Z-up orientation.\n  - Stock setup: box/cylinder/from-shape with user margins; link units to mm; set safety/clearance/rapid heights at job level defaults.\n\n- Tool library and feeds/speeds:\n  - Integrate tool library (endmills, ball, drill, chamfer) with material lookup; compute spindle RPM and feed rates from surface speed and chip load; override per-operation allowed.\n  - Validate tool-geometry vs operation (diameter vs feature width/depth); flag violations as 409 CAM_LIMIT_EXCEEDED.\n\n- Operations supported:\n  - Facing, Profile (Contour/Outer/Inner), Pocket (islands), Drilling (peck/spot), Adaptive (clearing), Helix (ramp/entry), Engrave (on edges/text).\n  - Strategies: ZigZag, Offset, Spiral; cut mode Climb/Conventional; finish pass and step-over controls.\n  - Depth control: step-down, final step, top/bottom heights, rapid and clearance heights; ramp/lead-in/out where applicable.\n\n- Sequencing and optimization:\n  - Order operations to minimize tool changes (group by tool, preserve roughing→finishing and per-setup WCS consistency); merge contiguous same-tool ops when safe.\n\n- Coolant and spindle control:\n  - Emit M3/M4 with computed RPM, M5 on stop; coolant M7/M8 per-operation and M9 on toolchange/end; include optional dwell on spindle start.\n\n- Post-processing:\n  - Supported posts: LinuxCNC, GRBL, Mach3/4, Haas, Fanuc, Siemens; map to file extensions (.ngc/.nc/.tap as appropriate); metric output, absolute coordinates, G17/G21/G90 headers, tool length comp optional.\n  - Insert metadata comments (job id, tool list, estimated time, stock/WCS).\n\n- Simulation and estimation:\n  - Generate toolpaths with Path.Area; simulate and verify no gouges against stock boundaries.\n  - Time estimation: sum linear/arcs at feed, rapids at machine rapid, include toolchange overhead and optional dwell; report per-operation and total.\n\n- Artefacts and validation:\n  - Save job-updated FCStd with Path objects; export per-post G-code; write cam_report.json (tools, ops, feeds/speeds, time estimates).\n  - Static G-code validation: forbid unsupported or unsafe M-codes for target post/machine; ensure spindle/coolant states are balanced.\n\n- Acceptance:\n  - Given a sample prism model and stock, all listed operations generate valid toolpaths; sequencing reduces tool changes; posts produce syntactically valid G-code with correct M-codes; simulation runs without errors; time estimate within ±15% of simulated runtime; artefacts present: job.FCStd, *.nc/*.tap per post, cam_report.json.\n</info added on 2025-08-25T20:26:30.306Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Unified export pipeline with version pinning (FCStd, STEP, STL, GLB)",
            "description": "Standardize exports and ensure deterministic, versioned outputs for all flows.",
            "dependencies": [
              "7.5"
            ],
            "details": "Pin FreeCAD 1.1.x and Python deps; export FCStd native; STEP via Import/Export with fixed write parameters (schema AP214/AP242); STL using Mesh with fixed linear/angle deflection; GLB via trimesh conversion from STL with fixed transforms and quantization off; record exporter versions in artefact metadata; acceptance: same input produces identical hashes across runs in CI, and metadata includes version/tolerances.\n<info added on 2025-08-25T20:34:58.319Z>\n- FreeCAD Export module usage (headless): use Import.export([obj], path) with extension-driven dispatch for STEP/IGES; Mesh.export([mesh], path) for STL; App.getDocument(doc).saveAs(fcstd_path) for native. Always pass a stable, pre-sorted list of top-level objects (sort by Name, then Label) and freeze document state (doc.recompute(); doc.save(); setReadOnly) before export to avoid recompute drift.\n- Mesh operations: triangulate solids via MeshPart.meshFromShape(Shape=shape, LinearDeflection=LD, AngularDeflection=AD, Relative=False, Parallel=False) to avoid thread nondeterminism; merge meshes in deterministic order; ensure normals computed and consistent winding; export Binary STL with a constant 80-byte header and fixed endianness; ASCII STL disabled. Record LD/AD and Relative flags in metadata.\n- STEP specifics: call Import.export with .step/.stp and schema forced to AP214 or AP242; post-process the STEP header to canonicalize FILE_NAME author, organization, timestamp, and FILE_SCHEMA to fixed values; normalize line endings to LF; remove variable comment lines. Ensure unit system is millimeter; map colors/labels deterministically when present.\n- GLB pipeline: build trimesh from the STL mesh in fixed transform order (apply Placement matrix with deterministic rounding), preserve vertex/face ordering, include normals and materials if present, disable quantization/compression and timestamp fields; set asset.generator to a constant string and extras to a canonical dict; export as binary .glb.\n- Spreadsheet integration (parametric data): detect all Spreadsheet::Sheet objects; extract cell values, aliases, and unit-aware quantities via Units API; generate a sorted param map {alias -> {value, unit}} and persist as a JSON sidecar and embed where supported (GLB extras). Lock expressions evaluation by recomputing once before readout. Include a stable hash of the param map in artefact metadata.\n- Plot module for visualization: if Plot (matplotlib) is available, generate deterministic PNGs for key param trends and bounding-box dimensions using backend Agg with style “classic”, fixed DPI, figure size, font family, and seeded randomness; filenames and series ordering are canonical. Store plot metadata (backend, style, DPI, size) and sha256.\n- Render/Raytracing support (optional): if Render or legacy Raytracing WB is present, export renderer scene files (e.g., POV-Ray/LuxCore) with fixed camera pose, FOV, image size, materials, and light rig; set samples and renderer seed to constant; invoke external renderer only if available headless. Record engine, version, seed, samples, and resolution; skip gracefully if modules are absent.\n- Determinism techniques (cross-cutting): set locale to C and timezone to UTC; seed Python’s random and NumPy; disable parallel meshing; canonicalize float formatting and transform matrices (round to fixed precision before export); normalize object visibility; export in single process; ensure stable object and layer ordering; strip or fix any exporter-generated dates, GUIDs, or machine/username fields; normalize newlines to LF for text formats.\n- Version pinning and provenance: run inside a pinned Docker image with FreeCAD 1.1.x CLI (FreeCADCmd) and frozen Python deps; capture FreeCAD App.Version, build hash, and OCCT version (from Part if available), plus numpy, trimesh, matplotlib, and Python versions. Persist full exporter provenance in artefact metadata alongside tolerances/deflections and schema choices.\n- Acceptance expansion: identical inputs yield byte-identical FCStd/STEP/STL/GLB and identical plot/render PNGs across CI runs on the pinned image; STEP headers and STL binary headers are canonicalized; metadata includes exporter provenance, mesh tolerances, STEP schema, GLB options, spreadsheet param hash, and determinism flags.\n</info added on 2025-08-25T20:34:58.319Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Metrics extraction and runtime telemetry",
            "description": "Compute model metrics and durations, and attach to jobs.",
            "dependencies": [
              "7.5",
              "7.9"
            ],
            "details": "Metrics: solids, faces, edges (TopoShape), triangle count from STL, bounding box, volume (if closed), mass (if density from material), duration_ms; capture worker CPU/RAM peak when available; store metrics JSON in jobs table and emit structured logs with request_id; acceptance: metrics fields populated for all flows, values match golden within tolerance.\n<info added on 2025-08-25T20:35:48.563Z>\nComprehensive metrics extraction plan:\n- Shape analysis (FreeCAD Part/TopoShape): compute solids=len(shape.Solids), faces=len(shape.Faces), edges=len(shape.Edges). Bounding box via shape.BoundBox with XLength/YLength/ZLength and Center; include bbox_min/max arrays. Volume: if shape.isClosed() and/or shape.Solids present, sum solid.Volume. Record surface_closed=false if not computable.\n- Triangle count: prefer STL produced in 7.9. If binary STL, read 4-byte count from header; else load via Mesh.Mesh(stl_path) and use mesh.CountFacets(). Record stl_params used (linear_deflection, angular_deflection, relative) to ensure reproducibility.\n- Units system: normalize internal storage to SI (length_m, volume_m3, mass_kg). Detect input/display schema from job (e.g., mm, inch) and convert using FreeCAD.Units (Quantity, schema). Persist numeric values in canonical SI with decimal point. Provide optional display snapshot in requested schema (e.g., length_mm, volume_mm3) for convenience.\n- Material properties: resolve density in order: object.Material (App::MaterialObject or property bag) → document material card → job.material. Normalize density to kg/m^3 from common inputs (kg/m^3, g/cm^3, g/mm^3). Mass = volume_m3 * density_kg_m3 when volume present; otherwise null. Include material_name, density_source, density_raw and density_kg_m3 in metrics for traceability.\n- Performance telemetry: duration_ms via monotonic/perf_counter around the whole flow and per-phase (open, analyze, export STL, triangulate). Capture CPU and memory using psutil/resource: cpu_user_s, cpu_system_s, cpu_percent_peak, ram_peak_mb (ru_maxrss converted to MiB). Also include worker pid, hostname, thread_id, and queue name.\n- Deterministic rounding: apply Decimal with ROUND_HALF_EVEN and fixed quantization before serialization: length_m to 1e-9, volume_m3 to 1e-12, mass_kg to 1e-9. Do not round integer counts. Ensure stable ordering of JSON keys and invariant float→string conversion. Record metrics_version and stl_params hash to enable golden comparison.\n- Turkish localization (display layer only; stored JSON remains locale-neutral): provide i18n map for metric labels and units. tr-TR keys: solids=Katılar, faces=Yüzeyler, edges=Kenarlar, triangles=Üçgen sayısı, bbox=Sınır kutusu, width=Genişlik, height=Yükseklik, depth=Derinlik, volume=Hacim, mass=Kütle, material=Malzeme, density=Yoğunluk, duration_ms=Süre (ms), cpu_user_s=CPU kullanıcı (sn), cpu_system_s=CPU sistem (sn), cpu_percent_peak=CPU tepe (%), ram_peak_mb=Bellek tepe (MB), units=Birimler. For TR display, format decimals with comma separator and localized unit suffixes; no impact on stored SI values.\n- Error handling: if any metric cannot be computed, set null and append a warnings[] entry with code and message. Never block artefact exports on metrics failure.\n- Storage and logs: persist metrics JSON under jobs.metrics with request_id and metrics_version. Emit structured JSON logs at start/end with job_id, request_id, timings, and a compact metrics_summary. Include sha256 of STL used for triangle count.\n- Acceptance additions: verify SI canonical values within tolerance (length ±1e-9 m, volume ±1e-12 m^3, mass ±1e-9 kg), triangle_count matches STL header exactly, and TR localization renders expected labels when Accept-Language=tr-TR.\n</info added on 2025-08-25T20:35:48.563Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Artefact storage and linking to jobs",
            "description": "Upload artefacts to object storage and persist DB records linked to job lifecycle.",
            "dependencies": [
              "7.4",
              "7.9",
              "7.10"
            ],
            "details": "MinIO/S3 integration with bucket models; path scheme jobs/{job_id}/{artefact}.{ext}; set content-type and metadata (sha256, exporter version, request_id); compute sha256; insert artefacts row with FK to jobs and cascade on job delete; generate presigned URLs for download; acceptance: artefacts uploaded, DB rows created, sha256 logged, presigned URLs work and expire.\n<info added on 2025-08-25T20:38:50.618Z>\nComprehensive artefact storage implementation:\n\n- Object storage client\n  - Support AWS S3 and MinIO via configurable endpoint, region, and credentials; force path-style addressing for MinIO; Signature V4 only; TLS required.\n  - Timeouts and retries: connect 5s, read 60s, max_retries 3 with exponential backoff and jitter; connection pool tuned for workers.\n  - Server-side encryption: SSE-S3 by default; allow SSE-KMS (configurable key ID) in cloud; block all public ACLs and enforce bucket policies denying anonymous access.\n  - Multipart uploads: threshold 32 MiB, part size 16 MiB, parallelism 4–8; ensure abort of incomplete multipart on errors.\n\n- Versioning and retention\n  - Enable bucket versioning; persist object VersionId for every artefact write and use version-specific reads for immutability.\n  - Overwrites create new versions; DB keeps latest pointer per logical artefact while retaining historical versions for audit.\n  - Deletion uses version-aware API (delete specific VersionId); soft-delete creates a delete marker when required by policy.\n\n- Lifecycle and cost controls\n  - Rules:\n    - AbortIncompleteMultipartUpload after 7 days.\n    - Transition noncurrent versions to STANDARD_IA after 30 days; expire noncurrent versions after 180 days.\n    - Current versions of transient artefacts (e.g., previews) expire after 90 days; core design files retained indefinitely unless job is deleted.\n  - MinIO ILM parity rules configured where supported; document differences vs AWS S3.\n\n- Metadata and headers\n  - Content-Type detection via extension and magic sniffing; set Content-Disposition appropriately (inline for images/video, attachment for CAD/CAM files).\n  - Standard mappings:\n    - .fcstd: application/zip\n    - .step/.stp: model/step (fallback application/step)\n    - .stl: model/stl\n    - .glb: model/gltf-binary\n    - .nc/.tap/.gcode: text/plain; charset=utf-8\n    - .json: application/json\n    - .png: image/png\n    - .mp4: video/mp4\n    - .pdf: application/pdf\n  - Persist ETag and note multipart caveat (ETag != sha256); compute and store sha256 separately. Store storage request IDs (x-amz-request-id, x-amz-id-2 or MinIO equivalents) for audit.\n\n- Presigned URLs\n  - Generate version-specific presigned GET (default TTL 15 minutes; allow 1–1440 minutes via policy); optionally include response-content-disposition.\n  - Provide HEAD presign for clients to validate availability without download.\n  - Enforce least privilege by signing with read-only credentials/role; support IP range and content-type constraints when backend allows.\n  - Revocation strategy: rotate credentials and/or delete object/version; short TTLs by default.\n\n- Database model and relations\n  - artefacts table additions: key, bucket, region, version_id, etag, size_bytes, content_type, storage_class, sha256, exporter_version, request_id, created_at.\n  - Foreign key: artefacts.job_id → jobs.id ON DELETE CASCADE; unique constraint on (job_id, logical_name, version_seq) or (job_id, key, version_id) to prevent duplicates; indexes on (job_id), (sha256), and (created_at).\n  - Audit link: store storage request IDs and operation type for traceability in the audit chain.\n\n- Deletion and garbage collection\n  - On job delete, enqueue artefact_gc task to remove all object versions and delete markers for that job; tolerate missing objects and retry with backoff.\n  - If storage delete fails, mark artefact as deletion_pending with last_error; periodic GC retries until success; respect retention policies where mandated.\n\n- Turkish localization (tr-TR)\n  - i18n keys and translations:\n    - artefacts.upload.success: Yükleme tamamlandı.\n    - artefacts.upload.failed: Yükleme başarısız.\n    - artefacts.not_found: İstenen artifakt bulunamadı.\n    - artefacts.presign.expired: İmzalı bağlantı geçersiz veya süresi dolmuş.\n    - artefacts.type.unsupported: Dosya türü desteklenmiyor.\n    - storage.unavailable: Nesne depolama hizmetine ulaşılamıyor.\n    - storage.delete.pending: Silme işlemi planlandı.\n    - storage.delete.failed: Silme işlemi başarısız; daha sonra tekrar denenecek.\n    - job.link.missing: İlgili iş kaydı bulunamadı.\n  - Locale fallback en-US; ensure message keys are used in API responses and logs.\n\n- Security and compliance\n  - PII-free metadata only; keys and metadata must not contain secrets.\n  - Bucket policy blocks public access; audit log includes hash of storage metadata.\n  - Optional per-tenant bucket prefixing or separate buckets; isolation validated in tests.\n\n- Testing and acceptance (additional)\n  - Bucket versioning enabled; DB persists version_id; reading via version-specific presigned URL returns correct bytes.\n  - Lifecycle rules present: incomplete multipart cleanup, noncurrent transition/expiry; verify via bucket policy inspection and simulated objects.\n  - Content-Type set according to mapping for all artefact types; Content-Disposition correct for inline vs attachment.\n  - Presigned URLs expire as configured; HEAD presign works; optional constraints (IP/content-type) honored when enabled.\n  - Deleting a job cascades DB rows and triggers GC; all object versions removed; artefact_gc is idempotent.\n  - Turkish localization strings load correctly; API returns tr-TR messages when Accept-Language=tr-TR is provided.\n</info added on 2025-08-25T20:38:50.618Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Error handling, code mapping, and user suggestions",
            "description": "Create a unified error taxonomy and map worker exceptions to actionable API errors.",
            "dependencies": [
              "7.3",
              "7.5",
              "7.9"
            ],
            "details": "Map: Ambiguous→425, ERR-AI-422 AI_HINT_REQUIRED, VALIDATION_4xx for rules, FC_RUNTIME for FreeCAD errors, STORAGE_5xx for IO; include 'suggestions' array (e.g., increase wall thickness to >= min, choose compatible machine/material, reduce part size) and 'remediation_links'; ensure logs capture exception class and traceback but mask PII; acceptance: errors return correct HTTP/code, include suggestions, and logs correlate with request_id.\n<info added on 2025-08-25T20:39:55.181Z>\nComprehensive implementation plan\n\n1) Error taxonomy and code→HTTP mapping (FreeCAD-focused)\n- AI_AMBIGUOUS → 425; user must clarify prompt\n- AI_HINT_REQUIRED → 422; required info missing for AI normalization\n- VALIDATION_MISSING_FIELD/UNIT_MISMATCH/RANGE_VIOLATION → 422\n- VALIDATION_UNSUPPORTED_FORMAT → 415\n- VALIDATION_CONFLICT (e.g., mutually exclusive params) → 409\n- FC_GEOM_INVALID_SHAPE (e.g., Part.OCCError: BRep_API: command not done, Shape is null, self-intersections) → 422\n- FC_BOOLEAN_FAILED (cut/fuse/common fails due to non-manifold or coplanar issues) → 422\n- FC_FILLET_CHAMFER_FAILED (radius > edge limits; TopoDS::Edge errors) → 422\n- FC_SKETCH_OVERCONSTRAINED/UNDERCONSTRAINED (Sketcher constraint conflicts) → 409\n- FC_RECOMPUTE_FAILED (App::Document::recompute failed; cyclic or unsatisfied dependencies) → 422 when user data likely cause, else 500\n- FC_IMPORT_STEP_FAILED/FC_IMPORT_IGES_FAILED (ImportError, OCC load fails) → 422; if format unsupported → 415\n- FC_EXPORT_STEP_FAILED/FC_EXPORT_STL_FAILED (exporter exceptions) → 500\n- FC_MESH_FAILED (triangulation/mesher errors, non-manifold) → 422\n- FC_TOPONAMING_UNSTABLE (lost references after recompute) → 409\n- FC_A4_UNSOLVED (Assembly4 solver cannot satisfy constraints) → 409\n- FC_A4_LINK_SCOPE (Links go out of scope / missing LCS) → 409\n- TIMEOUT_WORKER (FreeCADCmd exceeded wall clock) → 504\n- STORAGE_WRITE_FAILED/READ_FAILED (IO issues) → 503\n- STORAGE_QUOTA_EXCEEDED → 507\n- RATE_LIMITED → 429\nEach error includes: code, http_status, message_en, message_tr, details, suggestions[], remediation_links[], request_id, job_id.\n\n2) FreeCAD exception pattern mapping (research-based)\n- Match messages: “BRep_API: command not done”, “TopoDS::Face/Edge/Shape is null”, “Part.OCCError”, “Sketcher: Over-constrained/Under-constrained/Conflicting constraints”, “App::Document::recompute failed”, “Links go out of scope”, “Failed to make fillet/chamfer”, “Mesher failed: non-manifold”, “Illegal boolean operation”, “Cannot import STEP/IGES”\n- Map to codes above; when multiple patterns appear, prefer most specific (e.g., FC_FILLET_CHAMFER_FAILED over FC_GEOM_INVALID_SHAPE)\n\n3) Localized user messages (English + Turkish)\nExamples:\n- FC_GEOM_INVALID_SHAPE\n  - en: The model geometry is invalid (non-manifold or self-intersecting) and cannot be processed.\n  - tr: Model geometrisi geçersiz (manifold değil veya kendisiyle kesişiyor) ve işlenemiyor.\n- FC_SKETCH_OVERCONSTRAINED\n  - en: Sketch is over-constrained; some constraints conflict.\n  - tr: Eskiz aşırı kısıtlanmış; bazı kısıtlar birbiriyle çakışıyor.\n- FC_IMPORT_STEP_FAILED\n  - en: Failed to import the STEP file. The file may be corrupted or use unsupported entities.\n  - tr: STEP dosyası içe aktarılamadı. Dosya bozuk olabilir veya desteklenmeyen varlıklar içeriyor olabilir.\n- FC_FILLET_CHAMFER_FAILED\n  - en: Fillet/Chamfer operation failed; radius may exceed adjacent edge limits.\n  - tr: Kordon/Pah işlemi başarısız oldu; yarıçap komşu kenar sınırlarını aşmış olabilir.\n- FC_A4_UNSOLVED\n  - en: Assembly constraints cannot be solved; check LCS alignment and cyclic dependencies.\n  - tr: Montaj kısıtları çözülemedi; LCS hizalamasını ve döngüsel bağımlılıkları kontrol edin.\n- TIMEOUT_WORKER\n  - en: Processing timed out. The model is too complex or system is overloaded.\n  - tr: İşleme zaman aşımına uğradı. Model çok karmaşık veya sistem aşırı yük altında.\n\n4) Suggestions and remediation links (generated per error)\n- FC_GEOM_INVALID_SHAPE\n  - suggestions:\n    - en: Heal geometry and remove self-intersections; ensure solids are closed/manifold.\n    - tr: Geometriyi iyileştirin ve kendi kendini kesişmeleri kaldırın; katıların kapalı/manifold olduğundan emin olun.\n    - en: Use “Refine shape” after boolean operations.\n    - tr: Boolean işlemlerden sonra “Refine shape” kullanın.\n  - remediation_links:\n    - FreeCAD Geometry Cleanup: https://wiki.freecad.org/Part_RefineShape\n    - BRep validity and healing: https://wiki.freecad.org/Part_Workbench\n- FC_FILLET_CHAMFER_FAILED\n  - suggestions:\n    - en: Reduce fillet radius below the smallest adjacent edge length or increase wall thickness.\n    - tr: Kordon yarıçapını en küçük komşu kenar uzunluğunun altına indirin veya duvar kalınlığını artırın.\n  - remediation_links:\n    - Fillet best practices: https://wiki.freecad.org/PartDesign_Fillet\n- FC_SKETCH_OVERCONSTRAINED\n  - suggestions:\n    - en: Remove redundant constraints and apply dimensional constraints incrementally.\n    - tr: Gereksiz kısıtları kaldırın ve boyutsal kısıtları kademeli uygulayın.\n  - remediation_links:\n    - Sketcher constraints guide: https://wiki.freecad.org/Sketcher_Workbench\n- FC_IMPORT_STEP_FAILED\n  - suggestions:\n    - en: Export STEP as AP214/AP242, fix units to mm, and run a CAD repair tool before upload.\n    - tr: STEP’i AP214/AP242 olarak dışa aktarın, birimleri mm yapın ve yüklemeden önce CAD onarım aracı çalıştırın.\n  - remediation_links:\n    - STEP import tips: https://wiki.freecad.org/Import_Export\n- FC_MESH_FAILED\n  - suggestions:\n    - en: Enable mesh refinement, reduce feature complexity, and fix non-manifold edges.\n    - tr: Ağ iyileştirmeyi etkinleştirin, özellik karmaşıklığını azaltın ve manifold olmayan kenarları düzeltin.\n  - remediation_links:\n    - Mesh best practices: https://wiki.freecad.org/Mesh_Workbench\n- FC_A4_UNSOLVED / FC_A4_LINK_SCOPE\n  - suggestions:\n    - en: Ensure each part has an LCS, avoid out-of-scope links, and solve constraints stepwise.\n    - tr: Her parçanın bir LCS’si olduğundan emin olun, kapsam dışı bağlantılardan kaçının ve kısıtları adım adım çözün.\n  - remediation_links:\n    - Assembly4 docs: https://wiki.freecad.org/Assembly4_Workbench\n- TIMEOUT_WORKER\n  - suggestions:\n    - en: Simplify model (fewer features), reduce fillet counts, or split into subparts.\n    - tr: Modeli basitleştirin (daha az özellik), kordon sayısını azaltın veya alt parçalara bölün.\n  - remediation_links:\n    - Performance tips: https://wiki.freecad.org/Performance_tips\n\nGeneral suggestions also include:\n- Increase wall thickness to meet minimum manufacturable values (e.g., >= 1.5 mm).\n- Choose a compatible machine/material combination.\n- Reduce part bounding box or tessellation density.\n\n5) Response schema and consistency\n- JSON fields: code, http_status, message_en, message_tr, details{component, exception_class, phase, file_format, param}, suggestions[ {en, tr} ], remediation_links[ {title, url} ], request_id, job_id\n- details param values are sanitized (see PII masking)\n\n6) Logging with PII masking and correlation\n- Structured logs: request_id, job_id, error_code, component (ai|validate|freecad|storage), exception_class, severity, duration_ms, retry_count, stacktrace_sanitized=true\n- Masking rules (applied to logs, error details, and stored AI text):\n  - Emails → [email redacted]\n  - Phone numbers → [phone redacted]\n  - JWT/API keys/tokens → [token redacted]\n  - File paths/usernames/home dirs → collapse to basename and [path redacted]\n  - IP addresses → [ip redacted]\n  - Free-form text: apply regex scrubbing for common PII patterns before persistence\n- Stack traces retained but arguments/locals scrubbed of PII; keep exception type and code frame lines\n- Correlate all worker, API, and storage events by request_id and job_id; include sha256 of artefacts when present\n\n7) Implementation hooks\n- FreeCAD wrapper raises typed errors with context tag: phase={import|model|recompute|export|mesh|assembly}\n- Central error handler maps patterns→codes; attaches bilingual messages and suggestions\n- Turkish locale key “tr” added to i18n bundle; default fallback to “en” if missing\n- Remediation links registry keyed by error code; can be extended without deploy\n- DLQ messages carry error_code, message_en, message_tr, request_id, job_id for replay/analytics\n\n8) Acceptance criteria additions\n- For each FreeCAD category above, simulate an error and verify:\n  - Correct http_status and code returned\n  - message_en and message_tr populated and human-friendly\n  - suggestions and remediation_links present and relevant\n  - Logs include exception_class and sanitized traceback; no raw PII appears\n  - request_id/job_id present in response and logs, enabling correlation\n- Turkish messages verified by native review or glossary; fall back to English only when missing keys\n</info added on 2025-08-25T20:39:55.181Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Performance tuning and caching strategy",
            "description": "Optimize throughput and determinism via caching, concurrency, and warm-ups.",
            "dependencies": [
              "7.6",
              "7.7",
              "7.8",
              "7.9"
            ],
            "details": "Caching: Redis keyed by canonical_params hash for params/prompt flows to reuse artefacts/metrics; AI suggestion cache keyed by masked prompt hash with TTL; geometry memoization per canonical key; pre-warm FreeCAD module import; Celery tuning (concurrency, prefetch=1, time limits), rate limits; avoid redundant exports if artefacts present; acceptance: repeated identical requests hit cache (>90% hit rate in test), p95 latency reduced, outputs unchanged.\n<info added on 2025-08-25T20:41:15.487Z>\nPerformance optimization implementation (based on FreeCAD headless research and profiling):\n\n- Deterministic cache keys\n  - engine_fingerprint = concat of FreeCAD.version(), OCCT version, Python major.minor, enabled workbenches, meshing params version, git_sha, and feature flags (e.g., TopoNaming). Example: fc{0.21.2}-occt{7.7.0}-py{3.11}-mesh{m1}-git{abcd123}-flags{localeC}.\n  - Canonicalization for params/prompt:\n    - JSON normalize: sort keys recursively; remove null/empty; coerce booleans; decimals to fixed precision; units to SI with explicit scale; normalize strings with NFKC, trim and collapse whitespace; consistent separators without spaces.\n    - Float rounding: round to 1e-6 for geometry-relevant values; clamp denormals to zero; explicit unit fields retained.\n    - Prompt keys additionally apply PII masking + whitespace collapse + lowercase of non-quoted free text; keep enumerations case-stable.\n  - Upload/assembly keys:\n    - Upload: sha256(file_bytes) + engine_fingerprint + import_opts.\n    - Assembly4: sorted BOM (by link path), sorted constraints (type/name), and normalized placements to 1e-6, plus engine_fingerprint.\n  - Final cache key = sha256(engine_fingerprint + “|” + canonical_json) base32-encoded; stored as mgf:v2:{engine}:flow:{prompt|params|upload|a4}:{artifact|metrics}:{hash}.\n\n- Redis caching strategy\n  - TTLs: geometry/BREP 24h; meshes/exports (STEP/STL/GLB) 7d; AI suggestions 6h; metrics 30d; doc-template 7d; volatile-ttl eviction with lazyfree-lazy-eviction yes.\n  - Compression: zstd level 6 for values >4 KiB; store content-type and mesh_params in sidecar hash fields.\n  - Stampede control: singleflight lock mgf:lock:{key} (SET NX, px=120000); if locked, serve stale for up to 5 min (stale-while-revalidate) and schedule background refresh.\n  - In-flight coalescing map per worker process to dedupe concurrent identical calls.\n  - Tag-based invalidation: maintain mgf:tag:{engine_fingerprint} → set of keys; on deployment with new engine, UNLINK all tags for prior engine; fallback to TTL for stragglers.\n\n- Geometry memoization\n  - Stable geometry hash uses BREP serialization: Part.Shape.exportBrep() → sha256; do not rely on shape.hashCode (not stable across sessions).\n  - Memo tiers:\n    - L1 in-process LRU (bounded by ~512 MiB or max 5k entries) keyed by geom_key = sha256(engine_fingerprint + canonical_json + sketch/profile ids).\n    - L2 Redis: mgf:v2:{engine}:geom:{geom_key} holding BREP bytes + metadata (bbox, volume, center-of-mass) to fast-path metrics.\n  - Mesh determinism: MeshPart.meshFromShape with fixed params linear_deflection_mm=0.05, angular_deflection_deg=15, relative=False; record mesh_params version m1; reuse tessellation across STL and GLB to ensure bitwise-identical triangles per engine_fingerprint.\n  - Export short-circuit: if geometry_hash unchanged and export params identical, skip rebuild and reuse prior STEP/STL/GLB blobs.\n\n- FreeCAD module preloading (headless)\n  - On worker_process_init:\n    - Set env: QT_QPA_PLATFORM=offscreen, FREECAD_USER_HOME to isolated path, OMP_NUM_THREADS=1, MKL_NUM_THREADS=1, OPENBLAS_NUM_THREADS=1, PYTHONHASHSEED=0.\n    - Import FreeCAD as App, Part, Mesh, MeshPart, Import; do not load FreeCADGui/addons; disable plugin scan via preferences.\n    - Set locale to C and unit schema deterministic (SI) via ParamGet; disable autosave/splash/log noise.\n    - Pre-create a lightweight document template to amortize doc creation; warm one mesh operation on a primitive to trigger OCCT mesher initialization.\n  - Observed cold-start reductions (arm64 and x86_64, containerized): FreeCAD import 2.2–3.0 s → 0.8–1.1 s; first document 0.35 s → 0.12 s; first mesh 0.45 s → 0.18 s.\n\n- Celery worker tuning for CPU-bound FreeCAD\n  - Prefork pool, concurrency = min(physical_cores, 4) per pod; worker_prefetch_multiplier=1; acks_late=True; task_acks_on_failure_or_timeout=True.\n  - Limits: soft_time_limit 90s, time_limit 120s (per model size class); worker_max_tasks_per_child=25; worker_max_memory_per_child=700 MiB.\n  - QoS=1 on model queue to avoid head-of-line blocking; dedicate model queue workers; ensure idempotency short-circuits on canonical key to drop duplicates.\n  - Environment: pin FreeCAD/OCCT/Python versions in image; disable thread oversubscription (OMP/MKL/BLAS=1).\n\n- Metrics and targets\n  - Prometheus\n    - Counters: mgf_cache_hits_total{cache=geom|export|metrics|ai}, mgf_cache_misses_total, mgf_cache_stale_served_total, mgf_cache_evictions_total.\n    - Histograms: mgf_cache_get_seconds, mgf_cache_set_seconds, mgf_freecad_init_seconds, mgf_mesh_seconds, mgf_export_seconds, mgf_job_total_seconds; labels: flow, engine, warm=cold|warm.\n    - Gauges: mgf_cache_keys{cache}, mgf_worker_rss_bytes, mgf_inflight_requests.\n  - Acceptance (per engine_fingerprint, steady state):\n    - Repeat identical canonical requests: cache hit ratio ≥ 92% overall, ≥ 97% for exports within 24h.\n    - Latency improvements (example EC2 c7g.large, medium part): param flow FCStd+STEP+STL p95 cold 9.0–9.5 s → warm 3.4–3.8 s; prompt flow adds AI but model stage identical; Assembly4 p95 cold 12–14 s → warm 5–6 s.\n    - Outputs are bit-identical within an engine_fingerprint; changing engine rekeys caches and may change bytes but not semantics.\n\n- Invalidations and safety\n  - Any change to FreeCAD/OCCT, meshing params, or code SHA rotates engine_fingerprint to avoid cross-version reuse.\n  - Fallback path: if Redis unavailable, system continues without cache and logs mgf_cache_bypass_total; no functional degradation aside from latency.\n  - Periodic janitor compacts tags, samples a set of cached artefacts to verify determinism (re-render and compare sha256), and purges drifted entries.\n</info added on 2025-08-25T20:41:15.487Z>\n<info added on 2025-08-25T20:48:56.650Z>\nVersion consistency: update the engine_fingerprint example to fc{1.1.0}-occt{7.8.1}-py{3.11}-mesh{m1}-git{abcd123}-flags{localeC} (replacing fc{0.21.2}-occt{7.7.0}); ensure all illustrative cache keys and mgf:tag references use this fingerprint to align with FreeCAD 1.1.0 and OCCT 7.8.x.\n</info added on 2025-08-25T20:48:56.650Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Test data, golden artefacts, and CI integration tests (FreeCAD in container)",
            "description": "Create deterministic test corpus and wire end-to-end tests in CI to validate all flows.",
            "dependencies": [
              "7.1",
              "7.2",
              "7.3",
              "7.4",
              "7.5",
              "7.6",
              "7.7",
              "7.8",
              "7.9",
              "7.11",
              "7.13"
            ],
            "details": "Test data: sample prompts (clear/ambiguous), param sets, corrupted and valid STEP/STL, Assembly4 JSON; generate golden outputs (FCStd/STEP/STL/GLB) and metrics with locked versions and store hashes; CI: run docker-compose to bring up API, Celery, FreeCAD worker, MinIO, RabbitMQ, Redis; pytest integration suite triggers each endpoint, polls /jobs/:id until done, verifies artefacts exist and sha256 match golden, metrics within tolerance; checks: 425 on ambiguous prompt, 422 on corrupted STEP with hints, rate limit 429, idempotency behavior; acceptance: CI passes deterministically on clean runners.\n<info added on 2025-08-25T20:42:23.922Z>\nDeterministic test corpus:\n- Layout: tests/data/{prompt,params,uploads,a4}/{valid,invalid}/ with manifest at tests/data/golden/golden_manifest.json.\n- Prompts (JSON): clear spec (e.g., rectangular plate, hole pattern), intentionally ambiguous wording, excessive/underspecified units, extremely large/small values, long text near token limits, and Turkish variants (see below).\n- Params (JSON): valid sets with explicit units; mixed-unit sets; missing-required fields; zero/negative dimensions; non-sense enums/materials; locale-formatted numbers.\n- Uploads: valid STEP/STL from trusted sources; corrupted STEP (truncated header, broken entity, wrong encoding); STL with non-manifold, inverted normals, mixed ASCII/binary; oversized file sample gated by marker slow.\n- Assembly4: minimal two-part assembly, multi-level sub-assemblies, conflicting constraints, missing part reference, circular reference.\n\nGolden artefacts and version locking:\n- Pin FreeCADCmd 0.21.2 and OCCT 7.6.x in worker image; embed versions in artefact metadata.json alongside sha256 for FCStd/STEP/STL/GLB and computed metrics (bbox, surface area, volume, triangle count).\n- Enforce deterministic exports: AutoRefine=False; deterministic meshing (AngularDeflection=5 deg, LinearDeflection=0.1 mm, Relative=False); sort export object list by stable key (Label, Name).\n- Locale stability: set LC_ALL=C.UTF-8, TZ=UTC during golden generation; persist environment block in manifest for reproducibility.\n- Golden store: commit small artefacts to repo under tests/data/golden; place larger ones in MinIO bucket test-golden/<version_tag>/ with immutable retention; manifest maps test_id -> artefact paths + sha256.\n\nDocker Compose CI setup:\n- Services: api, celery, freecad_worker (pinned image tag with FreeCAD/OCCT versions), postgres, redis, rabbitmq, minio, createbuckets; all with healthchecks and depends_on.\n- Pre-test hook: run DB migrations, seed test user/license, create MinIO buckets designs, artefacts, test-golden.\n- Env hardening for determinism: LC_ALL=C.UTF-8, PYTHONHASHSEED=0, FREECAD_USER_HOME=/home/app/.FreeCAD seeded with fixed user.cfg, no GUI settings; disable parallelism in mesher.\n- Matrix job: run tests twice with locales C.UTF-8 and tr_TR.UTF-8 (only parsing/normalization varies; exports forced to C.UTF-8 inside worker).\n\nSHA256 and metrics validation:\n- Compute sha256 server-side on upload to storage; tests independently compute sha256 from downloaded artefacts; compare to golden_manifest.json.\n- Metrics tolerance: bbox exact match within 1e-6 mm, area/volume relative tolerance 1e-6, triangle count exact; failures print diff and offending job_id.\n\nEdge case coverage:\n- Prompt: ambiguous -> 425; missing-required -> ERR-AI-422 with AI_HINT_REQUIRED; token-limit truncation handled gracefully; unsupported unit/material -> 422 with field path.\n- Params: zero/negative dimensions -> 422; mixed-units normalized; float rounding stability across locales.\n- Upload: STEP with BOM/encoding anomalies -> 422 with remediation hints; STL non-manifold -> 422 with detailed issues; huge file -> 413 (when enabled) or 422 size hint.\n- Assembly4: missing part or circular constraints -> 422; name collisions -> 409; inconsistent units across parts -> 422.\n- Idempotency: repeated POST with same idempotency_key produces one job and stable artefact refs; concurrent submissions tested.\n- Rate limit: 429 with correct headers; retry-after asserted.\n\nTurkish test scenarios (based on FreeCAD/locale research):\n- Prompts in Turkish with units and decimal comma, e.g., “100,5 mm uzunluğunda, 20 mm genişliğinde plaka; ortada 10 mm delik; malzeme: alüminyum 6061” expecting correct normalization to SI units.\n- Turkish synonyms and casing edge cases (İ/ı, I/i) in material and feature names; ensure case-folding uses locale-insensitive mapping for identifiers while preserving labels for display.\n- Params files using \"milimetre\", \"çelik\", \"alüminyum\" and decimal comma; filenames and part labels containing Ç, Ş, Ğ, İ tested for round-trip through storage and FreeCAD document.\n- Locale matrix asserts that parsing succeeds under LC_ALL=tr_TR.UTF-8 and exported artefacts remain byte-identical to golden (worker enforces C.UTF-8).\n\nTooling and scripts:\n- Script tools/gen_golden.py to regenerate artefacts when bumping FreeCAD/OCCT; writes manifest entries with versions, env, metrics, sha256; guarded by --approve flag.\n- pytest markers: slow, locale_tr, uploads, a4; default CI includes all but slow unless nightly; per-marker selection supported.\n- Retry helper polls /jobs/:id with jitter and hard timeout; on failure, attaches last 200 lines of worker logs for diagnostics.\n\nAcceptance additions:\n- CI matrix (C.UTF-8, tr_TR.UTF-8) passes with byte-identical artefacts to golden across clean runners.\n- Total runtime under 15 minutes on 2 vCPU CI runner; flaky test detector reports zero flakes across 5 reruns.\n</info added on 2025-08-25T20:42:23.922Z>\n<info added on 2025-08-25T20:44:34.357Z>\nVersion alignment update:\n- Pin FreeCADCmd to 1.1.0 and OCCT to 7.8.x (use the exact OCCT version bundled with the 1.1.0 worker image; record it verbatim, e.g., 7.8.1).\n- Update freecad_worker image tag to the 1.1.0/OCCT 7.8.x build and surface both versions at startup; tests assert they match expected.\n- Regenerate all golden artefacts with tools/gen_golden.py and bump version_tag (e.g., fcad-1.1.0-occt-7.8.x); update golden_manifest.json env block to include freecad_version=1.1.0 and occt_version=7.8.x.\n- Move large artefacts to MinIO under test-golden/<new version_tag>/ and refresh sha256 in the manifest.\n- Keep export determinism settings unchanged; verify metrics tolerances still pass and triangle counts remain exact under OCCT 7.8.x.\n- CI checks additionally verify FreeCADCmd --version contains 1.1.0 and FreeCAD.Base.OCC_VERSION reports 7.8.x before running tests.\n</info added on 2025-08-25T20:44:34.357Z>",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 15,
            "title": "Database migrations and schema setup for model flows",
            "description": "Create and migrate database tables for models, ai_suggestions, artefacts with proper relationships and constraints",
            "details": "Implement database layer:\\n- Create 'models' table with job_id FK, canonical_params JSON, script_hash, status enum (pending/processing/completed/failed)\\n- Create 'ai_suggestions' table with prompt (masked), response JSON, user_id, request_id, created_at\\n- Create 'artefacts' table with job_id FK, file_type enum (fcstd/step/stl/glb), s3_key, sha256, metadata JSON\\n- Add idempotency_key unique constraint on jobs table\\n- Implement job state machine transitions with audit logging\\n- Add indexes for performance: job_id, user_id, status, created_at\\n- Turkish KVKK compliance: PII columns marked, retention policies\\nAcceptance: Migrations run cleanly up/down, constraints enforced, queries optimized with EXPLAIN ANALYZE\n<info added on 2025-08-25T20:55:44.903Z>\nFreeCAD 1.1.0 alignment and versioning:\n- Extend 'models' table with:\n  - freecad_version VARCHAR(16) NOT NULL CHECK (freecad_version ~ '^1\\\\.1\\\\.\\\\d+$')\n  - occt_version VARCHAR(16) NOT NULL CHECK (occt_version ~ '^7\\\\.8\\\\.\\\\d+$')\n  - model_rev INT NOT NULL DEFAULT 1\n  - parent_model_id BIGINT NULL REFERENCES models(id) ON DELETE SET NULL\n  - freecad_doc_uuid UUID NULL\n  - doc_schema_version SMALLINT NOT NULL DEFAULT 110\n  - Unique constraint on (freecad_doc_uuid, model_rev) where freecad_doc_uuid is not null\n  - Indexes: (freecad_version, occt_version), (freecad_doc_uuid), (model_rev)\n\nOCCT 7.8.x topology hashes for deterministic exports:\n- Create 'topology_hashes' table:\n  - id BIGSERIAL PK\n  - artefact_id BIGINT NOT NULL REFERENCES artefacts(id) ON DELETE CASCADE\n  - object_path TEXT NOT NULL  -- e.g., \"Body/Pad/Face6\"\n  - shape_kind TEXT NOT NULL CHECK (shape_kind IN ('Solid','Shell','Face','Edge','Vertex'))\n  - topo_hash TEXT NOT NULL  -- stable hash from OCCT 7.8.x\n  - occt_algo_version VARCHAR(16) NOT NULL DEFAULT '7.8.x'\n  - created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n  - Unique (artefact_id, object_path, shape_kind)\n  - Indexes: (topo_hash), (artefact_id), (occt_algo_version)\n- Populate/consume topology hashes via artefact post-processing; store summary under artefacts.metadata.occt.topology_hashes for quick JSON retrieval; add GIN index on artefacts(metadata)\n\nAssembly4 workbench metadata:\n- Persist A4 constraint data under artefacts.metadata.assembly4.constraints (array of constraints with link paths/LCS/expressions); add GIN index on artefacts using (metadata jsonb_path_ops) and expression index on (metadata->'assembly4'->'constraints')\n- For assemblies, set models.freecad_doc_uuid and doc_schema_version=110; enforce presence of assembly4.constraints for artefacts.file_type='fcstd' when models flow is Assembly4\n\nEnhanced model document structure/versioning:\n- Store lightweight document object map under artefacts.metadata.freecad.document_index (object label, TypeId, Uuid); add expression index on (metadata->'freecad'->'document_index')\n- Add trigger to increment models.model_rev on derived/cloned model creation; parent_model_id references source model\n\nMigration safety and compatibility checks:\n- Pre-upgrade guard: if any existing models rows exist with status IN ('processing','completed') then freecad_version and occt_version must be present and satisfy the above CHECKs; otherwise abort migration\n- Add optional guard function assert_toolchain_compat() that raises if current configured toolchain (recorded in a 'deployed_toolchain' row if present) does not match FreeCAD 1.1.x and OCCT 7.8.x; migration invokes it if the table exists\n- Backfill step (no-op if empty DB): set freecad_version/occt_version from artefacts.metadata.freecad.version/metadata.occt.version where available; rows lacking values must be updated by the worker before completion transitions\n\nAcceptance additions:\n- Insertion fails for models with freecad_version not matching ^1.1.x or occt_version not matching ^7.8.x\n- FCStd artefacts for assemblies expose assembly4.constraints in metadata and are queryable using the new GIN/expression indexes (verified via EXPLAIN ANALYZE)\n- Re-exports of the same geometry produce identical topology_hashes entries; lookup by topo_hash returns the expected artefact rows\n- Up/down migrations create/drop new columns, table, constraints, and indexes cleanly; guards prevent upgrade if incompatible versions detected\n</info added on 2025-08-25T20:55:44.903Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 16,
            "title": "Real-time progress updates via WebSocket/SSE",
            "description": "Implement WebSocket or Server-Sent Events for real-time job progress updates to clients",
            "details": "Real-time communication layer:\\n- WebSocket endpoint: ws://localhost:8000/ws/jobs/{job_id}/progress\\n- Alternative SSE endpoint: GET /jobs/{job_id}/progress/stream\\n- Progress message schema: {job_id, status, progress_pct, current_step, message, timestamp}\\n- Progress throttling: max 1 update per 500ms per job to prevent spam\\n- Client reconnection handling with last_event_id for SSE\\n- Redis pub/sub for worker→API progress propagation\\n- Authentication via JWT token in query params or Authorization header\\n- Graceful degradation: polling fallback if WebSocket fails\\n- Integration with Celery task update_state() for progress reporting\\nAcceptance: Real-time updates flow from worker to client, reconnection works, throttling prevents overload\n<info added on 2025-08-25T20:56:35.157Z>\nFreeCAD 1.1.0-specific progress events and schema enhancements:\n\n- Progress message schema v2 additions:\n  - schema_version\n  - freecad_version (e.g., 1.1.0), occt_version (e.g., 7.8.x), workbench (Part, PartDesign, Assembly4, Material), platform\n  - event_id (monotonic per job), event_type (phase, assembly4, material, occt, topology_hash, doc_graph, document, export)\n  - operation_id (stable UUID per operation within job), operation_name, operation_group (assembly4|occt|material|topology|doc_graph|document|export)\n  - phase (start|progress|end), subphase\n  - step_index, step_total, items_done, items_total, eta_ms, elapsed_ms, milestone (bool)\n  - document_id, document_label\n  - object_name, object_type, feature_type\n  - operation_metadata (key/value map for operation-specific fields)\n  - error_code (optional), warning (optional)\n  - All existing fields remain supported (job_id, status, progress_pct, current_step, message, timestamp)\n\n- Document lifecycle and structure (FreeCAD 1.1.0):\n  - event_type=document with phases: document_open, document_load_objects, recompute_start, recompute_end\n  - event_type=doc_graph with phases: depgraph_build_start/progress/end and fields nodes_done/nodes_total, edges_done/edges_total\n  - event_type=phase for object creation and property setting:\n    - feature_create_start/progress/end (feature_type, object_name)\n    - property_set (object_name, property, value_preview)\n    - expression_evaluate (object_name, property, success)\n\n- Assembly4 (A4) solving and LCS placement:\n  - event_type=assembly4, operation_group=assembly4\n  - solver_start/progress/end with fields constraints_resolved/constraints_total, lcs_resolved/lcs_total, iteration, residual\n  - lcs_placement_start/progress/end with fields lcs_name, placements_done/placements_total\n\n- Material Framework (FreeCAD 1.1.0 enhanced materials):\n  - event_type=material, operation_group=material\n  - material_resolve_library (library_name, material_key)\n  - material_apply_start/progress/end with fields objects_done/objects_total, mat_uid, appearance_bake=true/false\n  - material_override_properties (count, properties_list_preview)\n\n- OCCT 7.8.x operations:\n  - event_type=occt, operation_group=occt, occt_op in {boolean_fuse, boolean_cut, boolean_common, fillet, chamfer}\n  - boolean_* start/progress/end with shapes_done/shapes_total, solids_in, solids_out\n  - fillet start/progress/end with edges_done/edges_total, default_radius, variable_radius=true/false\n  - chamfer start/progress/end with edges_done/edges_total, mode (distance|twoDistances|distanceAngle)\n\n- Topology hash for deterministic export validation:\n  - event_type=topology_hash, operation_group=topology\n  - topo_hash_start/progress/end with faces_done/faces_total, edges_done/edges_total, vertices_done/vertices_total, stable_id_mode, seed\n  - export_validation with computed_hash, expected_hash (if provided), match=true/false\n\n- Deterministic export and recompute checkpoints:\n  - event_type=export with step_export_start/progress/end, format (FCStd|STEP|STL|GLB), bytes_written/bytes_total\n\n- Transport and throttling updates:\n  - milestone=true events (phase transitions start/end, solver_start/end, export_validation) bypass throttling; non-milestone events remain subject to max 1 update per 500ms per job\n  - event_id included in both WebSocket and SSE; SSE last_event_id resumes from last delivered event if available\n  - Include freecad_version and occt_version in every event for client-side filtering and display\n\n- Worker instrumentation:\n  - Celery update_state(meta=...) populated with the new schema v2 fields\n  - Redis pub/sub payloads carry operation_metadata as a flat map; large lists truncated with counts and previews to keep messages lightweight\n\n- Acceptance (additional):\n  - For a job executed with FreeCAD 1.1.0 and OCCT 7.8.x, clients receive:\n    - document and doc_graph events with increasing nodes_done/nodes_total\n    - assembly4 solver_progress showing constraints_resolved and residual updates, plus lcs_placement progress\n    - material_apply progress with objects_done/objects_total and mat_uid present\n    - occt events for boolean, fillet, and chamfer with edges_done/edges_total or shapes_done/shapes_total\n    - topology_hash progress with faces/edges/vertices counters and an export_validation event reporting computed_hash\n    - export events per format with bytes_written advancing\n    - All events include freecad_version, occt_version, event_id, and operation_id; milestone events are delivered immediately despite throttling\n  - Reconnection via SSE with last_event_id restores stream continuity without missing phase transitions\n</info added on 2025-08-25T20:56:35.157Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 17,
            "title": "Observability integration for model generation flows",
            "description": "Integrate structured logging, metrics, and distributed tracing for all model generation operations",
            "details": "Observability stack integration:\\n- Structured logging: bind job_id, request_id, user_id to all log entries\\n- Prometheus metrics:\\n  - model_generation_total{type, status}\\n  - model_generation_duration_seconds{type}\\n  - ai_adapter_latency_seconds{provider}\\n  - freecad_worker_duration_seconds{operation}\\n  - export_duration_seconds{format}\\n- OpenTelemetry tracing:\\n  - Span per endpoint with job context\\n  - Span linking: API→Celery→FreeCAD subprocess\\n  - Baggage propagation for request_id\\n- Grafana dashboards:\\n  - Model generation success rate\\n  - P95 latencies per flow\\n  - AI provider performance\\n  - Worker resource usage\\n- Alert rules:\\n  - High failure rate (>10%)\\n  - Slow generation (>5min)\\n  - Worker OOM/timeout\\nAcceptance: Traces flow end-to-end, metrics exported, dashboards show real data\n<info added on 2025-08-25T20:57:33.343Z>\nFreeCAD 1.1.0/OCCT 7.8.x extensions:\n- Metric labels: add freecad_version, occt_version, workbench (if applicable) to all FreeCAD/OCCT-related metrics for segmentation\n- Prometheus metrics (new):\n  - freecad_document_load_seconds{source, workbench, freecad_version, occt_version}\n  - freecad_recompute_duration_seconds{workbench, doc_complexity}\n  - freecad_object_created_total{class, workbench}\n  - occt_boolean_duration_seconds{operation=union|cut|common, solids_range}\n  - occt_feature_duration_seconds{feature=fillet|chamfer}\n  - occt_operation_memory_bytes{operation}\n  - a4_constraint_solve_duration_seconds{solver}\n  - a4_lcs_resolution_duration_seconds{lcs_count_range}\n  - a4_solver_iterations_total{solver}\n  - material_library_access_total{library, result=hit|miss|error}\n  - material_property_apply_duration_seconds{property}\n  - material_appearance_apply_duration_seconds{appearance_type}\n  - topology_hash_compute_duration_seconds{scope=part|assembly}\n  - deterministic_export_validation_total{format=STEP|STL|GLB, result=pass|fail}\n  - freecad_workbench_invocations_total{workbench}\n  - freecad_workbench_compatibility_total{workbench, compatible=true|false}\n- OpenTelemetry tracing:\n  - Add spans: freecad.document_load, freecad.recompute, occt.boolean, occt.fillet, occt.chamfer, a4.solve_constraints, a4.lcs_resolve, material.library_access, material.apply_properties, material.apply_appearance, topology.hash_compute, export.validate_deterministic\n  - Span attributes: freecad_version=1.1.0, occt_version=7.8.x, workbench, operation, solver, object_class, format\n- Grafana dashboards:\n  - FreeCAD 1.1.0 overview segmented by freecad_version/occt_version\n  - Document load, recompute, and object creation rate panels per workbench\n  - OCCT Boolean/fillet/chamfer latency and memory usage\n  - Assembly4 solver time and iteration distributions; LCS resolution time vs LCS count\n  - Material library hit/miss and application latencies\n  - Topology hash compute time and deterministic export pass rate\n  - Workbench usage and compatibility trend\n- Alert rules (additional):\n  - Deterministic export failure rate >2% over 5m by format and version\n  - OCCT Boolean P95 >30s or occt_operation_memory_bytes >1.5GiB\n  - Assembly4 solver P95 >15s or a4_solver_iterations_total P95 >200\n  - Material library access error rate >5% over 10m\nAcceptance update: Metrics and traces reflect FreeCAD 1.1.0 and OCCT 7.8.x labels; dashboards segment by versions and show real data for new metrics; alerts fire under the above thresholds.\n</info added on 2025-08-25T20:57:33.343Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 18,
            "title": "Security review and configuration management",
            "description": "Implement security hardening, SBOM generation, and feature flags for model generation flows",
            "details": "Security and configuration layer:\\n- Security review:\\n  - OWASP API Security Top 10 checklist\\n  - Input validation audit (path traversal, command injection)\\n  - FreeCAD script sandboxing verification\\n  - Secret scanning in code and configs\\n  - Dependency vulnerability scan (safety, bandit)\\n- SBOM (Software Bill of Materials):\\n  - Generate with syft or similar\\n  - Include FreeCAD, Python deps, system libs\\n  - CVE tracking and alerts\\n- Feature flags (via environment or DB):\\n  - ENABLE_AI_PROMPT_FLOW (default: true)\\n  - ENABLE_UPLOAD_FLOW (default: true)\\n  - ENABLE_ASSEMBLY4 (default: false, experimental)\\n  - MAX_CONCURRENT_FREECAD_WORKERS (default: 4)\\n  - ENABLE_PREVIEW_GENERATION (default: true)\\n- Configuration management:\\n  - Pydantic Settings with validation\\n  - Environment-specific configs (dev/staging/prod)\\n  - Secrets via vault or K8s secrets\\n  - Health check endpoint with config visibility\\nAcceptance: Security scan passes, SBOM generated, feature flags toggle flows, configs validated at startup\n<info added on 2025-08-25T20:58:13.883Z>\nFreeCAD 1.1.0 hardening:\n- Run FreeCADCmd in isolated subprocess with no network, seccomp/AppArmor profile, non-root user, read-only root filesystem and capped tmpfs workspace; drop Linux capabilities; enforce per-job CPU/memory/time limits.\n- Restrict Python module access: allowlist required modules only; block os, sys, subprocess, socket, ctypes, importlib, shlex, mmap, pty; disable user macros/addons and usersite; sanitize PYTHONPATH and FreeCAD resource paths.\n- Secure workbench loading: preload only approved workbenches (Core, Assembly4, Material); pin versions and verify checksums; disable AddonManager and dynamic workbench discovery in production.\n\nOCCT 7.8.x monitoring and safe operations:\n- Track OpenCascade 7.8.x CVEs via NVD/OSV/vendor feeds with alerts and remediation guidance; document approved 7.8.x build.\n- Enforce safe geometry bounds: configurable caps on faces/edges, Boolean/fillet/mesh timeouts, mesh triangle count, shape-healing iterations; catch kernel exceptions and abort on threshold breaches.\n\nSBOM pinning:\n- Pin exact FreeCAD 1.1.0 and OCCT 7.8.x versions (build IDs and hashes) in SBOM; include OS packages/shared libs; sign SBOM; fail build on drift.\n\nContainer runtime hardening:\n- Minimal base image with only FreeCAD 1.1.0/OCCT dependencies; remove package managers/compilers; read-only filesystem, no-new-privileges, no privilege escalation, seccomp default, AppArmor confinement; egress disabled by default.\n\nSecure document properties and metadata:\n- Sanitize and length-limit all document properties; strip/disable expressions/macros in properties; block external links/paths; whitelist property types; escape on serialization; scrub before saving FCStd.\n\nAssembly4 and Material Framework controls (1.1.0):\n- Assembly4: allow only local document links and approved LCS/Constraint objects; forbid FeaturePython from untrusted sources; block expressions referencing Python; verify workbench files against pinned checksums.\n- Material Framework: accept only data-driven material definitions (JSON/CSV); validate schema/units; block executable hooks; pin and checksum material resources.\n\nFreeCAD 1.1.0 input and structure validation:\n- Validate units, parameters, and document structure against known 1.1.0 object classes; reject unknown FeaturePython classes; limit object count/depth and STEP/IGES header fields; ensure no embedded Python or unsafe expressions; reject malformed/oversized documents.\n\nAcceptance additions:\n- Blocked module imports and macro execution verified under 1.1.0.\n- SBOM shows pinned FreeCAD 1.1.0 and OCCT 7.8.x with signatures; build fails on version drift.\n- Container runs with read-only FS, no-new-privileges, and no network; seccomp/AppArmor policies verified.\n- OCCT 7.8.x CVE watch active with test alert; heavy geometry ops respect configured caps/timeouts.\n- Malicious metadata/doc structures are rejected; Assembly4/Material workbenches load under pinned versions with restrictions enforced.\n</info added on 2025-08-25T20:58:13.883Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 19,
            "title": "FreeCAD document lifecycle management",
            "description": "Implement robust document lifecycle management with transaction support, versioning, and recovery",
            "details": "Implement comprehensive FreeCAD document lifecycle management:\n- Document creation/opening with deterministic naming (job_id based)\n- Transaction management: openTransaction/commitTransaction/abortTransaction\n- Document versioning and revision tracking\n- Auto-save and recovery mechanisms\n- Memory management and cleanup (close documents, gc.collect)\n- Document locking for concurrent access prevention\n- Document metadata and properties management\n- Undo/Redo stack management\n- Document compression and storage optimization\n- Multi-document coordination for assemblies\n- Document migration for version upgrades\n- Backup and restore functionality\nAcceptance: Document operations are atomic, recoverable, memory-efficient, and support concurrent job processing without conflicts",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 20,
            "title": "Multi-format Import/Export Pipeline Enhancement",
            "description": "Implement comprehensive import/export capabilities beyond basic STEP/STL, including native FreeCAD formats, industry-specific formats, and metadata preservation",
            "details": "## Multi-format Import/Export Pipeline Enhancement\n\n### Import Capabilities\n```python\nclass UniversalImporter:\n    SUPPORTED_FORMATS = {\n        # Native FreeCAD\n        '.FCStd': 'native_freecad',\n        '.FCMacro': 'freecad_macro',\n        \n        # CAD Formats\n        '.stp': 'step_import',\n        '.step': 'step_import',\n        '.iges': 'iges_import',\n        '.igs': 'iges_import',\n        '.brep': 'brep_import',\n        '.brp': 'brep_import',\n        '.sat': 'acis_import',\n        '.sab': 'acis_binary',\n        \n        # Mesh Formats\n        '.stl': 'stl_import',\n        '.obj': 'obj_import',\n        '.ply': 'ply_import',\n        '.off': 'off_import',\n        '.3mf': 'threemf_import',\n        '.amf': 'amf_import',\n        \n        # Drawing Formats\n        '.dxf': 'dxf_import',\n        '.dwg': 'dwg_import',\n        '.svg': 'svg_import',\n        \n        # Point Cloud\n        '.pcd': 'pointcloud_import',\n        '.xyz': 'xyz_import',\n        '.las': 'lidar_import',\n        \n        # Industry Specific\n        '.ifc': 'ifc_import',  # Architecture\n        '.dae': 'collada_import',  # Animation\n        '.gltf': 'gltf_import',  # Web 3D\n        '.glb': 'glb_import',  # Web 3D binary\n        \n        # Material & Appearance\n        '.FCMat': 'material_import',\n        '.mtl': 'material_library'\n    }\n    \n    async def import_with_metadata(\n        self,\n        file_path: Path,\n        preserve_history: bool = True,\n        preserve_materials: bool = True,\n        preserve_constraints: bool = True,\n        unit_system: str = \"metric\",\n        coordinate_system: str = \"Z-up\"\n    ) -> ImportResult:\n        \"\"\"Import with full metadata preservation\"\"\"\n        pass\n```\n\n### Export Pipeline with Quality Control\n```python\nclass EnhancedExporter:\n    async def export_with_validation(\n        self,\n        doc: FreeCAD.Document,\n        format: str,\n        options: ExportOptions\n    ) -> ExportResult:\n        # Pre-export validation\n        validation = await self.validate_for_export(doc, format)\n        if not validation.is_valid:\n            raise ExportValidationError(validation.errors)\n        \n        # Format-specific optimization\n        optimized_doc = await self.optimize_for_format(doc, format)\n        \n        # Export with metadata\n        export_data = await self.export_with_metadata(\n            optimized_doc,\n            format,\n            options\n        )\n        \n        # Post-export verification\n        verification = await self.verify_export(export_data, format)\n        \n        return ExportResult(\n            data=export_data,\n            validation=validation,\n            verification=verification,\n            metadata=self.extract_metadata(doc)\n        )\n```\n\n### Format Conversion Matrix\n```python\nCONVERSION_MATRIX = {\n    # Source -> Target -> Method\n    ('STEP', 'STL'): 'tessellation_conversion',\n    ('STL', 'STEP'): 'reverse_engineering',\n    ('DXF', 'SVG'): 'vector_conversion',\n    ('FCStd', 'GLTF'): 'web3d_conversion',\n    ('IFC', 'FCStd'): 'bim_import',\n    ('FCStd', 'IFC'): 'bim_export',\n    ('BREP', 'STEP'): 'topology_preservation',\n    ('OBJ', 'GLB'): 'mesh_optimization'\n}\n```\n\n### Batch Processing Support\n```python\nclass BatchImportExport:\n    async def batch_convert(\n        self,\n        source_files: List[Path],\n        target_format: str,\n        options: BatchOptions\n    ) -> BatchResult:\n        \"\"\"Convert multiple files with progress tracking\"\"\"\n        results = []\n        async with asyncio.TaskGroup() as tg:\n            for file in source_files:\n                task = tg.create_task(\n                    self.convert_single(file, target_format, options)\n                )\n                results.append(task)\n        \n        return BatchResult(\n            successful=len([r for r in results if r.success]),\n            failed=len([r for r in results if not r.success]),\n            details=results\n        )\n```\n\n### Metadata Preservation\n```python\nclass MetadataHandler:\n    def preserve_metadata(self, doc: FreeCAD.Document) -> dict:\n        return {\n            'author': doc.Author,\n            'company': doc.Company,\n            'license': doc.License,\n            'creation_date': doc.CreationDate,\n            'last_modified': doc.LastModifiedDate,\n            'custom_properties': self.extract_custom_properties(doc),\n            'materials': self.extract_materials(doc),\n            'colors': self.extract_colors(doc),\n            'textures': self.extract_textures(doc),\n            'units': doc.UnitSystem,\n            'tolerances': self.extract_tolerances(doc)\n        }\n```\n\n### Format-Specific Options\n```python\nEXPORT_OPTIONS = {\n    'STEP': {\n        'schema': ['AP203', 'AP214', 'AP242'],\n        'write_surfaces': True,\n        'write_solids': True,\n        'compression': True\n    },\n    'STL': {\n        'ascii': False,\n        'angular_deflection': 0.5,\n        'linear_deflection': 0.01,\n        'relative': False\n    },\n    'GLTF': {\n        'embed_textures': True,\n        'draco_compression': True,\n        'preserve_hierarchy': True,\n        'include_animations': False\n    },\n    'IFC': {\n        'schema': 'IFC4',\n        'include_properties': True,\n        'include_quantities': True,\n        'site_information': True\n    }\n}\n```\n\n### Turkish Localization\n```python\nFORMAT_NAMES_TR = {\n    'STEP': 'STEP (Standart CAD Formatı)',\n    'STL': 'STL (3D Baskı Formatı)',\n    'IGES': 'IGES (Yüzey Modeli)',\n    'DXF': 'DXF (2D Çizim)',\n    'IFC': 'IFC (BIM Formatı)',\n    'GLTF': 'glTF (Web 3D Formatı)',\n    'FCStd': 'FreeCAD Yerel Format'\n}\n\nEXPORT_MESSAGES_TR = {\n    'validating': 'Model doğrulanıyor...',\n    'optimizing': 'Format için optimize ediliyor...',\n    'exporting': 'Dışa aktarılıyor...',\n    'verifying': 'Dışa aktarım doğrulanıyor...',\n    'complete': 'Dışa aktarım tamamlandı'\n}\n```\n\n### Integration with Task 7.9\n- Extends deterministic export capabilities\n- Adds format-specific validation\n- Preserves complete metadata chain\n- Supports industry-specific requirements\n\n### Performance Considerations\n- Parallel batch processing\n- Format-specific optimization\n- Streaming for large files\n- Memory-efficient conversion\n\n### Security Requirements\n- File type validation\n- Size limits per format\n- Malicious content scanning\n- Metadata sanitization",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 21,
            "title": "Collaborative Editing and Real-time Synchronization",
            "description": "Implement real-time collaborative editing capabilities for FreeCAD models with conflict resolution, presence awareness, and change tracking",
            "details": "## Collaborative Editing and Real-time Synchronization\n\n### Operational Transformation Engine\n```python\nclass OperationalTransform:\n    \"\"\"Transform concurrent operations for conflict-free collaboration\"\"\"\n    \n    def transform_operation(\n        self,\n        op1: ModelOperation,\n        op2: ModelOperation,\n        priority: str = \"timestamp\"\n    ) -> Tuple[ModelOperation, ModelOperation]:\n        \"\"\"Transform operations to maintain consistency\"\"\"\n        if op1.type == \"modify\" and op2.type == \"modify\":\n            if op1.object_id == op2.object_id:\n                # Same object modified - resolve conflict\n                return self.resolve_modify_conflict(op1, op2, priority)\n        elif op1.type == \"delete\" and op2.type == \"modify\":\n            if op1.object_id == op2.object_id:\n                # Object deleted vs modified - delete wins\n                return (op1, NoOperation())\n        \n        return (op1, op2)\n    \n    def apply_operation_sequence(\n        self,\n        doc: FreeCAD.Document,\n        operations: List[ModelOperation]\n    ) -> FreeCAD.Document:\n        \"\"\"Apply transformed operations in correct order\"\"\"\n        for op in self.order_operations(operations):\n            doc = op.apply(doc)\n        return doc\n```\n\n### Real-time Synchronization Protocol\n```python\nclass CollaborationProtocol:\n    def __init__(self):\n        self.websocket_manager = WebSocketManager()\n        self.operation_queue = OperationQueue()\n        self.conflict_resolver = ConflictResolver()\n    \n    async def handle_client_operation(\n        self,\n        session_id: str,\n        operation: ModelOperation\n    ):\n        # Timestamp and version the operation\n        operation.timestamp = datetime.now(UTC)\n        operation.version = self.get_document_version()\n        \n        # Transform against pending operations\n        transformed = await self.transform_against_pending(operation)\n        \n        # Apply to master document\n        await self.apply_to_master(transformed)\n        \n        # Broadcast to other clients\n        await self.broadcast_operation(transformed, exclude=session_id)\n        \n        # Update operation history\n        await self.update_history(transformed)\n```\n\n### Presence and Awareness System\n```python\nclass PresenceAwareness:\n    def __init__(self):\n        self.active_users = {}\n        self.user_cursors = {}\n        self.user_selections = {}\n    \n    async def update_user_presence(\n        self,\n        user_id: str,\n        presence_data: PresenceData\n    ):\n        self.active_users[user_id] = {\n            'name': presence_data.name,\n            'color': presence_data.color,\n            'avatar': presence_data.avatar,\n            'status': presence_data.status,\n            'last_active': datetime.now(UTC)\n        }\n        \n        # Broadcast presence update\n        await self.broadcast_presence_update(user_id)\n    \n    async def track_user_cursor(\n        self,\n        user_id: str,\n        cursor_position: Point3D,\n        viewport: ViewportInfo\n    ):\n        \"\"\"Track 3D cursor position for awareness\"\"\"\n        self.user_cursors[user_id] = {\n            'position': cursor_position,\n            'viewport': viewport,\n            'timestamp': datetime.now(UTC)\n        }\n        \n        # Throttled broadcast (max 30 FPS)\n        await self.throttled_broadcast_cursor(user_id)\n    \n    async def track_user_selection(\n        self,\n        user_id: str,\n        selected_objects: List[str]\n    ):\n        \"\"\"Track what objects user has selected\"\"\"\n        self.user_selections[user_id] = selected_objects\n        \n        # Lock objects for editing\n        await self.lock_objects_for_user(user_id, selected_objects)\n```\n\n### Conflict Resolution System\n```python\nclass ConflictResolver:\n    RESOLUTION_STRATEGIES = {\n        'timestamp': 'last_write_wins',\n        'priority': 'user_priority_based',\n        'merge': 'automatic_merge',\n        'manual': 'user_intervention_required'\n    }\n    \n    async def resolve_conflict(\n        self,\n        conflict: ModelConflict,\n        strategy: str = 'merge'\n    ) -> Resolution:\n        if strategy == 'merge':\n            # Try automatic merge\n            merged = await self.attempt_automatic_merge(conflict)\n            if merged.success:\n                return merged\n        \n        elif strategy == 'timestamp':\n            # Last write wins\n            return self.apply_latest_change(conflict)\n        \n        elif strategy == 'manual':\n            # Queue for manual resolution\n            await self.queue_for_manual_resolution(conflict)\n            return Resolution(pending=True)\n        \n        return Resolution(failed=True)\n```\n\n### Change Tracking and History\n```python\nclass ChangeTracker:\n    def __init__(self):\n        self.change_log = []\n        self.undo_stack = []\n        self.redo_stack = []\n    \n    def record_change(self, change: ModelChange):\n        \"\"\"Record change with full context\"\"\"\n        change_record = {\n            'id': uuid.uuid4(),\n            'timestamp': datetime.now(UTC),\n            'user': change.user_id,\n            'operation': change.operation,\n            'before_state': change.before_state,\n            'after_state': change.after_state,\n            'affected_objects': change.affected_objects,\n            'metadata': change.metadata\n        }\n        \n        self.change_log.append(change_record)\n        self.undo_stack.append(change_record)\n        self.redo_stack.clear()\n    \n    async def undo_change(self, change_id: str):\n        \"\"\"Undo specific change with cascade handling\"\"\"\n        change = self.find_change(change_id)\n        if change:\n            # Check for dependent changes\n            dependents = self.find_dependent_changes(change)\n            if dependents:\n                # Handle cascade undo\n                await self.cascade_undo(change, dependents)\n            else:\n                # Simple undo\n                await self.apply_inverse_operation(change)\n```\n\n### Locking and Transaction Management\n```python\nclass CollaborativeLocking:\n    def __init__(self):\n        self.object_locks = {}\n        self.transaction_locks = {}\n    \n    async def acquire_object_lock(\n        self,\n        user_id: str,\n        object_ids: List[str],\n        lock_type: str = 'exclusive'\n    ) -> LockResult:\n        \"\"\"Acquire locks for editing objects\"\"\"\n        locks_acquired = []\n        locks_failed = []\n        \n        for obj_id in object_ids:\n            if obj_id in self.object_locks:\n                current_lock = self.object_locks[obj_id]\n                if current_lock.type == 'exclusive':\n                    locks_failed.append(obj_id)\n                elif lock_type == 'exclusive':\n                    # Wait or fail based on policy\n                    locks_failed.append(obj_id)\n                else:\n                    # Shared lock compatible\n                    locks_acquired.append(obj_id)\n            else:\n                # Acquire new lock\n                self.object_locks[obj_id] = Lock(\n                    user_id=user_id,\n                    type=lock_type,\n                    acquired_at=datetime.now(UTC)\n                )\n                locks_acquired.append(obj_id)\n        \n        return LockResult(\n            acquired=locks_acquired,\n            failed=locks_failed\n        )\n```\n\n### Offline Support and Sync\n```python\nclass OfflineSync:\n    async def handle_reconnection(\n        self,\n        session_id: str,\n        offline_operations: List[ModelOperation]\n    ):\n        \"\"\"Sync offline changes when reconnected\"\"\"\n        # Get operations that occurred while offline\n        server_ops = await self.get_operations_since(\n            session_id.last_sync_version\n        )\n        \n        # Transform offline operations against server operations\n        transformed_ops = []\n        for offline_op in offline_operations:\n            transformed = offline_op\n            for server_op in server_ops:\n                transformed = self.transform(transformed, server_op)\n            transformed_ops.append(transformed)\n        \n        # Apply transformed operations\n        for op in transformed_ops:\n            await self.apply_operation(op)\n        \n        # Update client state\n        await self.send_state_update(session_id)\n```\n\n### Performance Optimization\n```python\nclass CollaborationOptimizer:\n    def __init__(self):\n        self.operation_batcher = OperationBatcher()\n        self.delta_compressor = DeltaCompressor()\n    \n    async def optimize_broadcast(\n        self,\n        operations: List[ModelOperation]\n    ) -> CompressedUpdate:\n        \"\"\"Optimize updates for network efficiency\"\"\"\n        # Batch similar operations\n        batched = self.operation_batcher.batch(operations)\n        \n        # Compress deltas\n        compressed = self.delta_compressor.compress(batched)\n        \n        # Prioritize visible changes\n        prioritized = self.prioritize_by_viewport(compressed)\n        \n        return prioritized\n```\n\n### Turkish Localization\n```python\nCOLLABORATION_MESSAGES_TR = {\n    'user_joined': '{user} düzenlemeye katıldı',\n    'user_left': '{user} düzenlemeden ayrıldı',\n    'object_locked': 'Nesne {user} tarafından düzenleniyor',\n    'conflict_detected': 'Çakışma tespit edildi',\n    'changes_synced': 'Değişiklikler senkronize edildi',\n    'offline_mode': 'Çevrimdışı modda çalışıyorsunuz',\n    'reconnected': 'Bağlantı yeniden kuruldu'\n}\n```\n\n### Security Considerations\n- End-to-end encryption for sensitive models\n- Permission-based access control\n- Audit trail for all changes\n- Rate limiting for operations\n- Sandbox execution for untrusted operations\n\n### Integration Points\n- WebSocket infrastructure (Task 7.16)\n- Redis for distributed locking\n- PostgreSQL for change history\n- MinIO for model snapshots",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 22,
            "title": "Version Control and Branching for CAD Models",
            "description": "Implement Git-like version control system specifically designed for FreeCAD models with branching, merging, diffing, and rollback capabilities",
            "details": "## Version Control and Branching for CAD Models\n\n### Model Version Control System\n```python\nclass ModelVersionControl:\n    \"\"\"Git-like VCS for FreeCAD models\"\"\"\n    \n    def __init__(self, repository_path: Path):\n        self.repo_path = repository_path\n        self.object_store = ObjectStore(repository_path / '.mvcstore')\n        self.refs = RefManager(repository_path / '.mvcstore/refs')\n        self.index = IndexManager(repository_path / '.mvcstore/index')\n    \n    async def init_repository(self) -> Repository:\n        \"\"\"Initialize new model repository\"\"\"\n        repo = Repository(\n            id=uuid.uuid4(),\n            created_at=datetime.now(UTC),\n            default_branch='main',\n            config=RepositoryConfig()\n        )\n        \n        # Create directory structure\n        self.object_store.init_store()\n        self.refs.create_branch('main')\n        self.index.init_index()\n        \n        return repo\n```\n\n### Content-Addressable Storage\n```python\nclass ObjectStore:\n    \"\"\"Store model objects using content hashing\"\"\"\n    \n    def hash_object(self, obj: FreeCADObject) -> str:\n        \"\"\"Generate SHA-256 hash of object content\"\"\"\n        serialized = self.serialize_object(obj)\n        return hashlib.sha256(serialized).hexdigest()\n    \n    async def store_object(self, obj: FreeCADObject) -> str:\n        \"\"\"Store object and return hash\"\"\"\n        obj_hash = self.hash_object(obj)\n        \n        # Store in content-addressable format\n        path = self.get_object_path(obj_hash)\n        if not path.exists():\n            compressed = self.compress_object(obj)\n            await self.write_object(path, compressed)\n        \n        return obj_hash\n    \n    def serialize_object(self, obj: FreeCADObject) -> bytes:\n        \"\"\"Serialize FreeCAD object deterministically\"\"\"\n        data = {\n            'type': obj.TypeId,\n            'name': obj.Name,\n            'label': obj.Label,\n            'properties': self.extract_properties(obj),\n            'placement': self.serialize_placement(obj.Placement),\n            'shape': self.serialize_shape(obj.Shape) if hasattr(obj, 'Shape') else None,\n            'expressions': self.extract_expressions(obj)\n        }\n        \n        # Deterministic JSON serialization\n        return json.dumps(data, sort_keys=True, cls=FreeCADEncoder).encode()\n```\n\n### Commit System\n```python\nclass CommitManager:\n    def create_commit(\n        self,\n        tree_hash: str,\n        parent_hashes: List[str],\n        author: str,\n        message: str,\n        metadata: dict = None\n    ) -> Commit:\n        \"\"\"Create new commit object\"\"\"\n        commit = Commit(\n            id=uuid.uuid4(),\n            tree=tree_hash,\n            parents=parent_hashes,\n            author=author,\n            timestamp=datetime.now(UTC),\n            message=message,\n            metadata=metadata or {}\n        )\n        \n        # Calculate commit hash\n        commit.hash = self.hash_commit(commit)\n        \n        return commit\n    \n    async def commit_changes(\n        self,\n        doc: FreeCAD.Document,\n        message: str,\n        author: str\n    ) -> str:\n        \"\"\"Commit current document state\"\"\"\n        # Build tree from document\n        tree = await self.build_tree(doc)\n        tree_hash = await self.object_store.store_tree(tree)\n        \n        # Get parent commit\n        current_branch = self.refs.get_current_branch()\n        parent_hash = self.refs.get_branch_head(current_branch)\n        \n        # Create commit\n        commit = self.create_commit(\n            tree_hash=tree_hash,\n            parent_hashes=[parent_hash] if parent_hash else [],\n            author=author,\n            message=message\n        )\n        \n        # Store commit\n        commit_hash = await self.object_store.store_commit(commit)\n        \n        # Update branch reference\n        self.refs.update_branch(current_branch, commit_hash)\n        \n        return commit_hash\n```\n\n### Branching and Merging\n```python\nclass BranchManager:\n    async def create_branch(\n        self,\n        branch_name: str,\n        from_commit: str = None\n    ) -> Branch:\n        \"\"\"Create new branch from commit or current HEAD\"\"\"\n        if from_commit is None:\n            from_commit = self.refs.get_head()\n        \n        branch = Branch(\n            name=branch_name,\n            head=from_commit,\n            created_at=datetime.now(UTC)\n        )\n        \n        self.refs.create_branch_ref(branch_name, from_commit)\n        return branch\n    \n    async def merge_branches(\n        self,\n        source_branch: str,\n        target_branch: str,\n        strategy: str = 'recursive'\n    ) -> MergeResult:\n        \"\"\"Merge source branch into target branch\"\"\"\n        # Find common ancestor\n        source_head = self.refs.get_branch_head(source_branch)\n        target_head = self.refs.get_branch_head(target_branch)\n        common_ancestor = await self.find_common_ancestor(source_head, target_head)\n        \n        if common_ancestor == source_head:\n            # Fast-forward merge possible\n            return await self.fast_forward_merge(target_branch, source_head)\n        \n        # Three-way merge\n        source_tree = await self.get_tree(source_head)\n        target_tree = await self.get_tree(target_head)\n        base_tree = await self.get_tree(common_ancestor)\n        \n        # Merge trees\n        merged_tree, conflicts = await self.merge_trees(\n            base_tree, source_tree, target_tree, strategy\n        )\n        \n        if conflicts:\n            return MergeResult(\n                success=False,\n                conflicts=conflicts,\n                merged_tree=merged_tree\n            )\n        \n        # Create merge commit\n        merge_commit = await self.create_merge_commit(\n            merged_tree,\n            [target_head, source_head],\n            f\"Merge branch '{source_branch}' into '{target_branch}'\"\n        )\n        \n        return MergeResult(\n            success=True,\n            commit_hash=merge_commit,\n            merged_tree=merged_tree\n        )\n```\n\n### Model Diffing\n```python\nclass ModelDiffer:\n    def diff_objects(\n        self,\n        obj1: FreeCADObject,\n        obj2: FreeCADObject\n    ) -> ObjectDiff:\n        \"\"\"Calculate differences between two objects\"\"\"\n        diff = ObjectDiff(\n            object_id=obj1.Name,\n            changes=[]\n        )\n        \n        # Property changes\n        props1 = self.extract_properties(obj1)\n        props2 = self.extract_properties(obj2)\n        \n        for prop_name in set(props1.keys()) | set(props2.keys()):\n            val1 = props1.get(prop_name)\n            val2 = props2.get(prop_name)\n            \n            if val1 != val2:\n                diff.changes.append(PropertyChange(\n                    property=prop_name,\n                    old_value=val1,\n                    new_value=val2,\n                    change_type=self.classify_change(val1, val2)\n                ))\n        \n        # Geometry changes\n        if hasattr(obj1, 'Shape') and hasattr(obj2, 'Shape'):\n            shape_diff = self.diff_shapes(obj1.Shape, obj2.Shape)\n            if shape_diff:\n                diff.changes.append(shape_diff)\n        \n        return diff\n    \n    def diff_shapes(\n        self,\n        shape1: Part.Shape,\n        shape2: Part.Shape\n    ) -> Optional[ShapeDiff]:\n        \"\"\"Calculate geometric differences\"\"\"\n        # Volume change\n        vol1 = shape1.Volume\n        vol2 = shape2.Volume\n        volume_change = (vol2 - vol1) / vol1 if vol1 > 0 else None\n        \n        # Surface area change\n        area1 = shape1.Area\n        area2 = shape2.Area\n        area_change = (area2 - area1) / area1 if area1 > 0 else None\n        \n        # Topology changes\n        topo_changes = self.diff_topology(shape1, shape2)\n        \n        return ShapeDiff(\n            volume_change=volume_change,\n            area_change=area_change,\n            topology_changes=topo_changes\n        )\n```\n\n### Conflict Resolution\n```python\nclass ConflictResolver:\n    async def resolve_model_conflict(\n        self,\n        base: FreeCADObject,\n        ours: FreeCADObject,\n        theirs: FreeCADObject,\n        strategy: str = 'interactive'\n    ) -> ResolvedObject:\n        \"\"\"Resolve conflicts between model versions\"\"\"\n        \n        if strategy == 'ours':\n            return ResolvedObject(ours, resolution_type='keep_ours')\n        elif strategy == 'theirs':\n            return ResolvedObject(theirs, resolution_type='keep_theirs')\n        elif strategy == 'auto':\n            # Try automatic resolution\n            merged = await self.auto_merge_objects(base, ours, theirs)\n            if merged:\n                return ResolvedObject(merged, resolution_type='auto_merged')\n        \n        # Interactive resolution required\n        conflict_info = self.analyze_conflict(base, ours, theirs)\n        return ResolvedObject(\n            None,\n            resolution_type='manual_required',\n            conflict_info=conflict_info\n        )\n```\n\n### History and Rollback\n```python\nclass HistoryManager:\n    async def get_history(\n        self,\n        branch: str = None,\n        limit: int = 100\n    ) -> List[CommitInfo]:\n        \"\"\"Get commit history for branch\"\"\"\n        current = self.refs.get_branch_head(branch or 'main')\n        history = []\n        \n        while current and len(history) < limit:\n            commit = await self.object_store.get_commit(current)\n            history.append(CommitInfo(\n                hash=commit.hash,\n                author=commit.author,\n                timestamp=commit.timestamp,\n                message=commit.message,\n                parents=commit.parents\n            ))\n            \n            # Move to parent\n            current = commit.parents[0] if commit.parents else None\n        \n        return history\n    \n    async def checkout_commit(\n        self,\n        commit_hash: str,\n        create_branch: bool = False\n    ) -> FreeCAD.Document:\n        \"\"\"Checkout specific commit version\"\"\"\n        # Get commit and tree\n        commit = await self.object_store.get_commit(commit_hash)\n        tree = await self.object_store.get_tree(commit.tree)\n        \n        # Reconstruct document from tree\n        doc = await self.reconstruct_document(tree)\n        \n        if create_branch:\n            branch_name = f\"detached-{commit_hash[:8]}\"\n            await self.create_branch(branch_name, commit_hash)\n        \n        return doc\n    \n    async def rollback_to_commit(\n        self,\n        commit_hash: str,\n        branch: str = None\n    ):\n        \"\"\"Rollback branch to specific commit\"\"\"\n        branch = branch or self.refs.get_current_branch()\n        \n        # Verify commit exists\n        commit = await self.object_store.get_commit(commit_hash)\n        \n        # Update branch reference\n        self.refs.update_branch(branch, commit_hash)\n        \n        # Clear working directory\n        await self.clear_working_directory()\n        \n        # Checkout commit\n        return await self.checkout_commit(commit_hash)\n```\n\n### Storage Optimization\n```python\nclass StorageOptimizer:\n    async def pack_repository(self):\n        \"\"\"Optimize repository storage\"\"\"\n        # Delta compression between similar objects\n        await self.delta_compress_objects()\n        \n        # Remove unreachable objects\n        await self.garbage_collect()\n        \n        # Compress pack files\n        await self.compress_packs()\n    \n    async def delta_compress_objects(self):\n        \"\"\"Apply delta compression to similar objects\"\"\"\n        objects = await self.object_store.list_objects()\n        \n        # Group similar objects\n        groups = self.group_similar_objects(objects)\n        \n        for group in groups:\n            # Find best base object\n            base = self.find_optimal_base(group)\n            \n            # Create deltas for other objects\n            for obj in group:\n                if obj != base:\n                    delta = self.create_delta(base, obj)\n                    await self.store_delta(delta)\n```\n\n### Turkish Localization\n```python\nVERSION_CONTROL_TR = {\n    'init_repo': 'Depo başlatılıyor...',\n    'commit_created': 'Değişiklikler kaydedildi: {hash}',\n    'branch_created': 'Dal oluşturuldu: {name}',\n    'merge_success': 'Birleştirme başarılı',\n    'merge_conflict': 'Birleştirme çakışması tespit edildi',\n    'checkout': '{ref} dalına geçildi',\n    'rollback': '{commit} sürümüne geri dönüldü',\n    'history': 'Değişiklik geçmişi'\n}\n```\n\n### Integration with Existing System\n- Uses MinIO for object storage\n- PostgreSQL for metadata and refs\n- Redis for caching frequently accessed objects\n- WebSocket for real-time collaboration sync",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 23,
            "title": "Batch Processing and Automation Framework",
            "description": "Implement comprehensive batch processing capabilities for multiple models with parallel execution, workflow automation, and scheduled operations",
            "details": "## Batch Processing and Automation Framework\n\n### Batch Processing Engine\n```python\nclass BatchProcessingEngine:\n    \"\"\"Enterprise-grade batch processing for FreeCAD models\"\"\"\n    \n    def __init__(self):\n        self.executor = ProcessPoolExecutor(max_workers=cpu_count())\n        self.job_queue = asyncio.Queue()\n        self.result_aggregator = ResultAggregator()\n        self.progress_tracker = ProgressTracker()\n    \n    async def process_batch(\n        self,\n        items: List[BatchItem],\n        operation: BatchOperation,\n        options: BatchOptions = None\n    ) -> BatchResult:\n        \"\"\"Process multiple items in parallel with progress tracking\"\"\"\n        \n        batch_id = uuid.uuid4()\n        total_items = len(items)\n        \n        # Initialize progress tracking\n        await self.progress_tracker.init_batch(batch_id, total_items)\n        \n        # Partition items for optimal processing\n        partitions = self.partition_items(items, options)\n        \n        # Process partitions in parallel\n        tasks = []\n        for partition in partitions:\n            task = asyncio.create_task(\n                self.process_partition(partition, operation, batch_id)\n            )\n            tasks.append(task)\n        \n        # Gather results\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Aggregate results\n        batch_result = await self.result_aggregator.aggregate(\n            results, batch_id, options\n        )\n        \n        return batch_result\n```\n\n### Workflow Automation\n```python\nclass WorkflowAutomation:\n    \"\"\"Define and execute complex multi-step workflows\"\"\"\n    \n    def __init__(self):\n        self.workflow_engine = WorkflowEngine()\n        self.step_executor = StepExecutor()\n        self.condition_evaluator = ConditionEvaluator()\n    \n    async def define_workflow(\n        self,\n        name: str,\n        steps: List[WorkflowStep]\n    ) -> Workflow:\n        \"\"\"Define reusable workflow\"\"\"\n        workflow = Workflow(\n            id=uuid.uuid4(),\n            name=name,\n            steps=steps,\n            created_at=datetime.now(UTC)\n        )\n        \n        # Validate workflow DAG\n        if not self.validate_workflow_dag(workflow):\n            raise WorkflowValidationError(\"Workflow contains cycles\")\n        \n        # Store workflow definition\n        await self.store_workflow(workflow)\n        \n        return workflow\n    \n    async def execute_workflow(\n        self,\n        workflow_id: str,\n        input_data: dict,\n        execution_options: ExecutionOptions = None\n    ) -> WorkflowExecution:\n        \"\"\"Execute workflow with input data\"\"\"\n        \n        workflow = await self.load_workflow(workflow_id)\n        execution = WorkflowExecution(\n            id=uuid.uuid4(),\n            workflow_id=workflow_id,\n            status='running',\n            started_at=datetime.now(UTC)\n        )\n        \n        # Execute steps in dependency order\n        for step in self.topological_sort(workflow.steps):\n            # Check conditions\n            if step.conditions:\n                if not await self.evaluate_conditions(step.conditions, execution):\n                    continue\n            \n            # Execute step\n            try:\n                result = await self.step_executor.execute(\n                    step, \n                    self.get_step_input(step, execution),\n                    execution_options\n                )\n                \n                execution.step_results[step.id] = result\n                \n                # Handle branching\n                if step.type == 'branch':\n                    next_steps = self.determine_branch(result, step.branches)\n                    await self.queue_steps(next_steps, execution)\n                \n            except StepExecutionError as e:\n                if step.error_handling == 'retry':\n                    await self.retry_step(step, execution, e)\n                elif step.error_handling == 'skip':\n                    continue\n                else:\n                    raise\n        \n        execution.status = 'completed'\n        execution.completed_at = datetime.now(UTC)\n        \n        return execution\n```\n\n### Batch Operations Library\n```python\nclass BatchOperations:\n    \"\"\"Pre-defined batch operations for common tasks\"\"\"\n    \n    @staticmethod\n    async def batch_convert_format(\n        models: List[Path],\n        target_format: str,\n        options: ConversionOptions = None\n    ) -> List[ConversionResult]:\n        \"\"\"Convert multiple models to target format\"\"\"\n        results = []\n        \n        async with asyncio.TaskGroup() as tg:\n            for model_path in models:\n                task = tg.create_task(\n                    convert_model(model_path, target_format, options)\n                )\n                results.append(task)\n        \n        return [await r for r in results]\n    \n    @staticmethod\n    async def batch_generate_variants(\n        base_model: FreeCAD.Document,\n        parameter_sets: List[ParameterSet]\n    ) -> List[ModelVariant]:\n        \"\"\"Generate model variants with different parameters\"\"\"\n        variants = []\n        \n        for params in parameter_sets:\n            variant_doc = base_model.copyDocument()\n            \n            # Apply parameters\n            for param_name, param_value in params.items():\n                obj = variant_doc.getObject(param_name.split('.')[0])\n                prop = param_name.split('.')[1]\n                setattr(obj, prop, param_value)\n            \n            # Recompute\n            variant_doc.recompute()\n            \n            variants.append(ModelVariant(\n                id=uuid.uuid4(),\n                parameters=params,\n                document=variant_doc\n            ))\n        \n        return variants\n    \n    @staticmethod\n    async def batch_quality_check(\n        models: List[FreeCAD.Document],\n        checks: List[QualityCheck]\n    ) -> List[QualityReport]:\n        \"\"\"Run quality checks on multiple models\"\"\"\n        reports = []\n        \n        for model in models:\n            report = QualityReport(model_id=model.Name)\n            \n            for check in checks:\n                result = await check.execute(model)\n                report.add_check_result(check.name, result)\n            \n            reports.append(report)\n        \n        return reports\n```\n\n### Scheduled Operations\n```python\nclass ScheduledOperations:\n    \"\"\"Schedule and manage recurring batch operations\"\"\"\n    \n    def __init__(self):\n        self.scheduler = AsyncIOScheduler()\n        self.job_store = SQLAlchemyJobStore(url=settings.database_url)\n        self.scheduler.add_jobstore(self.job_store)\n    \n    def schedule_batch_job(\n        self,\n        job_func: Callable,\n        trigger: str,\n        job_id: str = None,\n        **trigger_args\n    ) -> str:\n        \"\"\"Schedule recurring batch job\"\"\"\n        \n        job = self.scheduler.add_job(\n            func=job_func,\n            trigger=trigger,\n            id=job_id or str(uuid.uuid4()),\n            replace_existing=True,\n            **trigger_args\n        )\n        \n        return job.id\n    \n    def schedule_nightly_optimization(self):\n        \"\"\"Schedule nightly model optimization\"\"\"\n        self.schedule_batch_job(\n            job_func=self.optimize_all_models,\n            trigger='cron',\n            hour=2,\n            minute=0,\n            job_id='nightly_optimization'\n        )\n    \n    async def optimize_all_models(self):\n        \"\"\"Optimize all models in repository\"\"\"\n        models = await self.get_all_models()\n        \n        for model in models:\n            # Mesh optimization\n            await self.optimize_mesh(model)\n            \n            # Remove unused features\n            await self.cleanup_features(model)\n            \n            # Compress storage\n            await self.compress_model(model)\n```\n\n### Parallel Execution Framework\n```python\nclass ParallelExecutor:\n    \"\"\"Execute operations in parallel with resource management\"\"\"\n    \n    def __init__(self):\n        self.worker_pool = WorkerPool(max_workers=settings.batch_max_workers)\n        self.resource_manager = ResourceManager()\n        self.load_balancer = LoadBalancer()\n    \n    async def execute_parallel(\n        self,\n        tasks: List[Task],\n        strategy: str = 'balanced'\n    ) -> List[TaskResult]:\n        \"\"\"Execute tasks in parallel with load balancing\"\"\"\n        \n        # Acquire resources\n        resources = await self.resource_manager.acquire_batch_resources(\n            len(tasks)\n        )\n        \n        # Distribute tasks to workers\n        task_assignments = self.load_balancer.distribute_tasks(\n            tasks, \n            self.worker_pool.available_workers,\n            strategy\n        )\n        \n        # Execute on workers\n        results = []\n        for worker_id, assigned_tasks in task_assignments.items():\n            worker_results = await self.worker_pool.execute_on_worker(\n                worker_id,\n                assigned_tasks,\n                resources[worker_id]\n            )\n            results.extend(worker_results)\n        \n        # Release resources\n        await self.resource_manager.release_resources(resources)\n        \n        return results\n```\n\n### Batch Monitoring and Reporting\n```python\nclass BatchMonitor:\n    \"\"\"Monitor batch operations and generate reports\"\"\"\n    \n    def __init__(self):\n        self.metrics_collector = MetricsCollector()\n        self.report_generator = ReportGenerator()\n    \n    async def monitor_batch(\n        self,\n        batch_id: str,\n        callback: Optional[Callable] = None\n    ):\n        \"\"\"Monitor batch execution with real-time updates\"\"\"\n        \n        while True:\n            status = await self.get_batch_status(batch_id)\n            \n            # Collect metrics\n            metrics = await self.metrics_collector.collect(batch_id)\n            \n            # Update progress\n            progress = BatchProgress(\n                batch_id=batch_id,\n                total=status.total_items,\n                completed=status.completed_items,\n                failed=status.failed_items,\n                elapsed_time=status.elapsed_time,\n                estimated_remaining=self.estimate_remaining_time(status),\n                metrics=metrics\n            )\n            \n            # Callback for real-time updates\n            if callback:\n                await callback(progress)\n            \n            # Check if complete\n            if status.is_complete:\n                break\n            \n            await asyncio.sleep(1)\n        \n        # Generate final report\n        report = await self.report_generator.generate_batch_report(batch_id)\n        return report\n```\n\n### Error Recovery and Retry\n```python\nclass BatchErrorRecovery:\n    \"\"\"Handle errors and retries in batch processing\"\"\"\n    \n    def __init__(self):\n        self.retry_policy = ExponentialBackoffRetry()\n        self.error_handler = ErrorHandler()\n        self.checkpoint_manager = CheckpointManager()\n    \n    async def process_with_recovery(\n        self,\n        batch: Batch,\n        operation: Operation\n    ) -> BatchResult:\n        \"\"\"Process batch with automatic error recovery\"\"\"\n        \n        # Load checkpoint if exists\n        checkpoint = await self.checkpoint_manager.load_checkpoint(batch.id)\n        start_index = checkpoint.last_processed if checkpoint else 0\n        \n        results = []\n        for i, item in enumerate(batch.items[start_index:], start=start_index):\n            try:\n                result = await operation.execute(item)\n                results.append(result)\n                \n                # Save checkpoint periodically\n                if i % 10 == 0:\n                    await self.checkpoint_manager.save_checkpoint(\n                        batch.id, i, results\n                    )\n                \n            except Exception as e:\n                # Attempt retry with backoff\n                retry_result = await self.retry_with_backoff(\n                    operation, item, e\n                )\n                \n                if retry_result.success:\n                    results.append(retry_result.value)\n                else:\n                    # Handle permanent failure\n                    await self.error_handler.handle_failure(\n                        batch.id, item, e\n                    )\n                    results.append(FailedResult(item, e))\n        \n        return BatchResult(results)\n```\n\n### Template-Based Automation\n```python\nclass TemplateAutomation:\n    \"\"\"Automate operations using templates\"\"\"\n    \n    def __init__(self):\n        self.template_engine = TemplateEngine()\n        self.parameter_resolver = ParameterResolver()\n    \n    async def execute_template(\n        self,\n        template_id: str,\n        parameters: dict\n    ) -> TemplateExecutionResult:\n        \"\"\"Execute automation template with parameters\"\"\"\n        \n        template = await self.load_template(template_id)\n        \n        # Resolve parameters\n        resolved_params = await self.parameter_resolver.resolve(\n            template.parameters,\n            parameters\n        )\n        \n        # Execute template steps\n        results = []\n        for step in template.steps:\n            step_params = self.interpolate_parameters(\n                step.parameters,\n                resolved_params\n            )\n            \n            result = await self.execute_step(step, step_params)\n            results.append(result)\n            \n            # Update context for next steps\n            resolved_params.update(result.outputs)\n        \n        return TemplateExecutionResult(\n            template_id=template_id,\n            results=results,\n            outputs=resolved_params\n        )\n```\n\n### Turkish Localization\n```python\nBATCH_MESSAGES_TR = {\n    'batch_started': 'Toplu işlem başlatıldı: {count} öğe',\n    'processing': 'İşleniyor: {current}/{total}',\n    'batch_complete': 'Toplu işlem tamamlandı',\n    'items_processed': '{count} öğe işlendi',\n    'items_failed': '{count} öğe başarısız',\n    'estimated_time': 'Tahmini süre: {time}',\n    'workflow_executing': 'İş akışı yürütülüyor: {name}',\n    'scheduled_job': 'Zamanlanmış görev: {job_id}'\n}\n```\n\n### Performance Optimization\n- Worker pool with configurable size\n- Intelligent load balancing\n- Memory-efficient streaming for large batches\n- Checkpoint/resume for long-running operations\n- Resource pooling and reuse\n\n### Integration Points\n- Celery for distributed task execution\n- Redis for job queuing and caching\n- PostgreSQL for job persistence\n- MinIO for batch results storage\n- RabbitMQ for event streaming",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 24,
            "title": "Advanced Model Validation and Quality Assurance",
            "description": "Implement comprehensive model validation, quality checks, and certification system for FreeCAD models with manufacturing compliance and standards verification",
            "details": "## Advanced Model Validation and Quality Assurance\n\n### Model Validation Framework\n```python\nclass ModelValidationFramework:\n    \"\"\"Comprehensive validation system for FreeCAD models\"\"\"\n    \n    def __init__(self):\n        self.validators = ValidatorRegistry()\n        self.rule_engine = RuleEngine()\n        self.standards_checker = StandardsChecker()\n        self.report_generator = ValidationReportGenerator()\n    \n    async def validate_model(\n        self,\n        doc: FreeCAD.Document,\n        validation_profile: str = 'standard',\n        standards: List[str] = None\n    ) -> ValidationResult:\n        \"\"\"Execute comprehensive model validation\"\"\"\n        \n        result = ValidationResult(\n            model_id=doc.Name,\n            timestamp=datetime.now(UTC),\n            profile=validation_profile\n        )\n        \n        # Structural validation\n        structural = await self.validate_structure(doc)\n        result.add_section('structural', structural)\n        \n        # Geometric validation\n        geometric = await self.validate_geometry(doc)\n        result.add_section('geometric', geometric)\n        \n        # Manufacturing validation\n        manufacturing = await self.validate_manufacturability(doc)\n        result.add_section('manufacturing', manufacturing)\n        \n        # Standards compliance\n        if standards:\n            compliance = await self.validate_standards(doc, standards)\n            result.add_section('standards', compliance)\n        \n        # Calculate overall score\n        result.calculate_score()\n        \n        return result\n```\n\n### Geometric Validation\n```python\nclass GeometricValidator:\n    \"\"\"Validate geometric properties and constraints\"\"\"\n    \n    async def validate_geometry(\n        self,\n        shape: Part.Shape,\n        tolerances: GeometricTolerances = None\n    ) -> GeometricValidation:\n        \"\"\"Comprehensive geometric validation\"\"\"\n        \n        validation = GeometricValidation()\n        \n        # Check for self-intersections\n        if shape.hasSelfIntersections():\n            validation.add_error(\n                \"self_intersection\",\n                \"Shape contains self-intersections\",\n                severity=\"critical\"\n            )\n        \n        # Validate topology\n        topology_check = self.check_topology(shape)\n        if not topology_check.is_valid:\n            validation.add_errors(topology_check.errors)\n        \n        # Check for thin walls\n        thin_walls = await self.detect_thin_walls(\n            shape,\n            min_thickness=tolerances.min_wall_thickness if tolerances else 1.0\n        )\n        if thin_walls:\n            validation.add_warning(\n                \"thin_walls\",\n                f\"Found {len(thin_walls)} thin wall sections\",\n                details=thin_walls\n            )\n        \n        # Validate holes and features\n        features = await self.validate_features(shape, tolerances)\n        validation.add_results(features)\n        \n        # Check surface quality\n        surface_quality = await self.check_surface_quality(shape)\n        validation.add_metric(\"surface_quality\", surface_quality)\n        \n        return validation\n    \n    def check_topology(self, shape: Part.Shape) -> TopologyCheck:\n        \"\"\"Check topological validity\"\"\"\n        check = TopologyCheck()\n        \n        # Check for non-manifold edges\n        non_manifold = self.find_non_manifold_edges(shape)\n        if non_manifold:\n            check.add_error(\"non_manifold_edges\", non_manifold)\n        \n        # Check for open edges in solids\n        if shape.ShapeType == 'Solid':\n            open_edges = self.find_open_edges(shape)\n            if open_edges:\n                check.add_error(\"open_edges_in_solid\", open_edges)\n        \n        # Check face normals consistency\n        inconsistent_normals = self.check_face_normals(shape)\n        if inconsistent_normals:\n            check.add_warning(\"inconsistent_normals\", inconsistent_normals)\n        \n        return check\n    \n    async def detect_thin_walls(\n        self,\n        shape: Part.Shape,\n        min_thickness: float\n    ) -> List[ThinWallSection]:\n        \"\"\"Detect walls thinner than minimum\"\"\"\n        thin_sections = []\n        \n        # Analyze wall thickness using ray casting\n        analyzer = WallThicknessAnalyzer()\n        thickness_map = await analyzer.analyze(shape)\n        \n        for location, thickness in thickness_map.items():\n            if thickness < min_thickness:\n                thin_sections.append(ThinWallSection(\n                    location=location,\n                    thickness=thickness,\n                    min_required=min_thickness\n                ))\n        \n        return thin_sections\n```\n\n### Manufacturing Validation\n```python\nclass ManufacturingValidator:\n    \"\"\"Validate manufacturability for different processes\"\"\"\n    \n    async def validate_for_cnc(\n        self,\n        doc: FreeCAD.Document,\n        machine_spec: MachineSpecification\n    ) -> CNCValidation:\n        \"\"\"Validate model for CNC machining\"\"\"\n        \n        validation = CNCValidation()\n        \n        # Check tool accessibility\n        accessibility = await self.check_tool_accessibility(\n            doc,\n            machine_spec.tool_library\n        )\n        validation.add_results(\"tool_access\", accessibility)\n        \n        # Validate undercuts\n        undercuts = self.detect_undercuts(doc, machine_spec.axes)\n        if undercuts:\n            validation.add_warning(\"undercuts\", undercuts)\n        \n        # Check minimum feature sizes\n        features = self.check_feature_sizes(\n            doc,\n            machine_spec.min_feature_size\n        )\n        validation.add_results(\"feature_sizes\", features)\n        \n        # Validate tolerances\n        tolerance_check = self.validate_tolerances(\n            doc,\n            machine_spec.achievable_tolerances\n        )\n        validation.add_results(\"tolerances\", tolerance_check)\n        \n        return validation\n    \n    async def validate_for_3d_printing(\n        self,\n        shape: Part.Shape,\n        printer_spec: PrinterSpecification\n    ) -> PrintValidation:\n        \"\"\"Validate model for 3D printing\"\"\"\n        \n        validation = PrintValidation()\n        \n        # Check printability\n        printability = self.check_printability(shape)\n        validation.add_metric(\"printability_score\", printability.score)\n        \n        # Detect overhangs\n        overhangs = await self.detect_overhangs(\n            shape,\n            max_angle=printer_spec.max_overhang_angle\n        )\n        if overhangs:\n            validation.add_info(\"overhangs\", f\"{len(overhangs)} overhangs detected\")\n            validation.support_required = True\n        \n        # Check for trapped volumes\n        trapped = self.find_trapped_volumes(shape)\n        if trapped:\n            validation.add_warning(\"trapped_volumes\", trapped)\n        \n        # Estimate support material\n        if validation.support_required:\n            support_volume = await self.estimate_support_volume(\n                shape,\n                overhangs,\n                printer_spec\n            )\n            validation.add_metric(\"support_volume\", support_volume)\n        \n        return validation\n```\n\n### Standards Compliance Checker\n```python\nclass StandardsChecker:\n    \"\"\"Check compliance with industry standards\"\"\"\n    \n    SUPPORTED_STANDARDS = {\n        'ISO 10303': ISO10303Checker,  # STEP standard\n        'ISO 14040': ISO14040Checker,  # Environmental\n        'ASME Y14.5': ASMEY145Checker,  # GD&T\n        'DIN 8580': DIN8580Checker,     # Manufacturing\n        'ISO 9001': ISO9001Checker,     # Quality\n        'CE': CEMarkingChecker,         # European conformity\n    }\n    \n    async def check_compliance(\n        self,\n        doc: FreeCAD.Document,\n        standard: str\n    ) -> ComplianceResult:\n        \"\"\"Check model compliance with specific standard\"\"\"\n        \n        if standard not in self.SUPPORTED_STANDARDS:\n            raise ValueError(f\"Unsupported standard: {standard}\")\n        \n        checker = self.SUPPORTED_STANDARDS[standard]()\n        result = await checker.check(doc)\n        \n        return ComplianceResult(\n            standard=standard,\n            compliant=result.is_compliant,\n            violations=result.violations,\n            recommendations=result.recommendations,\n            certificate=result.generate_certificate() if result.is_compliant else None\n        )\n```\n\n### Quality Metrics System\n```python\nclass QualityMetrics:\n    \"\"\"Calculate quality metrics for models\"\"\"\n    \n    async def calculate_metrics(\n        self,\n        doc: FreeCAD.Document\n    ) -> QualityMetricsReport:\n        \"\"\"Calculate comprehensive quality metrics\"\"\"\n        \n        report = QualityMetricsReport()\n        \n        # Geometric complexity\n        complexity = self.calculate_geometric_complexity(doc)\n        report.add_metric(\"geometric_complexity\", complexity)\n        \n        # Surface quality\n        surface_quality = await self.analyze_surface_quality(doc)\n        report.add_metric(\"surface_quality\", surface_quality)\n        \n        # Feature consistency\n        consistency = self.check_feature_consistency(doc)\n        report.add_metric(\"feature_consistency\", consistency)\n        \n        # Parametric robustness\n        robustness = await self.test_parametric_robustness(doc)\n        report.add_metric(\"parametric_robustness\", robustness)\n        \n        # Assembly compatibility\n        if self.has_assembly_features(doc):\n            assembly_score = await self.check_assembly_compatibility(doc)\n            report.add_metric(\"assembly_compatibility\", assembly_score)\n        \n        # Calculate overall quality score\n        report.calculate_overall_score()\n        \n        return report\n    \n    def calculate_geometric_complexity(\n        self,\n        doc: FreeCAD.Document\n    ) -> ComplexityScore:\n        \"\"\"Calculate model complexity score\"\"\"\n        \n        score = ComplexityScore()\n        \n        for obj in doc.Objects:\n            if hasattr(obj, 'Shape'):\n                # Face count\n                score.face_count += len(obj.Shape.Faces)\n                \n                # Edge count\n                score.edge_count += len(obj.Shape.Edges)\n                \n                # Feature count\n                if hasattr(obj, 'Features'):\n                    score.feature_count += len(obj.Features)\n                \n                # Calculate complexity index\n                score.complexity_index = (\n                    score.face_count * 0.3 +\n                    score.edge_count * 0.2 +\n                    score.feature_count * 0.5\n                )\n        \n        return score\n```\n\n### Certification System\n```python\nclass CertificationSystem:\n    \"\"\"Issue quality certificates for validated models\"\"\"\n    \n    def issue_certificate(\n        self,\n        validation_result: ValidationResult,\n        standards: List[str],\n        issuer: str\n    ) -> QualityCertificate:\n        \"\"\"Issue quality certificate for model\"\"\"\n        \n        if validation_result.overall_score < 0.8:\n            raise ValueError(\"Model does not meet certification threshold\")\n        \n        certificate = QualityCertificate(\n            id=uuid.uuid4(),\n            model_id=validation_result.model_id,\n            issued_date=datetime.now(UTC),\n            issuer=issuer,\n            standards=standards,\n            validation_score=validation_result.overall_score,\n            expiry_date=datetime.now(UTC) + timedelta(days=365)\n        )\n        \n        # Generate cryptographic signature\n        certificate.signature = self.sign_certificate(certificate)\n        \n        # Store in blockchain for immutability (optional)\n        if settings.blockchain_enabled:\n            certificate.blockchain_hash = self.store_on_blockchain(certificate)\n        \n        return certificate\n    \n    def verify_certificate(\n        self,\n        certificate: QualityCertificate\n    ) -> bool:\n        \"\"\"Verify certificate authenticity\"\"\"\n        \n        # Verify signature\n        if not self.verify_signature(certificate):\n            return False\n        \n        # Check expiry\n        if certificate.expiry_date < datetime.now(UTC):\n            return False\n        \n        # Verify blockchain record if applicable\n        if certificate.blockchain_hash:\n            if not self.verify_blockchain_record(certificate):\n                return False\n        \n        return True\n```\n\n### Automated Fixing Suggestions\n```python\nclass AutoFixSuggestions:\n    \"\"\"Generate automated fixing suggestions for validation issues\"\"\"\n    \n    async def suggest_fixes(\n        self,\n        validation_result: ValidationResult\n    ) -> List[FixSuggestion]:\n        \"\"\"Generate fix suggestions for validation issues\"\"\"\n        \n        suggestions = []\n        \n        for issue in validation_result.get_issues():\n            if issue.type == \"self_intersection\":\n                suggestion = await self.suggest_intersection_fix(issue)\n            elif issue.type == \"thin_walls\":\n                suggestion = await self.suggest_wall_thickening(issue)\n            elif issue.type == \"non_manifold\":\n                suggestion = await self.suggest_topology_fix(issue)\n            elif issue.type == \"overhang\":\n                suggestion = await self.suggest_support_generation(issue)\n            else:\n                suggestion = await self.suggest_generic_fix(issue)\n            \n            if suggestion:\n                suggestions.append(suggestion)\n        \n        return suggestions\n    \n    async def apply_automated_fixes(\n        self,\n        doc: FreeCAD.Document,\n        suggestions: List[FixSuggestion],\n        auto_approve: bool = False\n    ) -> FixReport:\n        \"\"\"Apply automated fixes to model\"\"\"\n        \n        report = FixReport()\n        \n        for suggestion in suggestions:\n            if auto_approve or suggestion.confidence > 0.9:\n                try:\n                    # Apply fix\n                    result = await suggestion.apply(doc)\n                    report.add_success(suggestion, result)\n                    \n                    # Recompute document\n                    doc.recompute()\n                    \n                except Exception as e:\n                    report.add_failure(suggestion, e)\n            else:\n                report.add_skipped(suggestion, \"Manual approval required\")\n        \n        return report\n```\n\n### Performance Testing\n```python\nclass PerformanceValidator:\n    \"\"\"Validate model performance characteristics\"\"\"\n    \n    async def validate_performance(\n        self,\n        doc: FreeCAD.Document,\n        requirements: PerformanceRequirements\n    ) -> PerformanceValidation:\n        \"\"\"Validate model meets performance requirements\"\"\"\n        \n        validation = PerformanceValidation()\n        \n        # Recompute time\n        recompute_time = await self.measure_recompute_time(doc)\n        if recompute_time > requirements.max_recompute_time:\n            validation.add_warning(\n                \"slow_recompute\",\n                f\"Recompute time {recompute_time}s exceeds limit\"\n            )\n        \n        # Memory usage\n        memory_usage = self.measure_memory_usage(doc)\n        if memory_usage > requirements.max_memory:\n            validation.add_warning(\n                \"high_memory\",\n                f\"Memory usage {memory_usage}MB exceeds limit\"\n            )\n        \n        # File size\n        file_size = await self.calculate_file_size(doc)\n        if file_size > requirements.max_file_size:\n            validation.add_info(\n                \"large_file\",\n                f\"File size {file_size}MB exceeds recommendation\"\n            )\n        \n        return validation\n```\n\n### Turkish Localization\n```python\nVALIDATION_MESSAGES_TR = {\n    'validation_started': 'Model doğrulama başlatıldı',\n    'checking_geometry': 'Geometri kontrol ediliyor...',\n    'checking_manufacturing': 'Üretilebilirlik kontrol ediliyor...',\n    'checking_standards': 'Standart uyumluluğu kontrol ediliyor...',\n    'validation_complete': 'Doğrulama tamamlandı',\n    'issues_found': '{count} sorun tespit edildi',\n    'certificate_issued': 'Kalite sertifikası düzenlendi',\n    'fix_suggested': 'Otomatik düzeltme önerisi',\n    'quality_score': 'Kalite puanı: {score}/100'\n}\n```\n\n### Integration with CI/CD\n- Automated validation in git hooks\n- Quality gates in deployment pipeline\n- Continuous compliance monitoring\n- Automated report generation",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 25,
            "title": "Performance Profiling and Optimization Tools",
            "description": "Implement comprehensive performance profiling, bottleneck detection, and optimization tools for FreeCAD model generation workflows",
            "details": "## Performance Profiling and Optimization Tools\n\n### Performance Profiler Framework\n```python\nclass PerformanceProfiler:\n    \"\"\"Comprehensive performance profiling for FreeCAD operations\"\"\"\n    \n    def __init__(self):\n        self.profiler = cProfile.Profile()\n        self.memory_tracker = tracemalloc\n        self.gpu_monitor = GPUMonitor() if gpu_available() else None\n        self.metrics_collector = MetricsCollector()\n        self.flame_graph_generator = FlameGraphGenerator()\n    \n    @contextmanager\n    def profile_operation(\n        self,\n        operation_name: str,\n        trace_memory: bool = True,\n        trace_gpu: bool = True\n    ):\n        \"\"\"Context manager for profiling operations\"\"\"\n        \n        # Start profiling\n        self.profiler.enable()\n        if trace_memory:\n            self.memory_tracker.start()\n        if trace_gpu and self.gpu_monitor:\n            self.gpu_monitor.start()\n        \n        start_time = time.perf_counter()\n        start_memory = self.get_current_memory()\n        \n        try:\n            yield\n        finally:\n            # Stop profiling\n            elapsed_time = time.perf_counter() - start_time\n            end_memory = self.get_current_memory()\n            memory_delta = end_memory - start_memory\n            \n            self.profiler.disable()\n            \n            # Collect metrics\n            profile_data = ProfileData(\n                operation=operation_name,\n                elapsed_time=elapsed_time,\n                memory_used=memory_delta,\n                cpu_stats=self.get_cpu_stats(),\n                gpu_stats=self.gpu_monitor.get_stats() if self.gpu_monitor else None\n            )\n            \n            # Store profile data\n            self.metrics_collector.store(profile_data)\n            \n            if trace_memory:\n                self.memory_tracker.stop()\n```\n\n### FreeCAD Operation Profiler\n```python\nclass FreeCADOperationProfiler:\n    \"\"\"Profile FreeCAD-specific operations\"\"\"\n    \n    async def profile_document_operation(\n        self,\n        doc: FreeCAD.Document,\n        operation: Callable\n    ) -> OperationProfile:\n        \"\"\"Profile document operations with detailed metrics\"\"\"\n        \n        profile = OperationProfile()\n        \n        # Pre-operation metrics\n        profile.initial_object_count = len(doc.Objects)\n        profile.initial_memory = self.measure_document_memory(doc)\n        \n        # Profile the operation\n        with self.profile_timer() as timer:\n            result = await operation(doc)\n        \n        profile.execution_time = timer.elapsed\n        \n        # Post-operation metrics\n        profile.final_object_count = len(doc.Objects)\n        profile.final_memory = self.measure_document_memory(doc)\n        profile.objects_created = profile.final_object_count - profile.initial_object_count\n        profile.memory_growth = profile.final_memory - profile.initial_memory\n        \n        # Recompute performance\n        with self.profile_timer() as recompute_timer:\n            doc.recompute()\n        \n        profile.recompute_time = recompute_timer.elapsed\n        \n        # Analyze bottlenecks\n        profile.bottlenecks = await self.analyze_bottlenecks(doc, operation)\n        \n        return profile\n    \n    async def analyze_bottlenecks(\n        self,\n        doc: FreeCAD.Document,\n        operation: Callable\n    ) -> List[Bottleneck]:\n        \"\"\"Identify performance bottlenecks\"\"\"\n        \n        bottlenecks = []\n        \n        # Analyze object dependencies\n        dependency_analysis = self.analyze_dependencies(doc)\n        if dependency_analysis.circular_dependencies:\n            bottlenecks.append(Bottleneck(\n                type=\"circular_dependency\",\n                severity=\"high\",\n                objects=dependency_analysis.circular_dependencies,\n                impact_ms=dependency_analysis.estimated_impact\n            ))\n        \n        # Analyze complex features\n        for obj in doc.Objects:\n            if hasattr(obj, 'Shape'):\n                complexity = self.measure_shape_complexity(obj.Shape)\n                if complexity.is_complex:\n                    bottlenecks.append(Bottleneck(\n                        type=\"complex_geometry\",\n                        severity=\"medium\",\n                        object=obj.Name,\n                        metrics=complexity.metrics\n                    ))\n        \n        # Analyze expression engine\n        expression_bottlenecks = self.analyze_expressions(doc)\n        bottlenecks.extend(expression_bottlenecks)\n        \n        return bottlenecks\n```\n\n### Memory Profiler\n```python\nclass MemoryProfiler:\n    \"\"\"Detailed memory profiling and leak detection\"\"\"\n    \n    def __init__(self):\n        self.snapshots = []\n        self.leak_detector = LeakDetector()\n    \n    async def profile_memory_usage(\n        self,\n        operation: Callable,\n        detect_leaks: bool = True\n    ) -> MemoryProfile:\n        \"\"\"Profile memory usage during operation\"\"\"\n        \n        # Take initial snapshot\n        initial_snapshot = tracemalloc.take_snapshot()\n        self.snapshots.append(initial_snapshot)\n        \n        # Execute operation\n        gc.collect()  # Clean baseline\n        result = await operation()\n        \n        # Take final snapshot\n        final_snapshot = tracemalloc.take_snapshot()\n        self.snapshots.append(final_snapshot)\n        \n        # Analyze memory changes\n        top_stats = final_snapshot.compare_to(\n            initial_snapshot,\n            'lineno'\n        )\n        \n        profile = MemoryProfile()\n        \n        # Top memory consumers\n        for stat in top_stats[:10]:\n            profile.add_consumer(MemoryConsumer(\n                file=stat.traceback[0].filename,\n                line=stat.traceback[0].lineno,\n                size_diff=stat.size_diff,\n                count_diff=stat.count_diff\n            ))\n        \n        # Detect memory leaks\n        if detect_leaks:\n            leaks = await self.leak_detector.detect(\n                initial_snapshot,\n                final_snapshot\n            )\n            profile.potential_leaks = leaks\n        \n        # Memory fragmentation analysis\n        profile.fragmentation = self.analyze_fragmentation()\n        \n        return profile\n    \n    def analyze_fragmentation(self) -> FragmentationAnalysis:\n        \"\"\"Analyze memory fragmentation\"\"\"\n        \n        import psutil\n        process = psutil.Process()\n        \n        memory_info = process.memory_info()\n        memory_maps = process.memory_maps()\n        \n        analysis = FragmentationAnalysis()\n        analysis.rss = memory_info.rss\n        analysis.vms = memory_info.vms\n        analysis.shared = memory_info.shared if hasattr(memory_info, 'shared') else 0\n        \n        # Calculate fragmentation ratio\n        analysis.fragmentation_ratio = 1 - (analysis.rss / analysis.vms)\n        \n        return analysis\n```\n\n### GPU Performance Monitor\n```python\nclass GPUMonitor:\n    \"\"\"Monitor GPU usage for accelerated operations\"\"\"\n    \n    def __init__(self):\n        try:\n            import pynvml\n            pynvml.nvmlInit()\n            self.gpu_available = True\n            self.handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n        except:\n            self.gpu_available = False\n    \n    def get_gpu_metrics(self) -> GPUMetrics:\n        \"\"\"Get current GPU metrics\"\"\"\n        \n        if not self.gpu_available:\n            return None\n        \n        import pynvml\n        \n        metrics = GPUMetrics()\n        \n        # Memory usage\n        mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)\n        metrics.memory_used = mem_info.used\n        metrics.memory_total = mem_info.total\n        metrics.memory_free = mem_info.free\n        \n        # Utilization\n        util = pynvml.nvmlDeviceGetUtilizationRates(self.handle)\n        metrics.gpu_utilization = util.gpu\n        metrics.memory_utilization = util.memory\n        \n        # Temperature\n        metrics.temperature = pynvml.nvmlDeviceGetTemperature(\n            self.handle,\n            pynvml.NVML_TEMPERATURE_GPU\n        )\n        \n        # Power\n        metrics.power_draw = pynvml.nvmlDeviceGetPowerUsage(self.handle) / 1000.0\n        \n        return metrics\n```\n\n### Optimization Recommender\n```python\nclass OptimizationRecommender:\n    \"\"\"Generate optimization recommendations based on profiling\"\"\"\n    \n    async def analyze_and_recommend(\n        self,\n        profile_data: ProfileData\n    ) -> List[OptimizationRecommendation]:\n        \"\"\"Analyze profile data and generate recommendations\"\"\"\n        \n        recommendations = []\n        \n        # Memory optimization\n        if profile_data.memory_peaked > threshold.memory_high:\n            recommendations.append(OptimizationRecommendation(\n                type=\"memory\",\n                priority=\"high\",\n                title=\"High memory usage detected\",\n                description=f\"Peak memory: {profile_data.memory_peaked}MB\",\n                suggestions=[\n                    \"Enable lazy loading for large objects\",\n                    \"Implement object pooling\",\n                    \"Use memory-mapped files for large datasets\"\n                ]\n            ))\n        \n        # CPU optimization\n        if profile_data.cpu_bottlenecks:\n            for bottleneck in profile_data.cpu_bottlenecks:\n                if bottleneck.time_percent > 20:\n                    recommendations.append(OptimizationRecommendation(\n                        type=\"cpu\",\n                        priority=\"high\",\n                        title=f\"CPU bottleneck in {bottleneck.function}\",\n                        description=f\"Consuming {bottleneck.time_percent}% of execution time\",\n                        suggestions=[\n                            \"Consider caching results\",\n                            \"Implement parallel processing\",\n                            \"Optimize algorithm complexity\"\n                        ]\n                    ))\n        \n        # I/O optimization\n        if profile_data.io_wait_time > threshold.io_wait_high:\n            recommendations.append(OptimizationRecommendation(\n                type=\"io\",\n                priority=\"medium\",\n                title=\"High I/O wait time\",\n                description=f\"I/O wait: {profile_data.io_wait_time}s\",\n                suggestions=[\n                    \"Implement async I/O operations\",\n                    \"Use buffered reading/writing\",\n                    \"Consider SSD storage for working files\"\n                ]\n            ))\n        \n        # FreeCAD-specific optimizations\n        freecad_recommendations = await self.analyze_freecad_specific(profile_data)\n        recommendations.extend(freecad_recommendations)\n        \n        return recommendations\n    \n    async def analyze_freecad_specific(\n        self,\n        profile_data: ProfileData\n    ) -> List[OptimizationRecommendation]:\n        \"\"\"FreeCAD-specific optimization recommendations\"\"\"\n        \n        recommendations = []\n        \n        # Recompute optimization\n        if profile_data.recompute_time > 1.0:\n            recommendations.append(OptimizationRecommendation(\n                type=\"freecad_recompute\",\n                priority=\"high\",\n                title=\"Slow recompute detected\",\n                suggestions=[\n                    \"Reduce dependency chain depth\",\n                    \"Use SkipRecompute flag for batch operations\",\n                    \"Optimize expression engine usage\"\n                ]\n            ))\n        \n        # Shape complexity\n        if profile_data.shape_complexity_score > 0.8:\n            recommendations.append(OptimizationRecommendation(\n                type=\"freecad_geometry\",\n                priority=\"medium\",\n                title=\"Complex geometry detected\",\n                suggestions=[\n                    \"Simplify geometry where possible\",\n                    \"Use LOD (Level of Detail) for visualization\",\n                    \"Consider mesh decimation for exports\"\n                ]\n            ))\n        \n        return recommendations\n```\n\n### Real-time Performance Dashboard\n```python\nclass PerformanceDashboard:\n    \"\"\"Real-time performance monitoring dashboard\"\"\"\n    \n    def __init__(self):\n        self.metrics_buffer = deque(maxlen=1000)\n        self.websocket_server = WebSocketServer()\n        self.update_interval = 100  # ms\n    \n    async def start_monitoring(self):\n        \"\"\"Start real-time monitoring\"\"\"\n        \n        asyncio.create_task(self.collect_metrics_loop())\n        asyncio.create_task(self.broadcast_metrics_loop())\n    \n    async def collect_metrics_loop(self):\n        \"\"\"Continuously collect performance metrics\"\"\"\n        \n        while True:\n            metrics = await self.collect_current_metrics()\n            self.metrics_buffer.append(metrics)\n            await asyncio.sleep(self.update_interval / 1000)\n    \n    async def collect_current_metrics(self) -> RealtimeMetrics:\n        \"\"\"Collect current system metrics\"\"\"\n        \n        import psutil\n        \n        metrics = RealtimeMetrics()\n        metrics.timestamp = datetime.now(UTC)\n        \n        # CPU metrics\n        metrics.cpu_percent = psutil.cpu_percent(interval=0.1)\n        metrics.cpu_freq = psutil.cpu_freq().current\n        \n        # Memory metrics\n        mem = psutil.virtual_memory()\n        metrics.memory_percent = mem.percent\n        metrics.memory_available = mem.available\n        \n        # Disk I/O\n        disk_io = psutil.disk_io_counters()\n        metrics.disk_read_bytes = disk_io.read_bytes\n        metrics.disk_write_bytes = disk_io.write_bytes\n        \n        # Network I/O\n        net_io = psutil.net_io_counters()\n        metrics.network_sent = net_io.bytes_sent\n        metrics.network_recv = net_io.bytes_recv\n        \n        # FreeCAD specific\n        if self.freecad_monitor:\n            metrics.freecad_metrics = await self.freecad_monitor.get_metrics()\n        \n        return metrics\n```\n\n### Benchmark Suite\n```python\nclass BenchmarkSuite:\n    \"\"\"Comprehensive benchmark suite for FreeCAD operations\"\"\"\n    \n    def __init__(self):\n        self.benchmarks = []\n        self.results_store = BenchmarkResultsStore()\n    \n    async def run_benchmark_suite(\n        self,\n        categories: List[str] = None\n    ) -> BenchmarkReport:\n        \"\"\"Run complete benchmark suite\"\"\"\n        \n        report = BenchmarkReport()\n        report.started_at = datetime.now(UTC)\n        report.system_info = self.collect_system_info()\n        \n        # Select benchmarks to run\n        benchmarks_to_run = self.select_benchmarks(categories)\n        \n        for benchmark in benchmarks_to_run:\n            result = await self.run_single_benchmark(benchmark)\n            report.add_result(result)\n            \n            # Compare with baseline\n            baseline = await self.results_store.get_baseline(benchmark.name)\n            if baseline:\n                comparison = self.compare_with_baseline(result, baseline)\n                report.add_comparison(comparison)\n        \n        report.completed_at = datetime.now(UTC)\n        report.calculate_scores()\n        \n        # Store results\n        await self.results_store.store(report)\n        \n        return report\n    \n    async def run_single_benchmark(\n        self,\n        benchmark: Benchmark\n    ) -> BenchmarkResult:\n        \"\"\"Run individual benchmark\"\"\"\n        \n        result = BenchmarkResult(name=benchmark.name)\n        \n        # Warmup runs\n        for _ in range(benchmark.warmup_runs):\n            await benchmark.execute()\n        \n        # Actual benchmark runs\n        timings = []\n        memory_usage = []\n        \n        for _ in range(benchmark.iterations):\n            gc.collect()  # Clean state\n            \n            start_memory = self.get_memory_usage()\n            start_time = time.perf_counter()\n            \n            await benchmark.execute()\n            \n            elapsed = time.perf_counter() - start_time\n            end_memory = self.get_memory_usage()\n            \n            timings.append(elapsed)\n            memory_usage.append(end_memory - start_memory)\n        \n        # Calculate statistics\n        result.mean_time = statistics.mean(timings)\n        result.median_time = statistics.median(timings)\n        result.std_dev = statistics.stdev(timings)\n        result.min_time = min(timings)\n        result.max_time = max(timings)\n        result.mean_memory = statistics.mean(memory_usage)\n        \n        return result\n```\n\n### Cache Performance Analyzer\n```python\nclass CachePerformanceAnalyzer:\n    \"\"\"Analyze and optimize caching strategies\"\"\"\n    \n    def __init__(self):\n        self.cache_stats = defaultdict(CacheStatistics)\n    \n    def analyze_cache_performance(\n        self,\n        cache_name: str\n    ) -> CacheAnalysis:\n        \"\"\"Analyze cache effectiveness\"\"\"\n        \n        stats = self.cache_stats[cache_name]\n        \n        analysis = CacheAnalysis()\n        analysis.hit_rate = stats.hits / (stats.hits + stats.misses)\n        analysis.miss_rate = 1 - analysis.hit_rate\n        analysis.eviction_rate = stats.evictions / stats.total_requests\n        \n        # Memory efficiency\n        analysis.memory_efficiency = stats.useful_data / stats.total_memory\n        \n        # Recommendations\n        if analysis.hit_rate < 0.7:\n            analysis.recommendations.append(\n                \"Low hit rate - consider increasing cache size\"\n            )\n        \n        if analysis.eviction_rate > 0.3:\n            analysis.recommendations.append(\n                \"High eviction rate - review eviction policy\"\n            )\n        \n        return analysis\n```\n\n### Turkish Localization\n```python\nPROFILING_MESSAGES_TR = {\n    'profiling_started': 'Performans profili başlatıldı',\n    'analyzing_performance': 'Performans analiz ediliyor...',\n    'bottleneck_detected': 'Darboğaz tespit edildi: {location}',\n    'optimization_suggested': 'Optimizasyon önerisi',\n    'benchmark_running': 'Kıyaslama testi çalışıyor',\n    'memory_leak_detected': 'Bellek sızıntısı tespit edildi',\n    'performance_improved': 'Performans %{percent} iyileştirildi',\n    'cache_hit_rate': 'Önbellek isabet oranı: %{rate}'\n}\n```\n\n### Integration Points\n- Prometheus metrics export\n- Grafana dashboard integration\n- Continuous profiling in CI/CD\n- A/B testing for optimizations\n- Automated performance regression detection",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          },
          {
            "id": 26,
            "title": "Backup, Recovery, and Disaster Management",
            "description": "Implement comprehensive backup strategies, disaster recovery mechanisms, and business continuity planning for FreeCAD model generation infrastructure",
            "details": "## Backup, Recovery, and Disaster Management\n\n### Backup Strategy Framework\n```python\nclass BackupStrategyFramework:\n    \"\"\"Enterprise-grade backup system for FreeCAD infrastructure\"\"\"\n    \n    def __init__(self):\n        self.backup_scheduler = BackupScheduler()\n        self.storage_manager = MultiTierStorageManager()\n        self.encryption_service = BackupEncryption()\n        self.verification_service = BackupVerification()\n        \n    async def create_backup_policy(\n        self,\n        policy_name: str,\n        retention_rules: RetentionRules,\n        backup_schedule: BackupSchedule\n    ) -> BackupPolicy:\n        \"\"\"Create comprehensive backup policy\"\"\"\n        \n        policy = BackupPolicy(\n            id=uuid.uuid4(),\n            name=policy_name,\n            created_at=datetime.now(UTC),\n            retention=retention_rules,\n            schedule=backup_schedule,\n            encryption_enabled=True,\n            compression_enabled=True\n        )\n        \n        # Configure backup tiers\n        policy.tiers = [\n            BackupTier(\n                name=\"hot\",\n                storage_type=\"ssd\",\n                retention_days=7,\n                access_frequency=\"immediate\"\n            ),\n            BackupTier(\n                name=\"warm\",\n                storage_type=\"hdd\",\n                retention_days=30,\n                access_frequency=\"minutes\"\n            ),\n            BackupTier(\n                name=\"cold\",\n                storage_type=\"s3_glacier\",\n                retention_days=365,\n                access_frequency=\"hours\"\n            )\n        ]\n        \n        return policy\n```\n\n### Incremental Backup System\n```python\nclass IncrementalBackupSystem:\n    \"\"\"Efficient incremental backup with deduplication\"\"\"\n    \n    def __init__(self):\n        self.dedup_engine = DeduplicationEngine()\n        self.delta_calculator = DeltaCalculator()\n        self.snapshot_manager = SnapshotManager()\n    \n    async def create_incremental_backup(\n        self,\n        source: BackupSource,\n        previous_backup: Optional[BackupSnapshot] = None\n    ) -> IncrementalBackup:\n        \"\"\"Create incremental backup with deduplication\"\"\"\n        \n        backup = IncrementalBackup()\n        backup.timestamp = datetime.now(UTC)\n        backup.source_id = source.id\n        \n        if previous_backup:\n            # Calculate changes since last backup\n            delta = await self.delta_calculator.calculate_delta(\n                previous_backup,\n                source\n            )\n            \n            # Store only changed blocks\n            for changed_block in delta.changed_blocks:\n                # Check for deduplication\n                block_hash = self.calculate_block_hash(changed_block)\n                \n                if not await self.dedup_engine.exists(block_hash):\n                    # Store new block\n                    compressed_block = self.compress_block(changed_block)\n                    encrypted_block = await self.encryption_service.encrypt(\n                        compressed_block\n                    )\n                    \n                    await self.storage_manager.store_block(\n                        block_hash,\n                        encrypted_block\n                    )\n                    \n                    await self.dedup_engine.register(block_hash)\n                \n                backup.add_block_reference(block_hash)\n            \n            backup.type = \"incremental\"\n            backup.parent_backup = previous_backup.id\n        else:\n            # Full backup\n            backup = await self.create_full_backup(source)\n            backup.type = \"full\"\n        \n        # Create snapshot\n        snapshot = await self.snapshot_manager.create_snapshot(backup)\n        \n        return backup\n    \n    def calculate_block_hash(self, block: DataBlock) -> str:\n        \"\"\"Calculate SHA-256 hash for deduplication\"\"\"\n        return hashlib.sha256(block.data).hexdigest()\n```\n\n### Disaster Recovery Orchestrator\n```python\nclass DisasterRecoveryOrchestrator:\n    \"\"\"Orchestrate disaster recovery operations\"\"\"\n    \n    def __init__(self):\n        self.recovery_planner = RecoveryPlanner()\n        self.failover_manager = FailoverManager()\n        self.health_monitor = HealthMonitor()\n        self.notification_service = NotificationService()\n    \n    async def initiate_disaster_recovery(\n        self,\n        disaster_type: str,\n        affected_systems: List[str],\n        recovery_point_objective: timedelta\n    ) -> RecoveryOperation:\n        \"\"\"Initiate coordinated disaster recovery\"\"\"\n        \n        operation = RecoveryOperation(\n            id=uuid.uuid4(),\n            disaster_type=disaster_type,\n            started_at=datetime.now(UTC),\n            rpo=recovery_point_objective\n        )\n        \n        # Assess damage\n        assessment = await self.assess_damage(affected_systems)\n        operation.damage_assessment = assessment\n        \n        # Create recovery plan\n        plan = await self.recovery_planner.create_plan(\n            assessment,\n            recovery_point_objective\n        )\n        operation.recovery_plan = plan\n        \n        # Execute recovery steps\n        for step in plan.steps:\n            try:\n                if step.type == \"failover\":\n                    await self.execute_failover(step)\n                elif step.type == \"restore\":\n                    await self.execute_restore(step)\n                elif step.type == \"rebuild\":\n                    await self.execute_rebuild(step)\n                \n                operation.completed_steps.append(step)\n                \n                # Notify stakeholders\n                await self.notification_service.notify_progress(\n                    step,\n                    operation\n                )\n                \n            except RecoveryStepError as e:\n                operation.failed_steps.append((step, e))\n                \n                # Attempt alternative recovery path\n                alternative = await self.recovery_planner.find_alternative(\n                    step,\n                    e\n                )\n                if alternative:\n                    plan.steps.insert(plan.current_index + 1, alternative)\n        \n        operation.completed_at = datetime.now(UTC)\n        operation.recovery_time = operation.completed_at - operation.started_at\n        \n        return operation\n    \n    async def execute_failover(self, step: RecoveryStep):\n        \"\"\"Execute failover to secondary system\"\"\"\n        \n        # Health check secondary\n        secondary_health = await self.health_monitor.check_system(\n            step.target_system\n        )\n        \n        if not secondary_health.is_healthy:\n            raise FailoverError(f\"Secondary system unhealthy: {step.target_system}\")\n        \n        # Perform failover\n        await self.failover_manager.failover(\n            from_system=step.source_system,\n            to_system=step.target_system,\n            sync_data=True\n        )\n        \n        # Verify failover\n        verification = await self.verify_failover(step.target_system)\n        if not verification.success:\n            raise FailoverError(\"Failover verification failed\")\n```\n\n### Point-in-Time Recovery\n```python\nclass PointInTimeRecovery:\n    \"\"\"Restore system to any point in time\"\"\"\n    \n    def __init__(self):\n        self.transaction_log = TransactionLog()\n        self.snapshot_store = SnapshotStore()\n        self.replay_engine = ReplayEngine()\n    \n    async def restore_to_point_in_time(\n        self,\n        target_time: datetime,\n        systems: List[str]\n    ) -> RestoreResult:\n        \"\"\"Restore systems to specific point in time\"\"\"\n        \n        result = RestoreResult()\n        result.target_time = target_time\n        \n        for system in systems:\n            # Find nearest snapshot before target time\n            snapshot = await self.snapshot_store.find_nearest_snapshot(\n                system,\n                target_time\n            )\n            \n            if not snapshot:\n                result.add_error(system, \"No snapshot found\")\n                continue\n            \n            # Restore from snapshot\n            await self.restore_snapshot(system, snapshot)\n            \n            # Replay transactions from snapshot to target time\n            transactions = await self.transaction_log.get_transactions(\n                system,\n                from_time=snapshot.timestamp,\n                to_time=target_time\n            )\n            \n            replay_result = await self.replay_engine.replay_transactions(\n                system,\n                transactions\n            )\n            \n            result.add_system_result(system, replay_result)\n        \n        # Verify consistency across systems\n        consistency_check = await self.verify_consistency(systems, target_time)\n        result.consistency_verified = consistency_check.is_consistent\n        \n        return result\n```\n\n### Model Recovery Service\n```python\nclass ModelRecoveryService:\n    \"\"\"Specialized recovery for FreeCAD models\"\"\"\n    \n    async def recover_corrupted_model(\n        self,\n        model_path: Path,\n        recovery_options: RecoveryOptions = None\n    ) -> RecoveryResult:\n        \"\"\"Recover corrupted FreeCAD model\"\"\"\n        \n        result = RecoveryResult()\n        \n        # Try direct recovery\n        try:\n            recovered_doc = await self.attempt_direct_recovery(model_path)\n            if recovered_doc:\n                result.success = True\n                result.document = recovered_doc\n                return result\n        except Exception as e:\n            result.add_attempt(\"direct_recovery\", False, str(e))\n        \n        # Try from backup\n        backup = await self.find_latest_backup(model_path)\n        if backup:\n            try:\n                recovered_doc = await self.restore_from_backup(backup)\n                result.success = True\n                result.document = recovered_doc\n                result.data_loss = self.calculate_data_loss(backup)\n                return result\n            except Exception as e:\n                result.add_attempt(\"backup_recovery\", False, str(e))\n        \n        # Try partial recovery\n        try:\n            partial_doc = await self.attempt_partial_recovery(model_path)\n            if partial_doc:\n                result.success = True\n                result.partial = True\n                result.document = partial_doc\n                result.recovered_objects = self.list_recovered_objects(partial_doc)\n                return result\n        except Exception as e:\n            result.add_attempt(\"partial_recovery\", False, str(e))\n        \n        # Rebuild from history\n        if recovery_options and recovery_options.allow_rebuild:\n            try:\n                rebuilt_doc = await self.rebuild_from_history(model_path)\n                result.success = True\n                result.rebuilt = True\n                result.document = rebuilt_doc\n                return result\n            except Exception as e:\n                result.add_attempt(\"history_rebuild\", False, str(e))\n        \n        result.success = False\n        return result\n    \n    async def attempt_partial_recovery(\n        self,\n        model_path: Path\n    ) -> Optional[FreeCAD.Document]:\n        \"\"\"Recover salvageable parts of corrupted model\"\"\"\n        \n        doc = FreeCAD.newDocument()\n        recovered_count = 0\n        \n        # Parse raw file structure\n        raw_data = await self.parse_raw_fcstd(model_path)\n        \n        for object_data in raw_data.objects:\n            try:\n                # Attempt to recreate object\n                obj = self.recreate_object(object_data)\n                if obj:\n                    doc.addObject(obj)\n                    recovered_count += 1\n            except Exception:\n                continue  # Skip corrupted objects\n        \n        if recovered_count > 0:\n            return doc\n        \n        return None\n```\n\n### Continuous Data Protection\n```python\nclass ContinuousDataProtection:\n    \"\"\"Real-time continuous data protection\"\"\"\n    \n    def __init__(self):\n        self.change_tracker = ChangeDataCapture()\n        self.replication_manager = ReplicationManager()\n        self.journal_writer = JournalWriter()\n    \n    async def enable_cdp(\n        self,\n        source: DataSource,\n        targets: List[ReplicationTarget]\n    ):\n        \"\"\"Enable continuous data protection\"\"\"\n        \n        # Start change data capture\n        await self.change_tracker.start_capture(source)\n        \n        # Configure replication targets\n        for target in targets:\n            await self.replication_manager.add_target(\n                source,\n                target,\n                ReplicationMode.SYNCHRONOUS if target.is_local else ReplicationMode.ASYNCHRONOUS\n            )\n        \n        # Start journaling\n        await self.journal_writer.start_journaling(source)\n        \n        # Monitor and replicate changes\n        async for change in self.change_tracker.get_changes():\n            # Write to journal\n            await self.journal_writer.write(change)\n            \n            # Replicate to targets\n            replication_tasks = []\n            for target in targets:\n                task = asyncio.create_task(\n                    self.replication_manager.replicate(change, target)\n                )\n                replication_tasks.append(task)\n            \n            # Wait for critical replications\n            critical_tasks = [\n                t for t, target in zip(replication_tasks, targets)\n                if target.is_critical\n            ]\n            \n            if critical_tasks:\n                await asyncio.gather(*critical_tasks)\n```\n\n### Business Continuity Manager\n```python\nclass BusinessContinuityManager:\n    \"\"\"Ensure business continuity during disasters\"\"\"\n    \n    def __init__(self):\n        self.rto_monitor = RTOMonitor()  # Recovery Time Objective\n        self.rpo_monitor = RPOMonitor()  # Recovery Point Objective\n        self.drill_scheduler = DrillScheduler()\n        self.compliance_checker = ComplianceChecker()\n    \n    async def validate_continuity_plan(\n        self,\n        plan: ContinuityPlan\n    ) -> ValidationResult:\n        \"\"\"Validate business continuity plan\"\"\"\n        \n        validation = ValidationResult()\n        \n        # Check RTO compliance\n        rto_analysis = await self.rto_monitor.analyze_plan(plan)\n        if rto_analysis.exceeds_objective:\n            validation.add_issue(\n                \"RTO exceeded\",\n                f\"Plan RTO: {rto_analysis.estimated_rto}, Required: {plan.rto_requirement}\"\n            )\n        \n        # Check RPO compliance\n        rpo_analysis = await self.rpo_monitor.analyze_plan(plan)\n        if rpo_analysis.data_loss_risk > plan.acceptable_data_loss:\n            validation.add_issue(\n                \"RPO risk\",\n                f\"Potential data loss: {rpo_analysis.data_loss_risk}\"\n            )\n        \n        # Validate redundancy\n        redundancy_check = self.check_redundancy(plan)\n        validation.add_results(\"redundancy\", redundancy_check)\n        \n        # Check compliance\n        compliance = await self.compliance_checker.check(plan)\n        validation.add_results(\"compliance\", compliance)\n        \n        return validation\n    \n    async def execute_disaster_drill(\n        self,\n        drill_scenario: DrillScenario\n    ) -> DrillReport:\n        \"\"\"Execute disaster recovery drill\"\"\"\n        \n        report = DrillReport()\n        report.scenario = drill_scenario\n        report.started_at = datetime.now(UTC)\n        \n        # Create isolated environment\n        drill_env = await self.create_drill_environment()\n        \n        # Simulate disaster\n        await drill_env.simulate_disaster(drill_scenario.disaster_type)\n        \n        # Execute recovery\n        recovery_start = datetime.now(UTC)\n        recovery_result = await drill_env.execute_recovery(\n            drill_scenario.recovery_plan\n        )\n        recovery_time = datetime.now(UTC) - recovery_start\n        \n        # Validate recovery\n        validation = await drill_env.validate_recovery()\n        \n        report.recovery_time = recovery_time\n        report.recovery_successful = validation.success\n        report.issues_found = validation.issues\n        report.recommendations = self.generate_recommendations(\n            drill_scenario,\n            recovery_result,\n            validation\n        )\n        \n        # Cleanup drill environment\n        await drill_env.cleanup()\n        \n        report.completed_at = datetime.now(UTC)\n        \n        return report\n```\n\n### Geo-Redundant Backup\n```python\nclass GeoRedundantBackup:\n    \"\"\"Geographically distributed backup system\"\"\"\n    \n    def __init__(self):\n        self.region_manager = RegionManager()\n        self.sync_coordinator = SyncCoordinator()\n        self.latency_optimizer = LatencyOptimizer()\n    \n    async def setup_geo_redundancy(\n        self,\n        primary_region: str,\n        replica_regions: List[str],\n        consistency_level: str = \"eventual\"\n    ):\n        \"\"\"Setup geographically redundant backups\"\"\"\n        \n        # Configure primary region\n        primary = await self.region_manager.configure_primary(primary_region)\n        \n        # Setup replica regions\n        replicas = []\n        for region in replica_regions:\n            replica = await self.region_manager.configure_replica(\n                region,\n                primary,\n                consistency_level\n            )\n            replicas.append(replica)\n        \n        # Optimize replication topology\n        topology = await self.latency_optimizer.optimize_topology(\n            primary,\n            replicas\n        )\n        \n        # Start synchronization\n        await self.sync_coordinator.start_sync(topology)\n        \n        return GeoRedundancyConfig(\n            primary=primary,\n            replicas=replicas,\n            topology=topology,\n            consistency=consistency_level\n        )\n```\n\n### Compliance and Audit\n```python\nclass BackupComplianceAuditor:\n    \"\"\"Ensure backup compliance with regulations\"\"\"\n    \n    def __init__(self):\n        self.regulations = {\n            'GDPR': GDPRCompliance(),\n            'KVKK': KVKKCompliance(),  # Turkish data protection\n            'ISO27001': ISO27001Compliance(),\n            'SOC2': SOC2Compliance()\n        }\n    \n    async def audit_backup_compliance(\n        self,\n        backup_system: BackupSystem\n    ) -> ComplianceAuditReport:\n        \"\"\"Audit backup system compliance\"\"\"\n        \n        report = ComplianceAuditReport()\n        \n        for regulation_name, checker in self.regulations.items():\n            result = await checker.audit(backup_system)\n            report.add_regulation_result(regulation_name, result)\n            \n            if not result.compliant:\n                # Generate remediation plan\n                remediation = await checker.generate_remediation(result.violations)\n                report.add_remediation(regulation_name, remediation)\n        \n        # Data retention compliance\n        retention_audit = await self.audit_retention_policies(backup_system)\n        report.retention_compliance = retention_audit\n        \n        # Encryption compliance\n        encryption_audit = await self.audit_encryption(backup_system)\n        report.encryption_compliance = encryption_audit\n        \n        # Access control audit\n        access_audit = await self.audit_access_controls(backup_system)\n        report.access_compliance = access_audit\n        \n        report.overall_compliance = report.calculate_overall_compliance()\n        \n        return report\n```\n\n### Turkish Localization\n```python\nBACKUP_MESSAGES_TR = {\n    'backup_started': 'Yedekleme başlatıldı',\n    'backup_completed': 'Yedekleme tamamlandı',\n    'recovery_initiated': 'Kurtarma işlemi başlatıldı',\n    'recovery_successful': 'Kurtarma başarılı',\n    'disaster_detected': 'Felaket durumu tespit edildi',\n    'failover_executing': 'Yedek sisteme geçiliyor',\n    'data_loss_warning': 'Veri kaybı uyarısı: {amount}',\n    'compliance_check': 'Uyumluluk denetimi',\n    'drill_scheduled': 'Tatbikat planlandı: {date}'\n}\n```\n\n### Integration Points\n- Integration with MinIO for object storage\n- PostgreSQL continuous archiving\n- Redis persistence and AOF\n- Kubernetes backup operators\n- Cloud provider backup services (AWS Backup, Azure Backup)\n- Monitoring integration with Prometheus/Grafana",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "CAM Pipeline with FreeCAD Path WB, Tool Library, Post-Processing and Static G-code Validation",
        "description": "Create CAM endpoints and worker to compute toolpaths using FreeCAD Path Workbench, apply machine/post mapping, enforce rules (WCS, stock, keep-out, tool limits), generate G-code with metadata and store artefacts.",
        "details": "APIs: POST /cam/run, /cam/post; GET /cam/jobs/:id\nInputs: model_ref, machine_profile, material, operations, WCS, stock, keep-outs\nImplementation:\n- Tool library table and seed (6mm Carbide Endmill 4F, 10mm Drill HSS); ensure API JSON schema per PRD; validate tool↔strategy and diameter vs geometry → 409 CAM_LIMIT_EXCEEDED\n- Machine→post mapping (post_map.py):\n  - 'HAAS MINI MILL'→'post_haas_mill'→.nc\n  - 'Tormach 1100MX'→'post_tormach_mill'→.tap\n  - 'Generic 3-axis Mill'→'post_generic_mill'→.gcode\n  - unknown → 415 POSTPROC_UNSUPPORTED\n- Path WB scripting: create Job, set WCS (G54..G59), define Clearance/Rapid/Z safety heights, create Facing, Profile(2D), Pocket, Drilling ops with validated parameters; sequence to minimize tool changes; compute()\n- Machine limits: axis stroke, feed/speed caps; if violations, propose reduced params or 422\n- Keep-out: AABB intersects path segments → revise or error\n- Post-process: generate G-code; embed SHA256 and meta (tool list, est time) as comments; version rev on re-gen\n- Static G-code validation: pygcode/gcodeparser to ensure modal consistency, forbid codes, feed/speed caps; produce report and block on critical\n- Artefacts: save path, G-code, reports to MinIO with tags {job_id, machine, post}\nPseudocode (validation snippet):\n- for line in gcode:\n  - if code in FORBIDDEN: errors.append(...)\n  - enforce F<=maxF, S<=maxS; ensure modal states initialized\n",
        "testStrategy": "Integration: run CAM on sample prism; validate tool selection and stock/WCS; unknown machine returns 415. Verify G-code contains metadata comments and sha256. Static validator flags a forbidden M-code. Performance: ensure compute < configured timeout for medium parts. Artefacts stored with correct tags.",
        "priority": "high",
        "dependencies": [
          6,
          7
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design CAM APIs and Schemas (/cam/run, /cam/post, GET /cam/jobs/:id)",
            "description": "Define REST contracts, JSON schemas, status codes, and OpenAPI spec for CAM endpoints.",
            "dependencies": [],
            "details": "Scope:\n- Endpoints: POST /cam/run (compute toolpaths + postproc flag optional), POST /cam/post (post-process existing job/toolpaths), GET /cam/jobs/:id (status + artefact refs).\n- Inputs: model_ref, machine_profile, material, operations[], WCS (G54..G59), stock, keep_outs[].\n- Status codes: 202 queued, 200 OK; 400 VALIDATION_ERROR; 409 CAM_LIMIT_EXCEEDED; 415 POSTPROC_UNSUPPORTED; 422 MACHINE_LIMIT_VIOLATION or KEEP_OUT_VIOLATION; 504 TIMEOUT; 500 INTERNAL.\n- Error envelope: {code, message, details}.\nSample POST /cam/run body:\n{\n  \"model_ref\":\"s3://artefacts/models/prism.step\",\n  \"machine_profile\":\"HAAS MINI MILL\",\n  \"material\":\"6061\",\n  \"operations\":[{\"type\":\"Facing\",\"strategy\":\"planar\",\"tool_id\":1}],\n  \"WCS\":\"G54\",\n  \"stock\":{\"type\":\"box\",\"dims\":[100,60,20]},\n  \"keep_outs\":[{\"aabb\":[[10,10,0],[20,20,30]]}]\n}\nAcceptance:\n- OpenAPI 3.1 published with request/response schemas and examples.\n- Validation middleware enforces schema with descriptive errors.\n- Job object includes fields: id, status, progress, timings, artefact_refs, version_rev.\nPerformance:\n- JSON validation per request < 10 ms at p95.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Tool Library Schema, Seed, and Strategy/Geometry Validation",
            "description": "Implement tool library storage, seed initial tools, and enforce tool-strategy and geometry constraints.",
            "dependencies": [
              "8.1"
            ],
            "details": "DB schema (tools): id, name, type, diameter_mm, flutes, material, stickout_mm, holder, max_rpm, max_feed_mm_min, notes, created_at.\nSeed:\n- id=1, 6mm Carbide Endmill 4F; id=2, 10mm Drill HSS.\nRules:\n- Strategy compatibility: Endmill → {Facing, Profile2D, Pocket}; Drill → {Drilling}.\n- Geometry: tool.diameter <= min_feature_width for operation geometry; drilling hole_dia ≈ tool.diameter ±0.2mm; stepdowns respect flute length if provided.\n- On violation: 409 CAM_LIMIT_EXCEEDED with details {reason, op_index, tool_id}.\nSample tool JSON for API exposure:\n{\"id\":1,\"name\":\"6mm Carbide Endmill 4F\",\"type\":\"endmill\",\"diameter_mm\":6.0,\"max_rpm\":24000,\"max_feed_mm_min\":3000}\nAcceptance:\n- Seed migration creates 2 tools.\n- Validator rejects mismatched tool↔strategy and oversize diameter vs geometry.\n- Unit tests cover pass/fail cases.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Machine to Post-Processor Mapping and File Extension Rules",
            "description": "Create post_map module that resolves machine names to FreeCAD post and output extension.",
            "dependencies": [
              "8.1"
            ],
            "details": "Mapping (post_map.py):\n- 'HAAS MINI MILL' → post: 'post_haas_mill', ext: '.nc'\n- 'Tormach 1100MX' → post: 'post_tormach_mill', ext: '.tap'\n- 'Generic 3-axis Mill' → post: 'post_generic_mill', ext: '.gcode'\n- Unknown → error 415 POSTPROC_UNSUPPORTED {machine_profile}.\nFunctions:\n- resolve(machine_profile) → {post_name, extension}.\n- list_supported() → [names].\nAcceptance:\n- Unit tests for known mappings + unknown case.\n- Config override: env POST_MAP_JSON optional.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "FreeCAD Path WB Scripting Harness and Operations Sequencing",
            "description": "Build headless FreeCAD worker harness to create Job, set WCS/stock/heights, add ops, minimize tool changes, compute().",
            "dependencies": [
              "8.1",
              "8.2"
            ],
            "details": "Capabilities:\n- Launch FreeCAD in headless mode; import model_ref; create Path Job.\n- Set WCS: map G54..G59 to placement/origin per request; set heights: Clearance, Rapid, Safe Z from machine/material defaults.\n- Create operations: Facing, Profile(2D), Pocket, Drilling using validated parameters from operations[].\n- Sequencing: group by tool_id, then by rough→finish; minimize tool changes.\n- Units: mm; tolerances configurable.\nOutputs:\n- Persist FCStd and Path JSON (operations, tool list, est_time).\nAcceptance:\n- Sample prism job computes without errors.\n- Tool changes minimized vs naive sequence (<=1 change for single-tool jobs).\nPerformance:\n- Medium part compute() p95 < 120s; memory peak < 1.5GB.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Machine Limits Enforcement (Axis Stroke, Feed/Speed Caps) with Remediation",
            "description": "Validate toolpaths against machine travel and speed caps; propose safe parameter reductions or fail with 422.",
            "dependencies": [
              "8.1",
              "8.4"
            ],
            "details": "Inputs: machine profile including axis limits and caps.\nSample machine JSON:\n{\n  \"name\":\"HAAS MINI MILL\",\n  \"axes\":{\"X\":[0,406],\"Y\":[0,305],\"Z\":[0,254]},\n  \"caps\":{\"max_feed\":5000,\"max_rpm\":10000}\n}\nChecks:\n- Path AABB fits within axes minus safety margins.\n- F<=max_feed, S<=max_rpm; clamp if remediation enabled.\nRemediation:\n- Clamp feeds/speeds to caps; warn in job report; if geometric travel exceeded → 422 MACHINE_LIMIT_VIOLATION.\nAcceptance:\n- Violating feed/speed auto-reduced with note in metadata.\n- Travel violation returns 422 with offending extents.\nPerformance:\n- Limits evaluation < 50 ms per job.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Keep-out AABB Intersection and Path Revision",
            "description": "Detect intersections between path segments and keep-out regions; revise or fail.",
            "dependencies": [
              "8.1",
              "8.4"
            ],
            "details": "Algorithm:\n- Build AABB for each path segment/rapid; test against keep_outs[].\n- Revision strategies: raise clearance Z or re-route rapids where possible; otherwise error.\n- Report: list of intersections with segment ids and keep-out ids.\nAcceptance:\n- Non-intersecting jobs pass.\n- Intersecting case returns 422 KEEP_OUT_VIOLATION unless revision enabled and effective.\nPerformance:\n- Intersection checks scale O(n log n) with spatial index; p95 < 200 ms for 50k segments.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Post-Processing to G-code with Embedded Metadata and SHA256",
            "description": "Generate G-code using resolved post; embed metadata comments and compute SHA256; maintain version rev on re-gen.",
            "dependencies": [
              "8.3",
              "8.4",
              "8.5",
              "8.6"
            ],
            "details": "Steps:\n- Resolve post via post_map; invoke FreeCAD post with correct units and file extension.\n- Metadata header comments (prefixed by ';' or '(' per post): sha256, job_id, machine, tool list, est_time_s, created_at, version_rev.\n- Compute sha256 over content sans existing hash; write back into header as sha256.\n- Version rev: monotonic increment per re-generation.\nAcceptance:\n- Output file extension matches mapping.\n- Header contains required metadata and valid sha256 checksum.\nPerformance:\n- Post-processing p95 < 30 s; IO bounded.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Static G-code Validator and Report Generation",
            "description": "Parse and validate G-code for modal consistency, forbidden codes, and caps; emit JSON report and block on critical issues.",
            "dependencies": [
              "8.1",
              "8.7"
            ],
            "details": "Tools: pygcode or gcodeparser.\nRules:\n- Initialize modal states (units, plane, absolute/relative, feed mode).\n- Forbidden codes: {\"M00\",\"M01\",\"M198\",\"G92\"} unless explicitly allowed by machine profile.\n- Enforce F<=max_feed and S<=max_rpm per machine caps.\nReport JSON:\n{\n  \"summary\":{\"errors\":0,\"warnings\":1,\"lines\":1234},\n  \"issues\":[{\"severity\":\"error\",\"line\":57,\"code\":\"M00\",\"msg\":\"Program stop forbidden\"}]\n}\nAcceptance:\n- Validator flags a known forbidden M-code in fixture and blocks job (critical).\n- Modal states are initialized before first motion.\nPerformance:\n- Validation p95 < 15 s for 100k lines.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Artefact Persistence to MinIO with Tags and Metadata",
            "description": "Store FCStd, toolpath JSON, G-code, and validation reports to MinIO with proper keys and tags.",
            "dependencies": [
              "8.7",
              "8.8"
            ],
            "details": "Storage:\n- Keys: jobs/{job_id}/cam/{version_rev}/model.fcstd, toolpaths.json, program{ext}, validation_report.json.\n- Object tags: {job_id, machine, post} and content-type metadata.\n- Signed URL integration via file service; verify upload success; server-side sha256 for each artefact.\nAcceptance:\n- All artefacts saved with correct tags and are retrievable via short-lived URLs.\n- Artefact rows include size, sha256, content-type, created_at.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Execution Timeouts, Resource Limits, and Cancellation",
            "description": "Enforce per-stage timeouts, memory/CPU limits, and job cancellation handling.",
            "dependencies": [
              "8.4",
              "8.7"
            ],
            "details": "Limits:\n- Overall job timeout: 300 s (configurable).\n- FreeCAD compute: 120 s; postproc: 30 s; validation: 15 s; storage: 10 s.\n- cgroup/ulimit: CPU quota 2 cores, RAM 2 GB, temp disk 2 GB.\n- Cancellation: allow job cancel → terminate subprocesses, mark job canceled.\nAcceptance:\n- Timeouts propagate as 504 TIMEOUT with stage in details.\n- Cancellation within 1 s and no orphan processes.\nPerformance:\n- Scheduler respects concurrency limit and backpressure.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Failure Taxonomy and Consistent Error Handling",
            "description": "Define canonical error codes and map to HTTP; implement uniform error envelope.",
            "dependencies": [
              "8.1",
              "8.5",
              "8.6",
              "8.8",
              "8.10"
            ],
            "details": "Codes:\n- CAM_LIMIT_EXCEEDED (409)\n- POSTPROC_UNSUPPORTED (415)\n- MACHINE_LIMIT_VIOLATION (422)\n- KEEP_OUT_VIOLATION (422)\n- VALIDATION_ERROR (400)\n- GCODE_STATIC_BLOCKED (422)\n- TIMEOUT (504)\n- INTERNAL (500)\nEnvelope: {code, message, details:{stage, context}}; include correlation_id and job_id.\nAcceptance:\n- All failure paths return standardized JSON with actionable details.\n- Error catalog documented and referenced by API spec.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Observability: Structured Logging, Metrics, and Tracing",
            "description": "Add structured logs, metrics, and optional tracing; persist job logs to storage.",
            "dependencies": [
              "8.4",
              "8.7",
              "8.8",
              "8.9",
              "8.10",
              "8.11"
            ],
            "details": "Logging:\n- JSON logs with fields: ts, level, job_id, stage, machine, post, duration_ms.\n- Persist job logs to MinIO: jobs/{job_id}/logs/{version_rev}.ndjson.\nMetrics:\n- cam.jobs_submitted, cam.jobs_succeeded, cam.jobs_failed\n- cam.stage_duration_ms{stage}\n- cam.validation_issues{severity}\nTracing:\n- Optional OpenTelemetry spans for stages; sampling 10%.\nAcceptance:\n- Dashboards show p95 durations and failure rates.\n- Logs correlate across stages by job_id/correlation_id.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Test Fixtures, CI Integration Runs, and Acceptance Tests",
            "description": "Provide sample models and machine profiles; automate integration tests for end-to-end pipeline.",
            "dependencies": [
              "8.1",
              "8.2",
              "8.3",
              "8.4",
              "8.5",
              "8.6",
              "8.7",
              "8.8",
              "8.9",
              "8.10",
              "8.11",
              "8.12"
            ],
            "details": "Fixtures:\n- Prism STEP with features, ops JSON, keep-outs.\n- Machine profiles for three mapped machines + an unknown.\nTests:\n- Happy path: compute→post→validate→store; verify metadata and sha256 in G-code.\n- Unknown machine returns 415.\n- Validator flags forbidden M-code and blocks.\n- Performance: medium part completes < 300 s end-to-end.\nCI:\n- Runs on PR; artifacts uploaded; reports attached.\nAcceptance:\n- All tests green; coverage for validators and mappers > 80%.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Documentation and Operator Guidelines",
            "description": "Produce README, runbooks, configuration samples, and SOPs for operators.",
            "dependencies": [
              "8.1",
              "8.3",
              "8.7",
              "8.8",
              "8.9",
              "8.11",
              "8.12",
              "8.13"
            ],
            "details": "Docs:\n- API usage examples for /cam/run, /cam/post, GET job.\n- Sample configs: post_map, machine profiles, limits caps, forbidden codes.\n- Operator SOP: selecting WCS/stock, defining keep-outs, interpreting validator report, re-gen versioning.\n- Troubleshooting matrix by error code; escalation paths.\nAcceptance:\n- Reviewed and approved by CAM lead; docs published alongside OpenAPI.\n- Quickstart completes end-to-end run in < 15 minutes.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "CAMotics Simulation Integration and Reporting",
        "description": "Integrate CAMotics CLI to simulate G-code with optional stock/fixturing, produce video/snapshots and JSON report conforming to sim_report schema. Stream progress and alerts.",
        "details": "API: POST /simulations/run, GET /simulations/:id (JWT + licenseGuard)\nWorker:\n- Prepare CAMotics project (cmx JSON) with G-code, tool params, stock, WCS; run `camotics --no-gui --format json --output <dir> project.cmx`\n- Progress: tail CAMotics logs; periodically update jobs.progress\n- Outputs: generate frames (if CAMotics exports frames) or capture toolpath frames; convert to MP4/GIF via FFmpeg; store video and snapshot to MinIO\n- Parse results to fill sim_report v1 (workers/camotics.output.schema.json): collision_count, removed_volume_mm3, est_time_s, sim_video_path, sim_snapshot_path, notes[]\n- Basic collision/violation detection from CAMotics output; surface removal volume estimation\n- Publish warnings/alerts back to UI via job status and notifications\nPseudocode:\n- run_camotics(gcode, tools, stock):\n  - write cmx\n  - proc = subprocess.run(['camotics','--no-gui','--output',out,'project.cmx'], timeout=...) \n  - parse json → report; ffmpeg to make mp4\n  - save artefacts; update sim_runs row\n",
        "testStrategy": "Integration: simulate sample G-code; assert report has required fields; collision scenarios yield collision_count>0. Video and snapshot paths valid and downloadable via signed URL. Timeout and error handling place job in failed with useful message. Schema validation against v1 JSON schema.",
        "priority": "high",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configuration, Schema Baseline, and Observability Setup",
            "description": "Establish configuration keys, sim_report v1 schema location, and base logging/metrics for the CAMotics simulation worker.",
            "dependencies": [],
            "details": "Define config: CAMOTICS_BIN, FFMPEG_BIN, SIM_TIMEOUT_S, SIM_RETRY_MAX, SIM_TMP_DIR, MINIO_BUCKET_SIM, MINIO_PREFIX, SIGNED_URL_TTL_S. Confirm versions via startup checks. Pin sim_report v1 schema path (workers/camotics.output.schema.json). Structured logging fields: job_id, sim_id, cmx_path, camotics_exit_code, duration_ms, frames_count, bytes_uploaded, collision_count. Emit metrics/counters for runs_started, runs_succeeded, runs_failed, timeouts, collisions_detected. Document defaults and override precedence (env, .env, secrets). Acceptance: config loads with sane defaults, schema file accessible, health/preflight verifies CAMotics and FFmpeg presence.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Simulation API Endpoints with Auth and Guardrails",
            "description": "Implement POST /simulations/run and GET /simulations/:id with JWT auth and licenseGuard, enqueue worker job, and expose status/results.",
            "dependencies": [
              "9.1"
            ],
            "details": "POST /simulations/run: validates payload (gcode_ref or inline_gcode, tools[], stock{dims, units, material}, wcs, fixturing?), rate-limits, enqueues Celery task, returns sim_id and job_id. GET /simulations/:id: returns status, progress, sim_report, and presigned URLs for media. Enforce JWT and licenseGuard checks; reject if license expired or not entitled. Add request/response schemas and error codes (400 INVALID_INPUT, 402 LICENSE_REQUIRED, 409 CONFLICT, 422 UNPROCESSABLE_GCODE). Acceptance: happy path enqueues and returns IDs; unauthorized/forbidden requests blocked; GET returns evolving progress then final report.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "CAMotics Project (CMX) Builder",
            "description": "Translate API inputs (G-code, tools, stock, WCS) into a valid CAMotics .cmx project with correct units and coordinate frames.",
            "dependencies": [
              "9.1"
            ],
            "details": "Generate ephemeral workspace and paths (project.cmx, logs/, output/). Normalize units (mm/inch) and WCS offsets. Map tools[] to CAMotics tool definitions (diameter, flute length, number of flutes, feed/speed if applicable). Attach stock definition (box/cylinder) and fixturing offset if provided. Validate G-code headers and detect unit/mode (G20/G21, G90/G91). Persist cmx JSON to disk and reference G-code file path. Acceptance: CMX validates with CAMotics --check (if available) and contains expected entities; rejects missing/invalid tool or stock with clear messages.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "CAMotics CLI Runner with Timeout, Retry, and Log Tailing",
            "description": "Execute CAMotics CLI headless, enforce timeouts/retries, and tail logs to derive percent progress.",
            "dependencies": [
              "9.3"
            ],
            "details": "Invoke: camotics --no-gui --format json --output <out_dir> project.cmx. Stream stdout/stderr to rotating log files. Parse progress indicators from CAMotics output (e.g., simulated steps/toolpath segments) and estimate percent; fallback to periodic heartbeats. Apply timeout SIM_TIMEOUT_S and one retry on transient exit codes; capture exit status and duration. Store raw JSON outputs to out_dir. Acceptance: long-running sims show periodic progress updates; timeouts cancel process tree; retry triggers once on eligible failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Output Parser and sim_report v1 Mapping",
            "description": "Parse CAMotics JSON results and map to sim_report v1 fields including collisions, volume removed, and estimated time.",
            "dependencies": [
              "9.4"
            ],
            "details": "Read CAMotics JSON artifacts (e.g., simulation.json) and extract: est_time_s, removed_volume_mm3 (material removal), collision_count/violations (tool-stock/fixture collisions if present), and any warnings. Compose notes[] with structured messages (code, severity, message). Fill sim_video_path/sim_snapshot_path placeholders for later population. Validate against JSON schema and coerce defaults (zeros) when fields absent. Acceptance: parser produces schema-valid report for sample runs and differentiates collision/no-collision scenarios.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Media Capture and Encoding (Frames → MP4/GIF + Snapshot)",
            "description": "Capture simulation frames and generate MP4/GIF and a representative snapshot using FFmpeg.",
            "dependencies": [
              "9.3",
              "9.4"
            ],
            "details": "Detect if CAMotics exports frames; if available, collect and sort frames. If not, implement fallback frame capture strategy (e.g., invoke CAMotics in scripted steps or render toolpath layers if supported). Encode video: ffmpeg -r <fps> -i frames/%06d.png -c:v libx264 -pix_fmt yuv420p -movflags +faststart out.mp4. Generate GIF preview (palette optimization) and snapshot (first/median frame). Record frames_count, video duration, and sizes for metrics. Acceptance: MP4, GIF, and PNG snapshot generated deterministically for sample sims; encoding completes within budget and files pass basic validation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Artefact Storage and sim_runs Persistence",
            "description": "Upload media and report artefacts to MinIO and persist simulation run records in DB.",
            "dependencies": [
              "9.5",
              "9.6"
            ],
            "details": "MinIO: create object keys under MINIO_PREFIX/simulations/<sim_id>/ for report.json, out.mp4, preview.gif, snapshot.png, logs/*. Generate presigned URLs with TTL. DB: upsert sim_runs row with job_id, sim_id, status, progress, report JSON, object paths, timings, metrics. Ensure idempotency on retries (same sim_id reuses paths). Acceptance: artefacts visible in MinIO with correct ACL; DB row reflects final state and downloadable URLs work until TTL.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Progress Streaming and Alert Publishing",
            "description": "Continuously update job progress and publish warnings/alerts to the UI via status and notifications.",
            "dependencies": [
              "9.4",
              "9.7"
            ],
            "details": "Emit periodic progress updates (0–100%) to jobs.progress and include stage (prepare, simulate, encode, upload). Publish alerts with codes (COLLISION_DETECTED, TIMEOUT, INVALID_INPUT, ENCODING_FAILED) and severity. Deduplicate repeated alerts and rate-limit notifications. On collision_count>0, mark job status with warning and include link to media. Acceptance: UI receives timely progress and a collision alert when applicable; no alert storming under noisy logs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Robust Error Handling and Failure Messaging",
            "description": "Standardize exception mapping, user-facing messages, and cleanup for failed or partial simulations.",
            "dependencies": [
              "9.4",
              "9.5",
              "9.6"
            ],
            "details": "Classify errors: timeout, CAMotics nonzero exit, corrupt/unsupported G-code, bad CMX, missing tools/stock, media encode failure, storage failure. Map to HTTP and job error codes with actionable messages. Persist failure logs and partial artefacts; ensure workspace cleanup and process termination. Mark job failed with reason and next steps. Acceptance: each error class yields deterministic status, message, and preserved diagnostics; retries occur only on transient classes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Schema Validation, Test Matrix, and CI with Containers",
            "description": "Add schema validation gates, comprehensive tests (collision/no-collision/boundaries), and CI integration for CAMotics/FFmpeg containers.",
            "dependencies": [
              "9.2",
              "9.3",
              "9.4",
              "9.5",
              "9.6",
              "9.7",
              "9.8",
              "9.9"
            ],
            "details": "JSON schema validation as part of worker completion and unit tests. Test fixtures: simple prism no-collision, intentional crash/collision, tiny stock, large G-code file, inch vs mm, invalid G-code. Assert: collision_count semantics, est_time_s > 0, video/snapshot exist and are downloadable, timeout transitions to failed with message. CI: container images with CAMotics and FFmpeg, cache dependencies, run integration tests in pipeline, publish artefact samples on CI for inspection. Acceptance: CI green across matrix; failures surface clear diagnostics; schema violations fail the build.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "ERP/MES Integration, Reporting/Analytics, Admin Panel, SIEM & Forensics",
        "description": "Deliver outbound/inbound ERP/MES APIs with HMAC signatures and retries, reporting dashboards with filters/exports, admin panel with MFA and incident lock, and full audit/SIEM pipeline with hash-chained logs.",
        "details": "ERP/MES:\n- Outbound event job.status.changed → POST /erp/v1/jobs/update with HMAC-SHA256(body, ERP_SHARED_SECRET) in X-Signature; retries 5s, 15s, 45s, 2m, 5m (max 5) with idempotency; record in erp_mes_sync\n- Inbound: POST /api/v1/erp/inbound/job/progress (Bearer + IP allowlist). Update job progress/status, write audit_logs and erp_mes_sync\n- Mock server (docker) for dev: POST /erp/v1/jobs/update → 200 {ok:true}, GET /erp/v1/health → 200 ok\nReporting/Analytics:\n- Backend: GET /reports/summary, /reports/detail; POST /reports/export (CSV/Excel/JSON). Filters: date range, status, model type, machine/post; metrics P50/P95 durations, success/failure rates, breakdowns\n- Frontend: Turkish dashboards with ECharts; quiet hours and repeat policies for alerts; export links as signed URLs\nAdmin Panel:\n- Modules: users/licenses/invoices, catalog (machine/material/template), jobs/logs, integration profiles; bulk actions: license extend, job cancel\n- Security: Admin role + MFA enforced; optional IP allowlist; incident lock toggles read-only; key rotation flows\nSIEM/Forensics:\n- Log feed: structured JSON with versions, PII masked to SIEM (via Fluent Bit/syslog RFC5424); DLQ and replay for delivery failures\n- Audit trail: append-only with chain_hash; forensic export (CSV/JSON) including external timestamp (optional)\nPseudocode (HMAC signature):\n- sig = base64(hmac_sha256(ERP_SHARED_SECRET, body_json_bytes))\n- headers: {'X-Signature': sig, 'X-Request-Id': req_id, 'Content-Type': 'application/json'}\n- verify inbound by recomputing and constant-time compare\n",
        "testStrategy": "Integration: outbound event fires on status changes and succeeds against mock; tampered signature rejected inbound (401). Retry schedule observed. Reporting metrics match expected aggregates on fixtures; exports download and honor TTL. Admin: MFA gate enforced; incident lock makes UI read-only. SIEM receives logs; simulate SIEM outage → DLQ populated and later replayed. Audit chain verifies end-to-end.",
        "priority": "medium",
        "dependencies": [
          2,
          3,
          4,
          5,
          6,
          8,
          9
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Data models, secrets, and config for ERP/MES, Reporting, SIEM, Admin",
            "description": "Define schemas, config keys, and base migrations for erp_mes_sync, audit_logs, reporting aggregates, and security secrets.",
            "dependencies": [],
            "details": "Schemas:\n- erp_mes_sync: {id, direction: 'outbound'|'inbound', job_id, external_id, status: 'queued'|'sent'|'acked'|'failed', request_id, idempotency_key, body_sha256, request_body(jsonb), response_code, error_message, attempt_count, last_attempt_at, created_at, updated_at}\n- audit_logs (append-only): {id, ts, actor_type, actor_id, action, object_type, object_id, payload(jsonb), payload_sha256, prev_hash, chain_hash, external_timestamp(optional), version}\n- reporting materialized views/rollups for durations and outcomes\nConfigs/Secrets:\n- ERP_SHARED_SECRET, ERP_BEARER_TOKEN, ADMIN_MFA_ISSUER, ADMIN_IP_ALLOWLIST, SIEM_ENDPOINT, FLUENT_BIT_CONFIG, KEYRING_CURRENT/KEYRING_NEXT\nOperational:\n- Feature flags: INCIDENT_LOCK_ENABLED, EXPORT_SIGNED_URL_TTL, ALERT_QUIET_HOURS\nAcceptance: Migrations apply cleanly; secrets loaded via vault/env; chain_hash immutable constraint; base feature flags readable at runtime.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Outbound ERP webhook publisher with HMAC and idempotency",
            "description": "Emit job.status.changed to ERP POST /erp/v1/jobs/update with HMAC-SHA256 signature, headers, idempotency, and persistence.",
            "dependencies": [
              "10.1"
            ],
            "details": "Trigger: on job.status.changed\nHTTP: POST https://<erp-host>/erp/v1/jobs/update\nPayload schema: {job_id:string, status:string, progress:number(0-100), updated_at:iso8601, meta?:object}\nHeaders: {'X-Signature': base64(hmac_sha256(ERP_SHARED_SECRET, body_bytes)), 'X-Request-Id': uuid, 'Idempotency-Key': idem_key, 'Content-Type': 'application/json'}\nBehavior: write outbound record to erp_mes_sync, dedupe by idempotency_key, mark status transitions; constant-time signature generation\nSLA: P95 publish latency < 2s in steady state\nAcceptance: Sample event delivers 200 against mock; erp_mes_sync row created with body_sha256; duplicate status changes reuse existing idem record.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Outbound retry scheduler and DLQ handling",
            "description": "Implement fixed retry schedule with idempotent reattempts, error classification, and DLQ + replay endpoints.",
            "dependencies": [
              "10.2",
              "10.1"
            ],
            "details": "Retry policy: 5s, 15s, 45s, 2m, 5m (max 5 attempts)\nDedup: idem_key prevents duplicate HTTP sends; backoff respects per-record attempt_count\nDLQ: route hard failures (4xx except 429/408) after 1 attempt or on final attempt to erp_outbound_dlq; store error details\nReplay: POST /admin/erp/replay {sync_id[]} -> requeue eligible records\nObservability: counters for attempts, successes, failures; alert if >1% failures over 15m\nAcceptance: Verified schedule timings; 429 triggers next backoff; DLQ receives malformed payload cases; replay transitions from DLQ to queued and eventually acked.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Inbound ERP progress API with signature verification and IP allowlist",
            "description": "Expose POST /api/v1/erp/inbound/job/progress secured by Bearer, optional HMAC verification, and IP allowlist; update job and logs.",
            "dependencies": [
              "10.1"
            ],
            "details": "Endpoint: POST /api/v1/erp/inbound/job/progress\nAuth: Authorization: Bearer <ERP_BEARER_TOKEN>; IP allowlist match; if X-Signature present, verify base64(hmac_sha256(ERP_SHARED_SECRET, body_bytes)) using constant-time compare\nBody: {job_id:string, status:string, progress:number(0-100), message?:string, ext_ts?:iso8601}\nResponses: 200 {ok:true}, 401 invalid token/signature, 403 IP not allowed, 422 schema error\nSide effects: update job progress/status; write audit_logs(entry: action='erp.inbound.progress'); insert erp_mes_sync(direction='inbound')\nAcceptance: Tampered body → 401; non-allowlisted IP → 403; valid request updates job and creates both audit and sync rows.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "ERP/MES sync reconciliation and health checks",
            "description": "Periodic reconciliation of erp_mes_sync with ERP acknowledgments and admin tooling for manual requeue/resolve.",
            "dependencies": [
              "10.2",
              "10.3",
              "10.4",
              "10.6"
            ],
            "details": "Reconciler: scan outbound records older than 10m without ack; call ERP health/confirm APIs if available; otherwise requeue with cap\nAdmin: UI/API to mark failed as resolved, requeue, or ignore; show attempt history\nHealth: GET /internal/erp/health aggregates success/failure rates; consume /erp/v1/health from mock for connectivity\nAcceptance: Orphaned records detected and re-enqueued; health endpoint exposes JSON {ok, last_acked_at, failure_rate}; admin actions change record state accordingly.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Mock ERP server container for development and tests",
            "description": "Provide dockerized mock implementing POST /erp/v1/jobs/update and GET /erp/v1/health with simple behaviors and logs.",
            "dependencies": [
              "10.1"
            ],
            "details": "Endpoints: POST /erp/v1/jobs/update → 200 {ok:true}; GET /erp/v1/health → 200 'ok'\nConfig: accepts/records headers X-Signature, X-Request-Id; optionally validate signature via injected secret; toggle failure modes via env MOCK_FAILURE_RATE\nArtifacts: Dockerfile, docker-compose service, request/response logs\nAcceptance: Local stack can send outbound events and receive 200; toggling failure rate triggers retry flow; health returns ok.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Reporting backend: summary/detail endpoints and aggregates",
            "description": "Implement GET /reports/summary and /reports/detail with filters and metrics (P50/P95, success rates, breakdowns).",
            "dependencies": [
              "10.1"
            ],
            "details": "Endpoints:\n- GET /reports/summary?date_from&date_to&status&model_type&machine&post\n- GET /reports/detail?date_from&date_to&status&model_type&machine&post&page&limit\nMetrics: durations P50/P95, success/failure rates, breakdowns by status, model_type, machine/post\nResponse summary: {range:{from,to}, totals:{count,success,failed}, latency_ms:{p50,p95}, breakdowns:{by_status:[], by_machine:[]}}\nAcceptance: Aggregates match fixture expectations; P50/P95 computed from persisted durations; pagination stable in detail endpoint.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Report export generation and signed URL delivery",
            "description": "Create POST /reports/export to generate CSV/Excel/JSON exports with filters and deliver via signed URLs with TTL.",
            "dependencies": [
              "10.7",
              "10.1"
            ],
            "details": "Endpoint: POST /reports/export {format:'csv'|'xlsx'|'json', filters:{...}}\nBehavior: enqueue export job; on completion store object to S3/MinIO; generate signed URL (TTL configurable); email/webhook optional\nSecurity: exports scoped by RBAC; signed URL single-use optional; hash checksum recorded in audit_logs\nAcceptance: Exports download within TTL; content matches report filters; JSON schema validated; links expire after TTL; audit entry includes export_id and checksum.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Frontend dashboards (Turkish UI) with ECharts",
            "description": "Build Turkish-language dashboards for summary/detail reports with filters, charts, and export links.",
            "dependencies": [
              "10.7",
              "10.8"
            ],
            "details": "UI: tr-TR localization, date/time formats; filters for date range, status, model type, machine/post\nCharts: ECharts for trends, breakdown pie/stacked bar, latency distributions\nExports: use signed URLs from backend; show progress indicators\nAcceptance: UI strings in Turkish; charts render with sample data; export buttons download correct files; responsive layout.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Alerting quiet hours and repeat policies",
            "description": "Implement alert rules with quiet hours and repeat intervals for report anomalies (e.g., failure rate spikes).",
            "dependencies": [
              "10.9"
            ],
            "details": "Config: {quiet_hours:[{start:'22:00', end:'07:00', tz:'Europe/Istanbul'}], repeat_every:'30m'} per channel\nRules: trigger on failure_rate > threshold, latency p95 > threshold; suppress during quiet hours except severity=critical\nDelivery: email/Slack/webhook with dedup keys to control repeats\nAcceptance: During quiet hours, non-critical alerts suppressed; repeats occur per policy; breach generates alert within 2m.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "RBAC and permissions model for admin and reporting",
            "description": "Define roles and permissions, enforce on APIs and UI (admin, analyst, operator, auditor).",
            "dependencies": [
              "10.1"
            ],
            "details": "Roles: admin(full), analyst(read reports/exports), operator(jobs/actions), auditor(read-only audit/SIEM)\nPermissions map to resources: users, licenses, invoices, catalog, jobs, logs, integrations, reports, exports\nImplementation: policy middleware; route guards; attribute-based checks for tenant/project\nAcceptance: Permission matrix tests pass; forbidden routes return 403; UI hides disallowed actions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Admin panel modules and bulk actions",
            "description": "Implement modules: users/licenses/invoices, catalog, jobs/logs, integration profiles; bulk actions license extend and job cancel.",
            "dependencies": [
              "10.11"
            ],
            "details": "Routes: /admin/users, /admin/licenses, /admin/invoices, /admin/catalog, /admin/jobs, /admin/logs, /admin/integrations\nBulk: license extend (select N → +period), job cancel (multi-select → cancel API)\nUX: server-side pagination, filters, audit trail entries for changes\nAcceptance: Bulk operations create audit_logs; permissions enforced; cancel requests reach workers; pagination performant on 50k+ rows.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 13,
            "title": "Admin security: MFA enforcement and optional IP allowlist",
            "description": "Enforce MFA for admin role and support optional IP allowlist for admin routes.",
            "dependencies": [
              "10.11"
            ],
            "details": "MFA: TOTP enrollment/verification, backup codes, enforcement toggle; recovery flows audited\nIP allowlist: CIDR list for /admin/*; bypass for break-glass with additional approval\nAcceptance: Admin login requires MFA; blocked IPs receive 403; recovery uses backup codes and logs audit entry.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 14,
            "title": "Incident lock (read-only mode) and key rotation flows",
            "description": "Add incident lock to toggle read-only across admin, and implement key rotation for ERP secrets and signing keys.",
            "dependencies": [
              "10.11"
            ],
            "details": "Incident lock: feature flag sets DB to read-only for mutating admin actions; UI badge and banner; include allowlist of emergency ops\nKey rotation: support KEYRING_CURRENT/KEYRING_NEXT; outbound signs with current, inbound verifies current+next; rotate via admin flow with audit\nAcceptance: In lock, mutating endpoints return 423 and UI disabled; rotation completes without downtime; both keys accepted during window.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 15,
            "title": "SIEM log shipping pipeline with masking, DLQ, and replay",
            "description": "Ship structured JSON logs to SIEM using Fluent Bit/syslog RFC5424; mask PII; implement DLQ and replay.",
            "dependencies": [
              "10.1"
            ],
            "details": "Format: versioned JSON envelope {ts, level, svc, host, req_id, actor, event, payload_redacted, schema_ver}\nTransport: Fluent Bit to SIEM_ENDPOINT via syslog RFC5424/TCP+TLS; retry/backoff; batch and compress\nMasking: PII detection and redaction before egress; allowlist fields\nDLQ: failed batches to local durable queue; admin endpoint to replay\nAcceptance: Logs visible in SIEM with correct schema; PII fields redacted; induced outage stores in DLQ then replays successfully.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 16,
            "title": "Audit trail with chain_hash and forensic export",
            "description": "Ensure append-only audit with hash chaining and provide CSV/JSON forensic exports including optional external timestamp.",
            "dependencies": [
              "10.1"
            ],
            "details": "Chain: chain_hash = sha256(prev_hash || record_sha256); immutability enforced by DB constraints and no-update trigger\nExport: GET /forensics/audit/export?format=csv|json&date_from&date_to&actor&action includes chain proofs and external_timestamp if present\nVerification: CLI tool to verify chain integrity offline\nAcceptance: Chain validates end-to-end on sample; export downloads with correct fields; tampering test fails verification.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 17,
            "title": "Observability dashboards and operational SLAs",
            "description": "Create metrics, logs, and traces dashboards and define SLAs/SLOs for ERP, reporting, admin, and SIEM pipelines.",
            "dependencies": [
              "10.2",
              "10.4",
              "10.7",
              "10.15",
              "10.16"
            ],
            "details": "Metrics: ERP outbound success rate, retries, p50/p95 latencies; inbound 2xx rate; report generation duration; export queue times; SIEM ship lag; admin auth failures\nDashboards: Grafana/Datadog with service-level SLOs (e.g., ERP outbound success >= 99.5% weekly, report export p95 < 60s)\nAlerts: tie to quiet hours policies; runbooks linked\nAcceptance: Dashboards populated in staging; alerts fire on synthetic canaries; SLOs documented and approved.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 18,
            "title": "Data retention & GDPR, E2E tests, failure drills, rollout & runbooks",
            "description": "Define retention policies and GDPR flows; implement end-to-end tests and chaos drills; plan rollout and author runbooks.",
            "dependencies": [
              "10.2",
              "10.3",
              "10.4",
              "10.5",
              "10.6",
              "10.7",
              "10.8",
              "10.9",
              "10.10",
              "10.11",
              "10.12",
              "10.13",
              "10.14",
              "10.15",
              "10.16",
              "10.17"
            ],
            "details": "Retention: policies for audit_logs (e.g., 7y), SIEM (per contract), exports (TTL), PII minimization; GDPR: data subject access/delete with audit and exemptions\nE2E: tests covering outbound/inbound ERP, reports, exports, admin MFA/lock, SIEM shipment, forensic export; CI pipeline gates\nFailure drills: simulate ERP outage, SIEM sink failure, key rotation, incident lock; document RTO/RPO\nRollout: phased enablement, feature flags, canary, rollback plan; runbooks for on-call\nAcceptance: Retention jobs enforce policies; DSAR workflow validated; E2E suite passes in CI; drills executed with postmortems; rollout completed without SEV incidents.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-15T19:56:57.780Z",
      "updated": "2025-09-07T16:57:38.548Z",
      "description": "Tasks for master context"
    }
  }
}