name: Task 7.14 Integration Tests

on:
  push:
    branches:
      - main
      - 'fix/pr*'
      - 'feature/*'
    paths:
      - 'apps/api/**'
      - 'infra/**'
      - 'tests/data/**'
      - '.github/workflows/task-7-14-integration-tests.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'apps/api/**'
      - 'infra/**'
      - 'tests/data/**'

env:
  PYTHONHASHSEED: 0
  LC_ALL: C.UTF-8
  TZ: UTC

jobs:
  integration-tests:
    name: Golden Artefacts Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        locale: [C.UTF-8, tr_TR.UTF-8]
        include:
          - locale: C.UTF-8
            name: Standard
          - locale: tr_TR.UTF-8
            name: Turkish

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Cache Docker layers
        uses: actions/cache@v3
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-

      - name: Cache test data
        uses: actions/cache@v3
        with:
          path: apps/api/tests/data/golden
          key: golden-artefacts-${{ hashFiles('apps/api/tests/data/**/*.json') }}
          restore-keys: |
            golden-artefacts-

      - name: Install locale support
        run: |
          sudo apt-get update
          sudo apt-get install -y locales
          sudo locale-gen ${{ matrix.locale }}
          sudo update-locale LANG=${{ matrix.locale }}

      - name: Build test containers
        run: |
          docker compose -f infra/compose/docker-compose.test.yml build

      - name: Run integration tests
        env:
          LC_ALL: ${{ matrix.locale }}
          RUN_SLOW_TESTS: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          TEST_TURKISH_LOCALE: ${{ matrix.locale == 'tr_TR.UTF-8' }}
        run: |
          chmod +x scripts/ci/run_integration_tests.sh
          ./scripts/ci/run_integration_tests.sh

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results-${{ matrix.name }}
          path: test_results/

      - name: Upload coverage report
        if: success()
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report-${{ matrix.name }}
          path: test_results/coverage.xml

      - name: Publish test results
        if: always()
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: test_results/junit.xml
          check_name: Integration Test Results (${{ matrix.name }})

  verify-golden-artefacts:
    name: Verify Golden Artefacts
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: integration-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r apps/api/requirements.txt

      - name: Download golden artefacts
        uses: actions/download-artifact@v3
        with:
          name: golden-artefacts
          path: apps/api/tests/data/golden

      - name: Verify golden artefacts
        run: |
          cd apps/api
          python tools/gen_golden.py --verify --output golden_verification.json

      - name: Upload verification report
        uses: actions/upload-artifact@v3
        with:
          name: golden-verification
          path: apps/api/golden_verification.json

  performance-benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: integration-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run performance tests
        run: |
          docker compose -f infra/compose/docker-compose.test.yml run --rm \
            -e RUN_SLOW_TESTS=true \
            test_runner pytest tests/performance/ -v --benchmark-only

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: .benchmarks/

  matrix-summary:
    name: Test Matrix Summary
    runs-on: ubuntu-latest
    needs: [integration-tests, verify-golden-artefacts, performance-benchmark]
    if: always()

    steps:
      - name: Check results
        run: |
          if [ "${{ needs.integration-tests.result }}" != "success" ]; then
            echo "Integration tests failed"
            exit 1
          fi
          if [ "${{ needs.verify-golden-artefacts.result }}" != "success" ]; then
            echo "Golden artefact verification failed"
            exit 1
          fi
          echo "All tests passed successfully!"